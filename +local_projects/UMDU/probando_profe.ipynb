{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from b1 import *\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import statistics\n",
    "import sklearn.metrics as metrics\n",
    " \n",
    "# Evitar truncar data mostrada al usar jupyter notebook\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    " \n",
    "# Constante que aloja el diccionario JSON con toda la data\n",
    "DATA = None\n",
    "\n",
    "# Obtener data JSON\n",
    "if os.path.exists('./out/dataout.json'):\n",
    "    DATA = json.load(open('./out/dataout.json', 'r'))\n",
    "else:\n",
    "    data_url = urlopen('http://nutriexcel.cl/UMDU/dataout_v2.json')\n",
    "    DATA = json.loads(data_url.read())\n",
    " \n",
    "# Labels base de las columnas\n",
    "LABELS_BASE = {\n",
    "    # Parámetros del alumno (Target)\n",
    "    'p1':                            ['p1'],\n",
    "    'p2':                            ['p2'],\n",
    "    'np':                            ['np'],\n",
    "    'p1p2':                          ['p1p2'], # Promedio p1p2 y p2p2\n",
    "    'p2p2':                          ['p2p2'],\n",
    "    \n",
    "    # Parámetros del laboratorio (Features)\n",
    "    'grade':                         ['g_lab#'],\n",
    "    'attempts':                      ['a_lab#'],\n",
    "    'usedtime':                      ['ut_lab#'],\n",
    "    'activetime':                    ['act_lab#'],\n",
    "    'disconnections':                ['dis_lab#'],      # log\n",
    "    'compilationtime':               ['ct_lab#'],\n",
    "    'runtimedebuggingtime':          ['rt_lab#'],\n",
    "    'compilationtimeratio':          ['ctr_lab#'],\n",
    "    'runtimedebuggingtimeratio':     ['rtr_lab#'],\n",
    "    'errorsreductionratio':          ['err_lab#'],\n",
    "    'compilationerrorsratio':        ['cer_lab#'],\n",
    "    'activequartiles':               ['actq1_lab#','actq2_lab#','actq3_lab#'],\n",
    "    'questionsdifficulty':           ['qd$_lab#'],\n",
    "    'questionsgrades':               ['qg$_lab#'],      # Promedio\n",
    "    'questionsattempts':             ['qat$_lab#'],     # Sumar - Max   # log\n",
    "    'questionsactivetime':           ['qact$_lab#'],    # Promedio\n",
    "    'questionsavgtime':              ['qavt$_lab#'],    # Promedio\n",
    "    'questionsmaxerrors':            ['qme$_lab#'],     # Max\n",
    "    'questionsmaxconsecutiveerrors': ['qmce$_lab#'],    # Max\n",
    "    'questionsmaxsimilarityratio':   ['qmsr$_lab#'],    # Promedio\n",
    "    'questionscorrectness':          ['qc$_lab#']       # Promedio\n",
    "}\n",
    " \n",
    " \n",
    "# Cantidad de preguntas por lab\n",
    "LABS_LENGTHS = {\n",
    "    '1': 7,\n",
    "    '2': 6,\n",
    "    '3': 6,\n",
    "    '4': 5,\n",
    "    '5': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curso  7 : 55\n",
      "curso  13 : 22\n",
      "curso  19 : 54\n",
      "curso  24 : 28\n",
      "curso  30 : 53\n",
      "curso  36 : 41\n",
      "total: 253\n"
     ]
    }
   ],
   "source": [
    "total=0\n",
    "for id in DATA[\"courses\"]:\n",
    "    students=len(DATA[\"courses\"][id][\"students\"])\n",
    "    total+=students\n",
    "    print(\"curso \",id,\":\",students)\n",
    "print(\"total:\",total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Parameters**\n",
    "\n",
    "# Objective vector\n",
    "TARGET = 'mean(p$p2)'\n",
    "NORM_TYPE = 'col'\n",
    "N_FEATURES = 5\n",
    " \n",
    " \n",
    "# Import needed libraries ----------------------------------------\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import KFold\n",
    " \n",
    "random_state = None # Random state for train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "13\n",
      "19\n",
      "24\n",
      "30\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# CursoData retorna el curso de los alumnos del lab Correspondiente\n",
    "datalab1_all,cursoData = get_custom_dataframe(DATA, [1], ['p1p2','p2p2'], 'all', labels=True, index=None)\n",
    "#@title **Data preparation**\n",
    "\n",
    "datalab1 = copy.deepcopy(datalab1_all)\n",
    "\n",
    "# Remove questionsdifficulty\n",
    "remove_col(datalab1, 'qd?')\n",
    "# Group columns\n",
    "datalab1_all = apply(datalab1_all, ['p1p2','p2p2'], statistics.mean)\n",
    "datalab1 = apply(datalab1, ['p1p2','p2p2'], statistics.mean)\n",
    "datalab1 = apply(datalab1, 'dis_lab1', norm_log)\n",
    "datalab1 = apply(datalab1, 'qg?', statistics.mean)\n",
    "datalab1 = apply(datalab1, 'qat?', sum, replace=False)\n",
    "datalab1 = apply(datalab1, 'sum(qat$_lab1)', norm_log, replace=False)\n",
    "datalab1 = apply(datalab1, 'qat?', max)\n",
    "datalab1 = apply(datalab1, 'qact?', statistics.mean)\n",
    "datalab1 = apply(datalab1, 'qavt?', statistics.mean)\n",
    "datalab1 = apply(datalab1, 'qme?', max)\n",
    "datalab1 = apply(datalab1, 'qmce?', max)\n",
    "datalab1 = apply(datalab1, 'qmsr?', statistics.mean)\n",
    "datalab1 = apply(datalab1, 'qc?', statistics.mean)\n",
    "aux = datalab1['act_lab1'] / datalab1['sum(qat$_lab1)']\n",
    "for i in range(len(aux)):\n",
    "    if not aux[i] > 0:\n",
    "        aux[i] = 0\n",
    "datalab1['avgtime_lab1'] = aux\n",
    "datalab1 = datalab1.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se transforma a dataframe la info de ese curso en particular\n",
    "cursoDF = pd.DataFrame(cursoData,columns=['curso'])\n",
    "\n",
    "# Se concatenan los dos dataframes \n",
    "dfFinlab1 = pd.concat([datalab1,cursoDF],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean(p$p2)</th>\n",
       "      <th>g_lab1</th>\n",
       "      <th>a_lab1</th>\n",
       "      <th>ut_lab1</th>\n",
       "      <th>act_lab1</th>\n",
       "      <th>norm_log(dis_lab1)</th>\n",
       "      <th>ct_lab1</th>\n",
       "      <th>rt_lab1</th>\n",
       "      <th>ctr_lab1</th>\n",
       "      <th>rtr_lab1</th>\n",
       "      <th>err_lab1</th>\n",
       "      <th>cer_lab1</th>\n",
       "      <th>actq1_lab1</th>\n",
       "      <th>actq2_lab1</th>\n",
       "      <th>actq3_lab1</th>\n",
       "      <th>mean(qg$_lab1)</th>\n",
       "      <th>max(qat$_lab1)</th>\n",
       "      <th>mean(qact$_lab1)</th>\n",
       "      <th>mean(qavt$_lab1)</th>\n",
       "      <th>max(qme$_lab1)</th>\n",
       "      <th>max(qmce$_lab1)</th>\n",
       "      <th>mean(qmsr$_lab1)</th>\n",
       "      <th>mean(qc$_lab1)</th>\n",
       "      <th>sum(qat$_lab1)</th>\n",
       "      <th>norm_log(sum(qat$_lab1))</th>\n",
       "      <th>avgtime_lab1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.455061</td>\n",
       "      <td>-0.761629</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.049073</td>\n",
       "      <td>-0.890975</td>\n",
       "      <td>0.786036</td>\n",
       "      <td>-0.874950</td>\n",
       "      <td>-0.637584</td>\n",
       "      <td>0.383103</td>\n",
       "      <td>-0.514311</td>\n",
       "      <td>-0.665163</td>\n",
       "      <td>-0.709985</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.489600</td>\n",
       "      <td>-0.852848</td>\n",
       "      <td>-0.743363</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.475535</td>\n",
       "      <td>0.930445</td>\n",
       "      <td>-0.554857</td>\n",
       "      <td>-0.231774</td>\n",
       "      <td>-0.390801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.25</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.817620</td>\n",
       "      <td>-0.125169</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>-0.603989</td>\n",
       "      <td>-0.385012</td>\n",
       "      <td>-0.690473</td>\n",
       "      <td>-0.336087</td>\n",
       "      <td>0.553319</td>\n",
       "      <td>-0.195346</td>\n",
       "      <td>-1.196970</td>\n",
       "      <td>-1.260748</td>\n",
       "      <td>-1.289504</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.350438</td>\n",
       "      <td>-0.503732</td>\n",
       "      <td>0.468175</td>\n",
       "      <td>-0.805294</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>-0.975358</td>\n",
       "      <td>0.456763</td>\n",
       "      <td>-0.370575</td>\n",
       "      <td>-0.038347</td>\n",
       "      <td>0.348930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.25</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.016240</td>\n",
       "      <td>2.141457</td>\n",
       "      <td>1.727227</td>\n",
       "      <td>-0.647569</td>\n",
       "      <td>0.618115</td>\n",
       "      <td>-1.033409</td>\n",
       "      <td>-0.374360</td>\n",
       "      <td>1.189007</td>\n",
       "      <td>-0.304597</td>\n",
       "      <td>0.559647</td>\n",
       "      <td>0.838540</td>\n",
       "      <td>0.771844</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.275796</td>\n",
       "      <td>0.436110</td>\n",
       "      <td>0.508278</td>\n",
       "      <td>1.954690</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>-0.207310</td>\n",
       "      <td>-0.793488</td>\n",
       "      <td>0.366554</td>\n",
       "      <td>0.533396</td>\n",
       "      <td>1.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.911666</td>\n",
       "      <td>-0.877879</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.537167</td>\n",
       "      <td>0.542220</td>\n",
       "      <td>-0.117179</td>\n",
       "      <td>3.331465</td>\n",
       "      <td>-0.191185</td>\n",
       "      <td>-0.496710</td>\n",
       "      <td>-1.108546</td>\n",
       "      <td>-1.255053</td>\n",
       "      <td>-1.286561</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.837508</td>\n",
       "      <td>-0.257596</td>\n",
       "      <td>1.066704</td>\n",
       "      <td>-0.673866</td>\n",
       "      <td>-0.631388</td>\n",
       "      <td>0.653245</td>\n",
       "      <td>0.519023</td>\n",
       "      <td>-0.739140</td>\n",
       "      <td>-0.459359</td>\n",
       "      <td>-0.220591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.885970</td>\n",
       "      <td>-0.428801</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>0.253080</td>\n",
       "      <td>-0.496104</td>\n",
       "      <td>0.812065</td>\n",
       "      <td>-0.275336</td>\n",
       "      <td>-0.042385</td>\n",
       "      <td>0.195740</td>\n",
       "      <td>1.020738</td>\n",
       "      <td>0.854618</td>\n",
       "      <td>0.777731</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.350438</td>\n",
       "      <td>-0.519575</td>\n",
       "      <td>-0.314631</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.578996</td>\n",
       "      <td>0.505561</td>\n",
       "      <td>-0.481144</td>\n",
       "      <td>-0.150910</td>\n",
       "      <td>0.074525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.223061</td>\n",
       "      <td>0.834033</td>\n",
       "      <td>0.802062</td>\n",
       "      <td>1.102885</td>\n",
       "      <td>-0.547800</td>\n",
       "      <td>0.596023</td>\n",
       "      <td>-0.989770</td>\n",
       "      <td>-0.457405</td>\n",
       "      <td>0.725633</td>\n",
       "      <td>-0.137240</td>\n",
       "      <td>-0.287981</td>\n",
       "      <td>0.113180</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.484540</td>\n",
       "      <td>0.449124</td>\n",
       "      <td>0.280275</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>-0.127584</td>\n",
       "      <td>-0.066559</td>\n",
       "      <td>0.440267</td>\n",
       "      <td>0.578680</td>\n",
       "      <td>0.190098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.525164</td>\n",
       "      <td>1.814468</td>\n",
       "      <td>1.834012</td>\n",
       "      <td>3.475076</td>\n",
       "      <td>0.554319</td>\n",
       "      <td>1.625480</td>\n",
       "      <td>-0.308141</td>\n",
       "      <td>-0.973142</td>\n",
       "      <td>2.074644</td>\n",
       "      <td>-0.264286</td>\n",
       "      <td>-0.396178</td>\n",
       "      <td>0.761379</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>4.311521</td>\n",
       "      <td>2.609458</td>\n",
       "      <td>0.865472</td>\n",
       "      <td>0.246129</td>\n",
       "      <td>4.344975</td>\n",
       "      <td>0.193146</td>\n",
       "      <td>0.299430</td>\n",
       "      <td>3.241359</td>\n",
       "      <td>1.662132</td>\n",
       "      <td>-0.970884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.259429</td>\n",
       "      <td>0.711943</td>\n",
       "      <td>1.007758</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>-0.317917</td>\n",
       "      <td>0.142463</td>\n",
       "      <td>-0.698772</td>\n",
       "      <td>-0.682122</td>\n",
       "      <td>1.538945</td>\n",
       "      <td>-0.123689</td>\n",
       "      <td>-0.268217</td>\n",
       "      <td>-0.290390</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>1.528262</td>\n",
       "      <td>1.082286</td>\n",
       "      <td>-0.103934</td>\n",
       "      <td>-0.016727</td>\n",
       "      <td>2.722248</td>\n",
       "      <td>0.455451</td>\n",
       "      <td>0.113491</td>\n",
       "      <td>1.435392</td>\n",
       "      <td>1.073311</td>\n",
       "      <td>-0.787474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.911666</td>\n",
       "      <td>-0.877879</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.470344</td>\n",
       "      <td>-0.636893</td>\n",
       "      <td>0.038346</td>\n",
       "      <td>-0.051771</td>\n",
       "      <td>1.214819</td>\n",
       "      <td>-0.448682</td>\n",
       "      <td>-0.939152</td>\n",
       "      <td>-1.083546</td>\n",
       "      <td>-1.122386</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.698345</td>\n",
       "      <td>-0.886232</td>\n",
       "      <td>-0.533004</td>\n",
       "      <td>-0.016727</td>\n",
       "      <td>-0.631388</td>\n",
       "      <td>-0.011951</td>\n",
       "      <td>0.042817</td>\n",
       "      <td>-0.849709</td>\n",
       "      <td>-0.617982</td>\n",
       "      <td>0.102401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>2.519935</td>\n",
       "      <td>1.185957</td>\n",
       "      <td>-0.329437</td>\n",
       "      <td>3.794682</td>\n",
       "      <td>-0.881138</td>\n",
       "      <td>1.618888</td>\n",
       "      <td>-0.240785</td>\n",
       "      <td>0.224240</td>\n",
       "      <td>-0.280886</td>\n",
       "      <td>0.404077</td>\n",
       "      <td>0.386915</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>1.180355</td>\n",
       "      <td>2.582864</td>\n",
       "      <td>0.545034</td>\n",
       "      <td>1.034696</td>\n",
       "      <td>-0.090479</td>\n",
       "      <td>-0.273038</td>\n",
       "      <td>-0.788439</td>\n",
       "      <td>1.361679</td>\n",
       "      <td>1.042388</td>\n",
       "      <td>0.534182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.55</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.909336</td>\n",
       "      <td>-0.739865</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.946816</td>\n",
       "      <td>-0.714988</td>\n",
       "      <td>-1.092625</td>\n",
       "      <td>-0.445439</td>\n",
       "      <td>2.339420</td>\n",
       "      <td>-1.288383</td>\n",
       "      <td>-1.218991</td>\n",
       "      <td>-1.361240</td>\n",
       "      <td>-1.390234</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.976671</td>\n",
       "      <td>-1.111997</td>\n",
       "      <td>-0.934640</td>\n",
       "      <td>-1.068150</td>\n",
       "      <td>-0.847752</td>\n",
       "      <td>-3.661702</td>\n",
       "      <td>0.711693</td>\n",
       "      <td>-1.070848</td>\n",
       "      <td>-1.009106</td>\n",
       "      <td>1.611617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.210770</td>\n",
       "      <td>-1.174611</td>\n",
       "      <td>0.558591</td>\n",
       "      <td>-0.521187</td>\n",
       "      <td>-0.774383</td>\n",
       "      <td>0.401454</td>\n",
       "      <td>0.067909</td>\n",
       "      <td>-0.191185</td>\n",
       "      <td>-0.107735</td>\n",
       "      <td>1.026159</td>\n",
       "      <td>0.854953</td>\n",
       "      <td>0.772498</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.211275</td>\n",
       "      <td>-0.958658</td>\n",
       "      <td>-0.739180</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>-0.415025</td>\n",
       "      <td>0.606992</td>\n",
       "      <td>0.881646</td>\n",
       "      <td>-0.260006</td>\n",
       "      <td>0.065289</td>\n",
       "      <td>-1.483413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.448061</td>\n",
       "      <td>-0.285478</td>\n",
       "      <td>0.802062</td>\n",
       "      <td>0.011938</td>\n",
       "      <td>0.656612</td>\n",
       "      <td>0.273260</td>\n",
       "      <td>1.691789</td>\n",
       "      <td>-0.339985</td>\n",
       "      <td>0.407908</td>\n",
       "      <td>-0.820915</td>\n",
       "      <td>-0.674207</td>\n",
       "      <td>-0.718488</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.067051</td>\n",
       "      <td>0.236938</td>\n",
       "      <td>0.661192</td>\n",
       "      <td>-0.279583</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>-0.212179</td>\n",
       "      <td>-0.313075</td>\n",
       "      <td>0.145416</td>\n",
       "      <td>0.386676</td>\n",
       "      <td>-0.680124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.889654</td>\n",
       "      <td>-1.018548</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.691148</td>\n",
       "      <td>-1.173654</td>\n",
       "      <td>-0.350792</td>\n",
       "      <td>-1.574197</td>\n",
       "      <td>-0.191185</td>\n",
       "      <td>0.169879</td>\n",
       "      <td>-1.204423</td>\n",
       "      <td>-1.341142</td>\n",
       "      <td>-1.371265</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.837508</td>\n",
       "      <td>-1.242704</td>\n",
       "      <td>-1.227669</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>-0.415025</td>\n",
       "      <td>0.753663</td>\n",
       "      <td>0.772270</td>\n",
       "      <td>-0.960278</td>\n",
       "      <td>-0.798859</td>\n",
       "      <td>0.098353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.186075</td>\n",
       "      <td>-0.547175</td>\n",
       "      <td>1.611097</td>\n",
       "      <td>0.129604</td>\n",
       "      <td>-0.451007</td>\n",
       "      <td>0.776275</td>\n",
       "      <td>-0.052379</td>\n",
       "      <td>0.138806</td>\n",
       "      <td>0.738828</td>\n",
       "      <td>0.696179</td>\n",
       "      <td>0.569220</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.072112</td>\n",
       "      <td>-0.185170</td>\n",
       "      <td>0.120644</td>\n",
       "      <td>0.771840</td>\n",
       "      <td>-0.306843</td>\n",
       "      <td>0.202275</td>\n",
       "      <td>0.192578</td>\n",
       "      <td>-0.149436</td>\n",
       "      <td>0.161162</td>\n",
       "      <td>-0.677507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.00</td>\n",
       "      <td>-1.775665</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.905267</td>\n",
       "      <td>-0.855585</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>-0.820435</td>\n",
       "      <td>-0.349814</td>\n",
       "      <td>-0.786782</td>\n",
       "      <td>0.719162</td>\n",
       "      <td>0.652519</td>\n",
       "      <td>-0.983853</td>\n",
       "      <td>0.310977</td>\n",
       "      <td>0.151171</td>\n",
       "      <td>0.084401</td>\n",
       "      <td>-1.775568</td>\n",
       "      <td>-0.907089</td>\n",
       "      <td>-0.812674</td>\n",
       "      <td>0.219207</td>\n",
       "      <td>-0.936722</td>\n",
       "      <td>-0.847752</td>\n",
       "      <td>-0.975358</td>\n",
       "      <td>-0.879306</td>\n",
       "      <td>-1.181417</td>\n",
       "      <td>-1.260238</td>\n",
       "      <td>2.065196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.193974</td>\n",
       "      <td>1.365390</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.528451</td>\n",
       "      <td>1.681737</td>\n",
       "      <td>-0.893502</td>\n",
       "      <td>0.895948</td>\n",
       "      <td>-0.697306</td>\n",
       "      <td>0.075933</td>\n",
       "      <td>-0.506180</td>\n",
       "      <td>-0.649754</td>\n",
       "      <td>-0.685130</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.554121</td>\n",
       "      <td>1.012124</td>\n",
       "      <td>-0.050041</td>\n",
       "      <td>0.377556</td>\n",
       "      <td>-0.198661</td>\n",
       "      <td>0.083599</td>\n",
       "      <td>0.065534</td>\n",
       "      <td>0.513980</td>\n",
       "      <td>0.622541</td>\n",
       "      <td>0.617778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.895874</td>\n",
       "      <td>-0.883718</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.371564</td>\n",
       "      <td>-1.212151</td>\n",
       "      <td>0.276514</td>\n",
       "      <td>-1.702382</td>\n",
       "      <td>1.777120</td>\n",
       "      <td>0.360936</td>\n",
       "      <td>1.020738</td>\n",
       "      <td>0.858638</td>\n",
       "      <td>0.774787</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.976671</td>\n",
       "      <td>-1.143684</td>\n",
       "      <td>-0.764273</td>\n",
       "      <td>3.794680</td>\n",
       "      <td>-0.631388</td>\n",
       "      <td>0.274090</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-1.070848</td>\n",
       "      <td>-1.009106</td>\n",
       "      <td>1.090601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.257496</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.558591</td>\n",
       "      <td>0.574117</td>\n",
       "      <td>0.280439</td>\n",
       "      <td>0.702743</td>\n",
       "      <td>0.550274</td>\n",
       "      <td>-0.444245</td>\n",
       "      <td>-0.448682</td>\n",
       "      <td>-0.440456</td>\n",
       "      <td>-0.589793</td>\n",
       "      <td>-0.452603</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.350438</td>\n",
       "      <td>0.299179</td>\n",
       "      <td>1.096272</td>\n",
       "      <td>2.480402</td>\n",
       "      <td>-0.306843</td>\n",
       "      <td>-0.119672</td>\n",
       "      <td>-0.220526</td>\n",
       "      <td>-0.038867</td>\n",
       "      <td>0.250436</td>\n",
       "      <td>-0.030432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.338202</td>\n",
       "      <td>0.105210</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>-0.513924</td>\n",
       "      <td>0.043957</td>\n",
       "      <td>-0.648176</td>\n",
       "      <td>0.134735</td>\n",
       "      <td>-0.488784</td>\n",
       "      <td>0.294963</td>\n",
       "      <td>-0.213806</td>\n",
       "      <td>0.841554</td>\n",
       "      <td>0.765630</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>-0.079360</td>\n",
       "      <td>-0.155502</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>0.666793</td>\n",
       "      <td>0.523614</td>\n",
       "      <td>0.061327</td>\n",
       "      <td>0.550837</td>\n",
       "      <td>0.644018</td>\n",
       "      <td>-0.652784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.242991</td>\n",
       "      <td>0.082915</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.458723</td>\n",
       "      <td>-0.343215</td>\n",
       "      <td>-0.575294</td>\n",
       "      <td>-0.417493</td>\n",
       "      <td>-1.118905</td>\n",
       "      <td>1.316749</td>\n",
       "      <td>-0.141644</td>\n",
       "      <td>-0.291666</td>\n",
       "      <td>-0.345333</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>2.224077</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.567184</td>\n",
       "      <td>-0.805294</td>\n",
       "      <td>3.154975</td>\n",
       "      <td>0.897901</td>\n",
       "      <td>0.334767</td>\n",
       "      <td>1.214253</td>\n",
       "      <td>0.978473</td>\n",
       "      <td>-1.116212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.75</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.256895</td>\n",
       "      <td>-1.705436</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.845130</td>\n",
       "      <td>-1.127457</td>\n",
       "      <td>0.965638</td>\n",
       "      <td>-0.424176</td>\n",
       "      <td>2.339420</td>\n",
       "      <td>-2.128084</td>\n",
       "      <td>0.366199</td>\n",
       "      <td>0.842894</td>\n",
       "      <td>0.770209</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.976671</td>\n",
       "      <td>-1.284575</td>\n",
       "      <td>-1.317190</td>\n",
       "      <td>0.771840</td>\n",
       "      <td>-0.847752</td>\n",
       "      <td>0.389114</td>\n",
       "      <td>-0.142280</td>\n",
       "      <td>-1.255130</td>\n",
       "      <td>-1.459747</td>\n",
       "      <td>-1.565690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.25</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.461146</td>\n",
       "      <td>-0.098097</td>\n",
       "      <td>0.802062</td>\n",
       "      <td>-0.930837</td>\n",
       "      <td>0.508123</td>\n",
       "      <td>-1.130368</td>\n",
       "      <td>1.097035</td>\n",
       "      <td>-0.191185</td>\n",
       "      <td>-1.763387</td>\n",
       "      <td>-0.510246</td>\n",
       "      <td>-0.660473</td>\n",
       "      <td>-0.694614</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.628763</td>\n",
       "      <td>-0.427345</td>\n",
       "      <td>0.613374</td>\n",
       "      <td>-0.805294</td>\n",
       "      <td>-0.739570</td>\n",
       "      <td>0.809654</td>\n",
       "      <td>0.749554</td>\n",
       "      <td>-0.739140</td>\n",
       "      <td>-0.459359</td>\n",
       "      <td>1.586931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.886042</td>\n",
       "      <td>0.639751</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.275688</td>\n",
       "      <td>1.350661</td>\n",
       "      <td>-0.553169</td>\n",
       "      <td>1.256203</td>\n",
       "      <td>-0.081357</td>\n",
       "      <td>0.070656</td>\n",
       "      <td>1.021755</td>\n",
       "      <td>0.858303</td>\n",
       "      <td>0.782309</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.206214</td>\n",
       "      <td>0.849165</td>\n",
       "      <td>0.578307</td>\n",
       "      <td>1.166123</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.416501</td>\n",
       "      <td>0.261569</td>\n",
       "      <td>0.808832</td>\n",
       "      <td>0.784528</td>\n",
       "      <td>-0.392760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.125654</td>\n",
       "      <td>0.410965</td>\n",
       "      <td>0.802062</td>\n",
       "      <td>3.005867</td>\n",
       "      <td>-1.069162</td>\n",
       "      <td>2.893105</td>\n",
       "      <td>-1.520736</td>\n",
       "      <td>-0.881534</td>\n",
       "      <td>1.614418</td>\n",
       "      <td>0.695501</td>\n",
       "      <td>1.226106</td>\n",
       "      <td>1.142056</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.554121</td>\n",
       "      <td>0.245426</td>\n",
       "      <td>-0.164137</td>\n",
       "      <td>-0.411011</td>\n",
       "      <td>1.748611</td>\n",
       "      <td>0.504748</td>\n",
       "      <td>-0.076655</td>\n",
       "      <td>0.550837</td>\n",
       "      <td>0.644018</td>\n",
       "      <td>-0.357477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>2.163171</td>\n",
       "      <td>-0.150648</td>\n",
       "      <td>1.343199</td>\n",
       "      <td>2.122651</td>\n",
       "      <td>-0.722687</td>\n",
       "      <td>3.025203</td>\n",
       "      <td>-0.881633</td>\n",
       "      <td>0.138806</td>\n",
       "      <td>0.725633</td>\n",
       "      <td>-0.990309</td>\n",
       "      <td>-0.444749</td>\n",
       "      <td>-0.449660</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.628763</td>\n",
       "      <td>0.269191</td>\n",
       "      <td>1.016213</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>-0.198661</td>\n",
       "      <td>0.498053</td>\n",
       "      <td>0.348228</td>\n",
       "      <td>-0.260006</td>\n",
       "      <td>0.065289</td>\n",
       "      <td>0.078115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.894458</td>\n",
       "      <td>-0.257875</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>-0.646116</td>\n",
       "      <td>0.196846</td>\n",
       "      <td>-0.705440</td>\n",
       "      <td>0.813934</td>\n",
       "      <td>0.652519</td>\n",
       "      <td>-0.848741</td>\n",
       "      <td>1.024465</td>\n",
       "      <td>0.856293</td>\n",
       "      <td>0.779039</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.837508</td>\n",
       "      <td>-0.442622</td>\n",
       "      <td>1.845819</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.337992</td>\n",
       "      <td>0.389455</td>\n",
       "      <td>-0.997135</td>\n",
       "      <td>-0.865233</td>\n",
       "      <td>2.696585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.327727</td>\n",
       "      <td>1.612754</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>0.251627</td>\n",
       "      <td>1.504650</td>\n",
       "      <td>-0.391137</td>\n",
       "      <td>0.565461</td>\n",
       "      <td>-0.534841</td>\n",
       "      <td>0.708216</td>\n",
       "      <td>-0.514311</td>\n",
       "      <td>-0.655783</td>\n",
       "      <td>-0.692325</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>1.389099</td>\n",
       "      <td>1.501565</td>\n",
       "      <td>-0.211464</td>\n",
       "      <td>-0.411011</td>\n",
       "      <td>0.991339</td>\n",
       "      <td>0.158456</td>\n",
       "      <td>-0.089275</td>\n",
       "      <td>2.504229</td>\n",
       "      <td>1.453049</td>\n",
       "      <td>-0.787902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.726221</td>\n",
       "      <td>0.209251</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.717296</td>\n",
       "      <td>0.487224</td>\n",
       "      <td>-0.907818</td>\n",
       "      <td>0.660233</td>\n",
       "      <td>0.419703</td>\n",
       "      <td>0.031072</td>\n",
       "      <td>-0.508891</td>\n",
       "      <td>-0.657458</td>\n",
       "      <td>-0.699193</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.484540</td>\n",
       "      <td>-0.091808</td>\n",
       "      <td>-0.489942</td>\n",
       "      <td>-0.411011</td>\n",
       "      <td>-0.415025</td>\n",
       "      <td>0.535786</td>\n",
       "      <td>0.362531</td>\n",
       "      <td>0.108559</td>\n",
       "      <td>0.360541</td>\n",
       "      <td>-0.043074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.907453</td>\n",
       "      <td>-1.196375</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>-0.978775</td>\n",
       "      <td>-0.845878</td>\n",
       "      <td>-1.094578</td>\n",
       "      <td>-0.175704</td>\n",
       "      <td>2.339420</td>\n",
       "      <td>-1.796109</td>\n",
       "      <td>-1.192227</td>\n",
       "      <td>-1.337457</td>\n",
       "      <td>-1.370938</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-1.046252</td>\n",
       "      <td>-1.191779</td>\n",
       "      <td>-0.840162</td>\n",
       "      <td>-1.068150</td>\n",
       "      <td>-0.847752</td>\n",
       "      <td>-0.096545</td>\n",
       "      <td>0.683087</td>\n",
       "      <td>-1.181417</td>\n",
       "      <td>-1.260238</td>\n",
       "      <td>0.546071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.25</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.495115</td>\n",
       "      <td>0.697611</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>0.826880</td>\n",
       "      <td>0.804001</td>\n",
       "      <td>0.440498</td>\n",
       "      <td>0.573359</td>\n",
       "      <td>-0.444245</td>\n",
       "      <td>0.031072</td>\n",
       "      <td>-0.506180</td>\n",
       "      <td>-0.650424</td>\n",
       "      <td>-0.690690</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.698345</td>\n",
       "      <td>0.643769</td>\n",
       "      <td>2.462647</td>\n",
       "      <td>-0.805294</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.380594</td>\n",
       "      <td>-0.284469</td>\n",
       "      <td>-0.333719</td>\n",
       "      <td>-0.002896</td>\n",
       "      <td>1.591752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.75</td>\n",
       "      <td>-1.775665</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>2.302640</td>\n",
       "      <td>0.252248</td>\n",
       "      <td>1.483889</td>\n",
       "      <td>0.667087</td>\n",
       "      <td>0.622514</td>\n",
       "      <td>0.614894</td>\n",
       "      <td>0.796924</td>\n",
       "      <td>-0.659853</td>\n",
       "      <td>0.800578</td>\n",
       "      <td>1.831120</td>\n",
       "      <td>1.658558</td>\n",
       "      <td>1.559035</td>\n",
       "      <td>-1.775568</td>\n",
       "      <td>-0.141693</td>\n",
       "      <td>0.234675</td>\n",
       "      <td>-0.480760</td>\n",
       "      <td>0.377556</td>\n",
       "      <td>0.450430</td>\n",
       "      <td>-1.878515</td>\n",
       "      <td>-1.002143</td>\n",
       "      <td>0.108559</td>\n",
       "      <td>0.360541</td>\n",
       "      <td>0.008835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.065956</td>\n",
       "      <td>-0.415530</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>-0.176907</td>\n",
       "      <td>-0.603896</td>\n",
       "      <td>0.104720</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>0.989596</td>\n",
       "      <td>0.383103</td>\n",
       "      <td>1.014979</td>\n",
       "      <td>0.847919</td>\n",
       "      <td>0.768573</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.767926</td>\n",
       "      <td>-0.451110</td>\n",
       "      <td>0.529429</td>\n",
       "      <td>-0.411011</td>\n",
       "      <td>-0.415025</td>\n",
       "      <td>0.429890</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>-0.554857</td>\n",
       "      <td>-0.231774</td>\n",
       "      <td>0.277739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.25</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>2.195294</td>\n",
       "      <td>1.230029</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>1.108695</td>\n",
       "      <td>1.476052</td>\n",
       "      <td>0.365664</td>\n",
       "      <td>0.815756</td>\n",
       "      <td>-0.536360</td>\n",
       "      <td>0.306574</td>\n",
       "      <td>1.030563</td>\n",
       "      <td>0.869357</td>\n",
       "      <td>0.830057</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.206214</td>\n",
       "      <td>1.259956</td>\n",
       "      <td>1.358651</td>\n",
       "      <td>0.114701</td>\n",
       "      <td>0.125884</td>\n",
       "      <td>-0.517694</td>\n",
       "      <td>-0.076655</td>\n",
       "      <td>0.808832</td>\n",
       "      <td>0.784528</td>\n",
       "      <td>0.117781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>2.112675</td>\n",
       "      <td>-0.661303</td>\n",
       "      <td>0.802062</td>\n",
       "      <td>-1.026713</td>\n",
       "      <td>-0.470805</td>\n",
       "      <td>-1.256610</td>\n",
       "      <td>0.052721</td>\n",
       "      <td>0.532062</td>\n",
       "      <td>-0.672461</td>\n",
       "      <td>-1.049936</td>\n",
       "      <td>0.893475</td>\n",
       "      <td>1.659110</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.350438</td>\n",
       "      <td>-0.784948</td>\n",
       "      <td>-0.911407</td>\n",
       "      <td>-0.673866</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.771921</td>\n",
       "      <td>-0.069924</td>\n",
       "      <td>-0.554857</td>\n",
       "      <td>-0.231774</td>\n",
       "      <td>-0.197006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.55</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.502599</td>\n",
       "      <td>-0.065716</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>-0.865467</td>\n",
       "      <td>0.087954</td>\n",
       "      <td>-1.047725</td>\n",
       "      <td>0.375918</td>\n",
       "      <td>-0.191185</td>\n",
       "      <td>-0.983853</td>\n",
       "      <td>-0.505503</td>\n",
       "      <td>-0.654443</td>\n",
       "      <td>-0.695595</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.698345</td>\n",
       "      <td>-0.419423</td>\n",
       "      <td>0.196641</td>\n",
       "      <td>2.217546</td>\n",
       "      <td>-0.739570</td>\n",
       "      <td>0.205927</td>\n",
       "      <td>-0.394686</td>\n",
       "      <td>-0.702283</td>\n",
       "      <td>-0.410581</td>\n",
       "      <td>1.498481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.927485</td>\n",
       "      <td>-1.814787</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-1.026713</td>\n",
       "      <td>-1.212151</td>\n",
       "      <td>-1.256610</td>\n",
       "      <td>-1.702382</td>\n",
       "      <td>-2.721790</td>\n",
       "      <td>-2.607838</td>\n",
       "      <td>1.983574</td>\n",
       "      <td>1.799248</td>\n",
       "      <td>1.691160</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-1.185415</td>\n",
       "      <td>-1.398873</td>\n",
       "      <td>-1.815898</td>\n",
       "      <td>-1.331006</td>\n",
       "      <td>-0.955934</td>\n",
       "      <td>1.085348</td>\n",
       "      <td>0.466018</td>\n",
       "      <td>-1.402556</td>\n",
       "      <td>-1.984394</td>\n",
       "      <td>-1.886085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.25</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.375164</td>\n",
       "      <td>0.763433</td>\n",
       "      <td>0.558591</td>\n",
       "      <td>-0.608347</td>\n",
       "      <td>0.533421</td>\n",
       "      <td>-0.883090</td>\n",
       "      <td>0.219179</td>\n",
       "      <td>-0.238760</td>\n",
       "      <td>0.197851</td>\n",
       "      <td>-0.515667</td>\n",
       "      <td>-0.663488</td>\n",
       "      <td>-0.703444</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.693284</td>\n",
       "      <td>0.454216</td>\n",
       "      <td>-0.469507</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>0.558612</td>\n",
       "      <td>0.734797</td>\n",
       "      <td>1.080205</td>\n",
       "      <td>1.251110</td>\n",
       "      <td>0.994775</td>\n",
       "      <td>-0.635672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.25</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.375779</td>\n",
       "      <td>0.821824</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>0.398345</td>\n",
       "      <td>1.214271</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>0.911743</td>\n",
       "      <td>0.277483</td>\n",
       "      <td>0.087017</td>\n",
       "      <td>2.114685</td>\n",
       "      <td>1.940607</td>\n",
       "      <td>1.839310</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.141693</td>\n",
       "      <td>0.932342</td>\n",
       "      <td>2.876394</td>\n",
       "      <td>-0.016727</td>\n",
       "      <td>-0.198661</td>\n",
       "      <td>0.336775</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>0.071703</td>\n",
       "      <td>0.333888</td>\n",
       "      <td>0.766382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.887744</td>\n",
       "      <td>0.538894</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.364300</td>\n",
       "      <td>0.047257</td>\n",
       "      <td>-0.610433</td>\n",
       "      <td>-0.188461</td>\n",
       "      <td>0.599376</td>\n",
       "      <td>0.175156</td>\n",
       "      <td>-1.074328</td>\n",
       "      <td>-1.212512</td>\n",
       "      <td>-1.241756</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.414959</td>\n",
       "      <td>-0.168761</td>\n",
       "      <td>-0.985537</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>-0.198661</td>\n",
       "      <td>-0.498828</td>\n",
       "      <td>-0.355984</td>\n",
       "      <td>0.366554</td>\n",
       "      <td>0.533396</td>\n",
       "      <td>-0.019804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.768818</td>\n",
       "      <td>-0.818427</td>\n",
       "      <td>1.007758</td>\n",
       "      <td>-0.479060</td>\n",
       "      <td>-0.358614</td>\n",
       "      <td>-0.052106</td>\n",
       "      <td>0.612240</td>\n",
       "      <td>0.268879</td>\n",
       "      <td>-0.385348</td>\n",
       "      <td>-0.940168</td>\n",
       "      <td>-1.089241</td>\n",
       "      <td>-1.063845</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.067051</td>\n",
       "      <td>-0.621990</td>\n",
       "      <td>-0.945704</td>\n",
       "      <td>2.348974</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.040997</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>-0.260006</td>\n",
       "      <td>0.065289</td>\n",
       "      <td>-0.940237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.340774</td>\n",
       "      <td>0.039387</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>-0.165286</td>\n",
       "      <td>0.149549</td>\n",
       "      <td>-0.200473</td>\n",
       "      <td>0.356477</td>\n",
       "      <td>-0.494858</td>\n",
       "      <td>-0.124096</td>\n",
       "      <td>1.013285</td>\n",
       "      <td>0.847919</td>\n",
       "      <td>0.770863</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.211275</td>\n",
       "      <td>-0.224778</td>\n",
       "      <td>0.656359</td>\n",
       "      <td>0.640412</td>\n",
       "      <td>0.450430</td>\n",
       "      <td>0.537612</td>\n",
       "      <td>0.797511</td>\n",
       "      <td>0.219128</td>\n",
       "      <td>0.437394</td>\n",
       "      <td>-0.385855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.880628</td>\n",
       "      <td>-0.763221</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.830604</td>\n",
       "      <td>-1.015266</td>\n",
       "      <td>-0.845998</td>\n",
       "      <td>-1.194502</td>\n",
       "      <td>1.495717</td>\n",
       "      <td>-1.024492</td>\n",
       "      <td>1.339877</td>\n",
       "      <td>1.169160</td>\n",
       "      <td>1.078610</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.628763</td>\n",
       "      <td>-1.110300</td>\n",
       "      <td>-1.105338</td>\n",
       "      <td>-0.673866</td>\n",
       "      <td>-0.631388</td>\n",
       "      <td>-0.224959</td>\n",
       "      <td>-0.187713</td>\n",
       "      <td>-0.923422</td>\n",
       "      <td>-0.735720</td>\n",
       "      <td>0.703780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.75</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>2.187353</td>\n",
       "      <td>-0.276985</td>\n",
       "      <td>1.483889</td>\n",
       "      <td>1.142106</td>\n",
       "      <td>-1.187953</td>\n",
       "      <td>1.921563</td>\n",
       "      <td>-1.658641</td>\n",
       "      <td>-0.272670</td>\n",
       "      <td>1.590668</td>\n",
       "      <td>1.835863</td>\n",
       "      <td>1.659898</td>\n",
       "      <td>1.561324</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>-0.090111</td>\n",
       "      <td>-0.078546</td>\n",
       "      <td>-0.016727</td>\n",
       "      <td>0.774975</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>-0.557067</td>\n",
       "      <td>-0.038867</td>\n",
       "      <td>0.250436</td>\n",
       "      <td>-0.451997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.75</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.537822</td>\n",
       "      <td>0.152984</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.110085</td>\n",
       "      <td>-0.121031</td>\n",
       "      <td>-0.194616</td>\n",
       "      <td>-0.144113</td>\n",
       "      <td>-0.744881</td>\n",
       "      <td>1.152609</td>\n",
       "      <td>-0.514311</td>\n",
       "      <td>-0.662483</td>\n",
       "      <td>-0.703772</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.345377</td>\n",
       "      <td>0.351801</td>\n",
       "      <td>-0.441093</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>0.666793</td>\n",
       "      <td>-0.722791</td>\n",
       "      <td>-0.683270</td>\n",
       "      <td>1.287966</td>\n",
       "      <td>1.010819</td>\n",
       "      <td>-1.102280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.451735</td>\n",
       "      <td>-0.984575</td>\n",
       "      <td>1.185957</td>\n",
       "      <td>-0.689696</td>\n",
       "      <td>-0.997667</td>\n",
       "      <td>-0.380725</td>\n",
       "      <td>-1.015286</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>-0.320959</td>\n",
       "      <td>-0.824980</td>\n",
       "      <td>-0.666168</td>\n",
       "      <td>-0.709004</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.767926</td>\n",
       "      <td>-1.006187</td>\n",
       "      <td>-0.709677</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.810263</td>\n",
       "      <td>0.797511</td>\n",
       "      <td>-0.554857</td>\n",
       "      <td>-0.231774</td>\n",
       "      <td>-0.821456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.75</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.528498</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>1.217645</td>\n",
       "      <td>1.206572</td>\n",
       "      <td>0.984509</td>\n",
       "      <td>1.275643</td>\n",
       "      <td>-0.462466</td>\n",
       "      <td>1.003246</td>\n",
       "      <td>1.025142</td>\n",
       "      <td>0.861988</td>\n",
       "      <td>0.784599</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.345377</td>\n",
       "      <td>1.031362</td>\n",
       "      <td>0.818497</td>\n",
       "      <td>0.903268</td>\n",
       "      <td>0.342248</td>\n",
       "      <td>0.664808</td>\n",
       "      <td>0.237170</td>\n",
       "      <td>1.140540</td>\n",
       "      <td>0.945351</td>\n",
       "      <td>-0.773481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6.05</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.910124</td>\n",
       "      <td>-0.786577</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.631589</td>\n",
       "      <td>-0.712788</td>\n",
       "      <td>-0.412611</td>\n",
       "      <td>-0.387118</td>\n",
       "      <td>1.706769</td>\n",
       "      <td>-0.772212</td>\n",
       "      <td>-1.217975</td>\n",
       "      <td>-1.362580</td>\n",
       "      <td>-1.391869</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.698345</td>\n",
       "      <td>-0.988081</td>\n",
       "      <td>-0.917824</td>\n",
       "      <td>-0.279583</td>\n",
       "      <td>-0.739570</td>\n",
       "      <td>-0.375283</td>\n",
       "      <td>0.499672</td>\n",
       "      <td>-0.812853</td>\n",
       "      <td>-0.562865</td>\n",
       "      <td>0.215414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.907382</td>\n",
       "      <td>-0.624145</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.694054</td>\n",
       "      <td>-0.148529</td>\n",
       "      <td>-0.636463</td>\n",
       "      <td>0.742247</td>\n",
       "      <td>0.170186</td>\n",
       "      <td>-0.723128</td>\n",
       "      <td>-0.516683</td>\n",
       "      <td>-0.668177</td>\n",
       "      <td>-0.711948</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.141693</td>\n",
       "      <td>-0.666124</td>\n",
       "      <td>-0.780922</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>-0.739570</td>\n",
       "      <td>0.468840</td>\n",
       "      <td>0.285968</td>\n",
       "      <td>-0.628570</td>\n",
       "      <td>-0.318072</td>\n",
       "      <td>0.050765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.25</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.883263</td>\n",
       "      <td>0.804307</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>0.848669</td>\n",
       "      <td>1.544247</td>\n",
       "      <td>0.392344</td>\n",
       "      <td>1.286578</td>\n",
       "      <td>-0.681110</td>\n",
       "      <td>0.933051</td>\n",
       "      <td>-1.072973</td>\n",
       "      <td>-1.210502</td>\n",
       "      <td>-1.237178</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>1.041192</td>\n",
       "      <td>1.401980</td>\n",
       "      <td>0.449744</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.216882</td>\n",
       "      <td>-0.286152</td>\n",
       "      <td>1.251110</td>\n",
       "      <td>0.994775</td>\n",
       "      <td>-0.605689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3.75</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.868295</td>\n",
       "      <td>-0.887965</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>0.428851</td>\n",
       "      <td>-0.832679</td>\n",
       "      <td>2.164285</td>\n",
       "      <td>-0.602785</td>\n",
       "      <td>0.392879</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>-1.022494</td>\n",
       "      <td>-1.169635</td>\n",
       "      <td>-1.204473</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>-0.628763</td>\n",
       "      <td>-0.636701</td>\n",
       "      <td>-0.737941</td>\n",
       "      <td>-0.673866</td>\n",
       "      <td>-0.198661</td>\n",
       "      <td>-0.266344</td>\n",
       "      <td>-0.276897</td>\n",
       "      <td>-0.849709</td>\n",
       "      <td>-0.617982</td>\n",
       "      <td>0.075836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2.00</td>\n",
       "      <td>-0.779118</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.889223</td>\n",
       "      <td>0.111580</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>0.080213</td>\n",
       "      <td>-0.732586</td>\n",
       "      <td>0.052011</td>\n",
       "      <td>-1.003135</td>\n",
       "      <td>-0.301013</td>\n",
       "      <td>0.709800</td>\n",
       "      <td>1.025481</td>\n",
       "      <td>0.860983</td>\n",
       "      <td>0.781001</td>\n",
       "      <td>-0.779417</td>\n",
       "      <td>0.136633</td>\n",
       "      <td>-0.021080</td>\n",
       "      <td>-0.561847</td>\n",
       "      <td>-0.016727</td>\n",
       "      <td>0.342248</td>\n",
       "      <td>-0.111152</td>\n",
       "      <td>-0.174252</td>\n",
       "      <td>-0.370575</td>\n",
       "      <td>-0.038347</td>\n",
       "      <td>0.740912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.50</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.173240</td>\n",
       "      <td>1.460938</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>2.507605</td>\n",
       "      <td>-0.203525</td>\n",
       "      <td>1.244151</td>\n",
       "      <td>-0.822704</td>\n",
       "      <td>-0.842057</td>\n",
       "      <td>1.054441</td>\n",
       "      <td>-1.033335</td>\n",
       "      <td>-1.167625</td>\n",
       "      <td>-1.173077</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>0.345377</td>\n",
       "      <td>1.846155</td>\n",
       "      <td>2.451319</td>\n",
       "      <td>0.508984</td>\n",
       "      <td>0.234066</td>\n",
       "      <td>0.087251</td>\n",
       "      <td>0.504720</td>\n",
       "      <td>0.145416</td>\n",
       "      <td>0.386676</td>\n",
       "      <td>1.385265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.75</td>\n",
       "      <td>0.217428</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.725759</td>\n",
       "      <td>2.137741</td>\n",
       "      <td>1.185957</td>\n",
       "      <td>-0.083937</td>\n",
       "      <td>1.963316</td>\n",
       "      <td>-0.701536</td>\n",
       "      <td>0.603735</td>\n",
       "      <td>-0.528768</td>\n",
       "      <td>0.439047</td>\n",
       "      <td>-0.498388</td>\n",
       "      <td>0.515624</td>\n",
       "      <td>0.459518</td>\n",
       "      <td>0.217431</td>\n",
       "      <td>2.641565</td>\n",
       "      <td>2.293160</td>\n",
       "      <td>0.229569</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>0.666793</td>\n",
       "      <td>0.118289</td>\n",
       "      <td>0.286810</td>\n",
       "      <td>1.914526</td>\n",
       "      <td>1.257423</td>\n",
       "      <td>-0.177144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4.00</td>\n",
       "      <td>-6.758400</td>\n",
       "      <td>-7.348469</td>\n",
       "      <td>-0.928919</td>\n",
       "      <td>-1.899719</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-1.026713</td>\n",
       "      <td>-1.212151</td>\n",
       "      <td>-1.256610</td>\n",
       "      <td>-1.702382</td>\n",
       "      <td>-2.721790</td>\n",
       "      <td>-2.607838</td>\n",
       "      <td>2.163471</td>\n",
       "      <td>1.976784</td>\n",
       "      <td>1.864166</td>\n",
       "      <td>-6.758413</td>\n",
       "      <td>-1.254997</td>\n",
       "      <td>-1.398873</td>\n",
       "      <td>-1.815898</td>\n",
       "      <td>-1.331006</td>\n",
       "      <td>-0.955934</td>\n",
       "      <td>-4.988441</td>\n",
       "      <td>-6.443172</td>\n",
       "      <td>-1.660551</td>\n",
       "      <td>-4.674781</td>\n",
       "      <td>-2.589195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean(p$p2)    g_lab1    a_lab1   ut_lab1  act_lab1  norm_log(dis_lab1)  \\\n",
       "0         2.00  0.217428  0.136083 -0.455061 -0.761629           -0.123236   \n",
       "1         2.25  0.217428  0.136083 -0.817620 -0.125169           -0.664505   \n",
       "2         1.25  0.217428  0.136083  1.016240  2.141457            1.727227   \n",
       "3         3.00  0.217428  0.136083 -0.911666 -0.877879           -1.589670   \n",
       "4         2.00  0.217428  0.136083 -0.885970 -0.428801            0.260792   \n",
       "5         3.00  0.217428  0.136083 -0.223061  0.834033            0.802062   \n",
       "6         3.50  0.217428  0.136083  0.525164  1.814468            1.834012   \n",
       "7         1.00  0.217428  0.136083  0.259429  0.711943            1.007758   \n",
       "8         3.50  0.217428  0.136083 -0.911666 -0.877879           -1.589670   \n",
       "9         2.50  0.217428  0.136083  0.963889  2.519935            1.185957   \n",
       "10        5.55  0.217428  0.136083 -0.909336 -0.739865           -1.589670   \n",
       "11        1.50  0.217428  0.136083  0.210770 -1.174611            0.558591   \n",
       "12        3.50  0.217428  0.136083 -0.448061 -0.285478            0.802062   \n",
       "13        1.00  0.217428  0.136083 -0.889654 -1.018548           -0.123236   \n",
       "14        2.50  0.217428  0.136083  1.186075 -0.547175            1.611097   \n",
       "15        6.00 -1.775665  0.136083 -0.905267 -0.855585           -0.664505   \n",
       "16        4.00  0.217428  0.136083  0.193974  1.365390           -0.123236   \n",
       "17        2.00  0.217428  0.136083 -0.895874 -0.883718           -0.123236   \n",
       "18        0.50  0.217428  0.136083 -0.257496  0.043103            0.558591   \n",
       "19        2.00  0.217428  0.136083  0.338202  0.105210           -0.664505   \n",
       "20        2.50  0.217428  0.136083  0.242991  0.082915           -0.123236   \n",
       "21        5.75  0.217428  0.136083 -0.256895 -1.705436           -0.123236   \n",
       "22        1.25  0.217428  0.136083 -0.461146 -0.098097            0.802062   \n",
       "23        3.50  0.217428  0.136083 -0.886042  0.639751           -1.589670   \n",
       "24        3.50  0.217428  0.136083 -0.125654  0.410965            0.802062   \n",
       "25        2.50  0.217428  0.136083  2.163171 -0.150648            1.343199   \n",
       "26        1.00  0.217428  0.136083 -0.894458 -0.257875           -0.664505   \n",
       "27        5.50  0.217428  0.136083 -0.327727  1.612754           -0.123236   \n",
       "28        2.75  0.217428  0.136083  1.726221  0.209251           -0.123236   \n",
       "29        2.75  0.217428  0.136083 -0.907453 -1.196375           -0.664505   \n",
       "30        3.25  0.217428  0.136083 -0.495115  0.697611           -0.664505   \n",
       "31        1.75 -1.775665  0.136083  2.302640  0.252248            1.483889   \n",
       "32        2.50  0.217428  0.136083  1.065956 -0.415530            0.260792   \n",
       "33        2.25  0.217428  0.136083  2.195294  1.230029            0.260792   \n",
       "34        2.00  0.217428  0.136083  2.112675 -0.661303            0.802062   \n",
       "35        4.55  0.217428  0.136083 -0.502599 -0.065716            0.260792   \n",
       "36        0.50  0.217428  0.136083 -0.927485 -1.814787           -1.589670   \n",
       "37        2.25  0.217428  0.136083  0.375164  0.763433            0.558591   \n",
       "38        2.25  0.217428  0.136083  1.375779  0.821824           -0.123236   \n",
       "39        4.50  0.217428  0.136083 -0.887744  0.538894           -1.589670   \n",
       "40        5.00  0.217428  0.136083 -0.768818 -0.818427            1.007758   \n",
       "41        2.50  0.217428  0.136083  0.340774  0.039387            0.260792   \n",
       "42        1.50  0.217428  0.136083 -0.880628 -0.763221           -0.123236   \n",
       "43        4.75  0.217428  0.136083  2.187353 -0.276985            1.483889   \n",
       "44        2.75  0.217428  0.136083 -0.537822  0.152984           -0.123236   \n",
       "45        3.50  0.217428  0.136083 -0.451735 -0.984575            1.185957   \n",
       "46        4.75  0.217428  0.136083  0.528498  0.481565           -0.664505   \n",
       "47        6.05  0.217428  0.136083 -0.910124 -0.786577           -1.589670   \n",
       "48        4.00  0.217428  0.136083 -0.907382 -0.624145           -1.589670   \n",
       "49        4.25  0.217428  0.136083 -0.883263  0.804307           -1.589670   \n",
       "50        3.75  0.217428  0.136083 -0.868295 -0.887965            0.260792   \n",
       "51        2.00 -0.779118  0.136083 -0.889223  0.111580           -0.664505   \n",
       "52        4.50  0.217428  0.136083  1.173240  1.460938            0.260792   \n",
       "53        1.75  0.217428  0.136083  0.725759  2.137741            1.185957   \n",
       "54        4.00 -6.758400 -7.348469 -0.928919 -1.899719           -1.589670   \n",
       "\n",
       "     ct_lab1   rt_lab1  ctr_lab1  rtr_lab1  err_lab1  cer_lab1  actq1_lab1  \\\n",
       "0  -0.049073 -0.890975  0.786036 -0.874950 -0.637584  0.383103   -0.514311   \n",
       "1  -0.603989 -0.385012 -0.690473 -0.336087  0.553319 -0.195346   -1.196970   \n",
       "2  -0.647569  0.618115 -1.033409 -0.374360  1.189007 -0.304597    0.559647   \n",
       "3  -0.537167  0.542220 -0.117179  3.331465 -0.191185 -0.496710   -1.108546   \n",
       "4   0.253080 -0.496104  0.812065 -0.275336 -0.042385  0.195740    1.020738   \n",
       "5   1.102885 -0.547800  0.596023 -0.989770 -0.457405  0.725633   -0.137240   \n",
       "6   3.475076  0.554319  1.625480 -0.308141 -0.973142  2.074644   -0.264286   \n",
       "7   0.510200 -0.317917  0.142463 -0.698772 -0.682122  1.538945   -0.123689   \n",
       "8  -0.470344 -0.636893  0.038346 -0.051771  1.214819 -0.448682   -0.939152   \n",
       "9  -0.329437  3.794682 -0.881138  1.618888 -0.240785  0.224240   -0.280886   \n",
       "10 -0.946816 -0.714988 -1.092625 -0.445439  2.339420 -1.288383   -1.218991   \n",
       "11 -0.521187 -0.774383  0.401454  0.067909 -0.191185 -0.107735    1.026159   \n",
       "12  0.011938  0.656612  0.273260  1.691789 -0.339985  0.407908   -0.820915   \n",
       "13 -0.691148 -1.173654 -0.350792 -1.574197 -0.191185  0.169879   -1.204423   \n",
       "14  0.129604 -0.451007  0.776275 -0.052379  0.138806  0.738828    0.696179   \n",
       "15 -0.820435 -0.349814 -0.786782  0.719162  0.652519 -0.983853    0.310977   \n",
       "16 -0.528451  1.681737 -0.893502  0.895948 -0.697306  0.075933   -0.506180   \n",
       "17 -0.371564 -1.212151  0.276514 -1.702382  1.777120  0.360936    1.020738   \n",
       "18  0.574117  0.280439  0.702743  0.550274 -0.444245 -0.448682   -0.440456   \n",
       "19 -0.513924  0.043957 -0.648176  0.134735 -0.488784  0.294963   -0.213806   \n",
       "20 -0.458723 -0.343215 -0.575294 -0.417493 -1.118905  1.316749   -0.141644   \n",
       "21 -0.845130 -1.127457  0.965638 -0.424176  2.339420 -2.128084    0.366199   \n",
       "22 -0.930837  0.508123 -1.130368  1.097035 -0.191185 -1.763387   -0.510246   \n",
       "23 -0.275688  1.350661 -0.553169  1.256203 -0.081357  0.070656    1.021755   \n",
       "24  3.005867 -1.069162  2.893105 -1.520736 -0.881534  1.614418    0.695501   \n",
       "25  2.122651 -0.722687  3.025203 -0.881633  0.138806  0.725633   -0.990309   \n",
       "26 -0.646116  0.196846 -0.705440  0.813934  0.652519 -0.848741    1.024465   \n",
       "27  0.251627  1.504650 -0.391137  0.565461 -0.534841  0.708216   -0.514311   \n",
       "28 -0.717296  0.487224 -0.907818  0.660233  0.419703  0.031072   -0.508891   \n",
       "29 -0.978775 -0.845878 -1.094578 -0.175704  2.339420 -1.796109   -1.192227   \n",
       "30  0.826880  0.804001  0.440498  0.573359 -0.444245  0.031072   -0.506180   \n",
       "31  0.667087  0.622514  0.614894  0.796924 -0.659853  0.800578    1.831120   \n",
       "32 -0.176907 -0.603896  0.104720 -0.500723  0.989596  0.383103    1.014979   \n",
       "33  1.108695  1.476052  0.365664  0.815756 -0.536360  0.306574    1.030563   \n",
       "34 -1.026713 -0.470805 -1.256610  0.052721  0.532062 -0.672461   -1.049936   \n",
       "35 -0.865467  0.087954 -1.047725  0.375918 -0.191185 -0.983853   -0.505503   \n",
       "36 -1.026713 -1.212151 -1.256610 -1.702382 -2.721790 -2.607838    1.983574   \n",
       "37 -0.608347  0.533421 -0.883090  0.219179 -0.238760  0.197851   -0.515667   \n",
       "38  0.398345  1.214271 -0.011760  0.911743  0.277483  0.087017    2.114685   \n",
       "39 -0.364300  0.047257 -0.610433 -0.188461  0.599376  0.175156   -1.074328   \n",
       "40 -0.479060 -0.358614 -0.052106  0.612240  0.268879 -0.385348   -0.940168   \n",
       "41 -0.165286  0.149549 -0.200473  0.356477 -0.494858 -0.124096    1.013285   \n",
       "42 -0.830604 -1.015266 -0.845998 -1.194502  1.495717 -1.024492    1.339877   \n",
       "43  1.142106 -1.187953  1.921563 -1.658641 -0.272670  1.590668    1.835863   \n",
       "44 -0.110085 -0.121031 -0.194616 -0.144113 -0.744881  1.152609   -0.514311   \n",
       "45 -0.689696 -0.997667 -0.380725 -1.015286  0.039100 -0.320959   -0.824980   \n",
       "46  1.217645  1.206572  0.984509  1.275643 -0.462466  1.003246    1.025142   \n",
       "47 -0.631589 -0.712788 -0.412611 -0.387118  1.706769 -0.772212   -1.217975   \n",
       "48 -0.694054 -0.148529 -0.636463  0.742247  0.170186 -0.723128   -0.516683   \n",
       "49  0.848669  1.544247  0.392344  1.286578 -0.681110  0.933051   -1.072973   \n",
       "50  0.428851 -0.832679  2.164285 -0.602785  0.392879  0.510826   -1.022494   \n",
       "51  0.080213 -0.732586  0.052011 -1.003135 -0.301013  0.709800    1.025481   \n",
       "52  2.507605 -0.203525  1.244151 -0.822704 -0.842057  1.054441   -1.033335   \n",
       "53 -0.083937  1.963316 -0.701536  0.603735 -0.528768  0.439047   -0.498388   \n",
       "54 -1.026713 -1.212151 -1.256610 -1.702382 -2.721790 -2.607838    2.163471   \n",
       "\n",
       "    actq2_lab1  actq3_lab1  mean(qg$_lab1)  max(qat$_lab1)  mean(qact$_lab1)  \\\n",
       "0    -0.665163   -0.709985        0.217431       -0.489600         -0.852848   \n",
       "1    -1.260748   -1.289504        0.217431       -0.350438         -0.503732   \n",
       "2     0.838540    0.771844        0.217431        0.275796          0.436110   \n",
       "3    -1.255053   -1.286561        0.217431       -0.837508         -0.257596   \n",
       "4     0.854618    0.777731        0.217431       -0.350438         -0.519575   \n",
       "5    -0.287981    0.113180        0.217431        0.484540          0.449124   \n",
       "6    -0.396178    0.761379        0.217431        4.311521          2.609458   \n",
       "7    -0.268217   -0.290390        0.217431        1.528262          1.082286   \n",
       "8    -1.083546   -1.122386        0.217431       -0.698345         -0.886232   \n",
       "9     0.404077    0.386915        0.217431        1.180355          2.582864   \n",
       "10   -1.361240   -1.390234        0.217431       -0.976671         -1.111997   \n",
       "11    0.854953    0.772498        0.217431       -0.211275         -0.958658   \n",
       "12   -0.674207   -0.718488        0.217431        0.067051          0.236938   \n",
       "13   -1.341142   -1.371265        0.217431       -0.837508         -1.242704   \n",
       "14    0.569220    0.765957        0.217431       -0.072112         -0.185170   \n",
       "15    0.151171    0.084401       -1.775568       -0.907089         -0.812674   \n",
       "16   -0.649754   -0.685130        0.217431        0.554121          1.012124   \n",
       "17    0.858638    0.774787        0.217431       -0.976671         -1.143684   \n",
       "18   -0.589793   -0.452603        0.217431       -0.350438          0.299179   \n",
       "19    0.841554    0.765630        0.217431       -0.002530         -0.079360   \n",
       "20   -0.291666   -0.345333        0.217431        2.224077         -0.043147   \n",
       "21    0.842894    0.770209        0.217431       -0.976671         -1.284575   \n",
       "22   -0.660473   -0.694614        0.217431       -0.628763         -0.427345   \n",
       "23    0.858303    0.782309        0.217431        0.206214          0.849165   \n",
       "24    1.226106    1.142056        0.217431        0.554121          0.245426   \n",
       "25   -0.444749   -0.449660        0.217431       -0.628763          0.269191   \n",
       "26    0.856293    0.779039        0.217431       -0.837508         -0.442622   \n",
       "27   -0.655783   -0.692325        0.217431        1.389099          1.501565   \n",
       "28   -0.657458   -0.699193        0.217431        0.484540         -0.091808   \n",
       "29   -1.337457   -1.370938        0.217431       -1.046252         -1.191779   \n",
       "30   -0.650424   -0.690690        0.217431       -0.698345          0.643769   \n",
       "31    1.658558    1.559035       -1.775568       -0.141693          0.234675   \n",
       "32    0.847919    0.768573        0.217431       -0.767926         -0.451110   \n",
       "33    0.869357    0.830057        0.217431        0.206214          1.259956   \n",
       "34    0.893475    1.659110        0.217431       -0.350438         -0.784948   \n",
       "35   -0.654443   -0.695595        0.217431       -0.698345         -0.419423   \n",
       "36    1.799248    1.691160        0.217431       -1.185415         -1.398873   \n",
       "37   -0.663488   -0.703444        0.217431        0.693284          0.454216   \n",
       "38    1.940607    1.839310        0.217431       -0.141693          0.932342   \n",
       "39   -1.212512   -1.241756        0.217431        0.414959         -0.168761   \n",
       "40   -1.089241   -1.063845        0.217431        0.067051         -0.621990   \n",
       "41    0.847919    0.770863        0.217431       -0.211275         -0.224778   \n",
       "42    1.169160    1.078610        0.217431       -0.628763         -1.110300   \n",
       "43    1.659898    1.561324        0.217431       -0.002530         -0.090111   \n",
       "44   -0.662483   -0.703772        0.217431        0.345377          0.351801   \n",
       "45   -0.666168   -0.709004        0.217431       -0.767926         -1.006187   \n",
       "46    0.861988    0.784599        0.217431        0.345377          1.031362   \n",
       "47   -1.362580   -1.391869        0.217431       -0.698345         -0.988081   \n",
       "48   -0.668177   -0.711948        0.217431       -0.141693         -0.666124   \n",
       "49   -1.210502   -1.237178        0.217431        1.041192          1.401980   \n",
       "50   -1.169635   -1.204473        0.217431       -0.628763         -0.636701   \n",
       "51    0.860983    0.781001       -0.779417        0.136633         -0.021080   \n",
       "52   -1.167625   -1.173077        0.217431        0.345377          1.846155   \n",
       "53    0.515624    0.459518        0.217431        2.641565          2.293160   \n",
       "54    1.976784    1.864166       -6.758413       -1.254997         -1.398873   \n",
       "\n",
       "    mean(qavt$_lab1)  max(qme$_lab1)  max(qmce$_lab1)  mean(qmsr$_lab1)  \\\n",
       "0          -0.743363       -0.148155         0.017702          0.475535   \n",
       "1           0.468175       -0.805294        -0.523207         -0.975358   \n",
       "2           0.508278        1.954690        -0.523207         -0.207310   \n",
       "3           1.066704       -0.673866        -0.631388          0.653245   \n",
       "4          -0.314631       -0.148155         0.017702          0.578996   \n",
       "5           0.280275       -0.148155         0.017702         -0.127584   \n",
       "6           0.865472        0.246129         4.344975          0.193146   \n",
       "7          -0.103934       -0.016727         2.722248          0.455451   \n",
       "8          -0.533004       -0.016727        -0.631388         -0.011951   \n",
       "9           0.545034        1.034696        -0.090479         -0.273038   \n",
       "10         -0.934640       -1.068150        -0.847752         -3.661702   \n",
       "11         -0.739180       -0.542439        -0.415025          0.606992   \n",
       "12          0.661192       -0.279583         0.017702         -0.212179   \n",
       "13         -1.227669       -0.542439        -0.415025          0.753663   \n",
       "14          0.120644        0.771840        -0.306843          0.202275   \n",
       "15          0.219207       -0.936722        -0.847752         -0.975358   \n",
       "16         -0.050041        0.377556        -0.198661          0.083599   \n",
       "17         -0.764273        3.794680        -0.631388          0.274090   \n",
       "18          1.096272        2.480402        -0.306843         -0.119672   \n",
       "19         -0.155502       -0.542439         0.666793          0.523614   \n",
       "20         -0.567184       -0.805294         3.154975          0.897901   \n",
       "21         -1.317190        0.771840        -0.847752          0.389114   \n",
       "22          0.613374       -0.805294        -0.739570          0.809654   \n",
       "23          0.578307        1.166123        -0.523207          0.416501   \n",
       "24         -0.164137       -0.411011         1.748611          0.504748   \n",
       "25          1.016213       -0.148155        -0.198661          0.498053   \n",
       "26          1.845819       -0.542439        -0.523207          0.337992   \n",
       "27         -0.211464       -0.411011         0.991339          0.158456   \n",
       "28         -0.489942       -0.411011        -0.415025          0.535786   \n",
       "29         -0.840162       -1.068150        -0.847752         -0.096545   \n",
       "30          2.462647       -0.805294        -0.523207          0.380594   \n",
       "31         -0.480760        0.377556         0.450430         -1.878515   \n",
       "32          0.529429       -0.411011        -0.415025          0.429890   \n",
       "33          1.358651        0.114701         0.125884         -0.517694   \n",
       "34         -0.911407       -0.673866        -0.523207          0.771921   \n",
       "35          0.196641        2.217546        -0.739570          0.205927   \n",
       "36         -1.815898       -1.331006        -0.955934          1.085348   \n",
       "37         -0.469507       -0.542439         0.558612          0.734797   \n",
       "38          2.876394       -0.016727        -0.198661          0.336775   \n",
       "39         -0.985537       -0.542439        -0.198661         -0.498828   \n",
       "40         -0.945704        2.348974        -0.523207          0.040997   \n",
       "41          0.656359        0.640412         0.450430          0.537612   \n",
       "42         -1.105338       -0.673866        -0.631388         -0.224959   \n",
       "43         -0.078546       -0.016727         0.774975          0.005699   \n",
       "44         -0.441093       -0.542439         0.666793         -0.722791   \n",
       "45         -0.709677       -0.148155        -0.523207          0.810263   \n",
       "46          0.818497        0.903268         0.342248          0.664808   \n",
       "47         -0.917824       -0.279583        -0.739570         -0.375283   \n",
       "48         -0.780922       -0.542439        -0.739570          0.468840   \n",
       "49          0.449744       -0.542439         0.017702          0.216882   \n",
       "50         -0.737941       -0.673866        -0.198661         -0.266344   \n",
       "51         -0.561847       -0.016727         0.342248         -0.111152   \n",
       "52          2.451319        0.508984         0.234066          0.087251   \n",
       "53          0.229569       -0.148155         0.666793          0.118289   \n",
       "54         -1.815898       -1.331006        -0.955934         -4.988441   \n",
       "\n",
       "    mean(qc$_lab1)  sum(qat$_lab1)  norm_log(sum(qat$_lab1))  avgtime_lab1  \n",
       "0         0.930445       -0.554857                 -0.231774     -0.390801  \n",
       "1         0.456763       -0.370575                 -0.038347      0.348930  \n",
       "2        -0.793488        0.366554                  0.533396      1.668700  \n",
       "3         0.519023       -0.739140                 -0.459359     -0.220591  \n",
       "4         0.505561       -0.481144                 -0.150910      0.074525  \n",
       "5        -0.066559        0.440267                  0.578680      0.190098  \n",
       "6         0.299430        3.241359                  1.662132     -0.970884  \n",
       "7         0.113491        1.435392                  1.073311     -0.787474  \n",
       "8         0.042817       -0.849709                 -0.617982      0.102401  \n",
       "9        -0.788439        1.361679                  1.042388      0.534182  \n",
       "10        0.711693       -1.070848                 -1.009106      1.611617  \n",
       "11        0.881646       -0.260006                  0.065289     -1.483413  \n",
       "12       -0.313075        0.145416                  0.386676     -0.680124  \n",
       "13        0.772270       -0.960278                 -0.798859      0.098353  \n",
       "14        0.192578       -0.149436                  0.161162     -0.677507  \n",
       "15       -0.879306       -1.181417                 -1.260238      2.065196  \n",
       "16        0.065534        0.513980                  0.622541      0.617778  \n",
       "17       -0.000092       -1.070848                 -1.009106      1.090601  \n",
       "18       -0.220526       -0.038867                  0.250436     -0.030432  \n",
       "19        0.061327        0.550837                  0.644018     -0.652784  \n",
       "20        0.334767        1.214253                  0.978473     -1.116212  \n",
       "21       -0.142280       -1.255130                 -1.459747     -1.565690  \n",
       "22        0.749554       -0.739140                 -0.459359      1.586931  \n",
       "23        0.261569        0.808832                  0.784528     -0.392760  \n",
       "24       -0.076655        0.550837                  0.644018     -0.357477  \n",
       "25        0.348228       -0.260006                  0.065289      0.078115  \n",
       "26        0.389455       -0.997135                 -0.865233      2.696585  \n",
       "27       -0.089275        2.504229                  1.453049     -0.787902  \n",
       "28        0.362531        0.108559                  0.360541     -0.043074  \n",
       "29        0.683087       -1.181417                 -1.260238      0.546071  \n",
       "30       -0.284469       -0.333719                 -0.002896      1.591752  \n",
       "31       -1.002143        0.108559                  0.360541      0.008835  \n",
       "32        0.008322       -0.554857                 -0.231774      0.277739  \n",
       "33       -0.076655        0.808832                  0.784528      0.117781  \n",
       "34       -0.069924       -0.554857                 -0.231774     -0.197006  \n",
       "35       -0.394686       -0.702283                 -0.410581      1.498481  \n",
       "36        0.466018       -1.402556                 -1.984394     -1.886085  \n",
       "37        1.080205        1.251110                  0.994775     -0.635672  \n",
       "38       -0.005981        0.071703                  0.333888      0.766382  \n",
       "39       -0.355984        0.366554                  0.533396     -0.019804  \n",
       "40        0.522388       -0.260006                  0.065289     -0.940237  \n",
       "41        0.797511        0.219128                  0.437394     -0.385855  \n",
       "42       -0.187713       -0.923422                 -0.735720      0.703780  \n",
       "43       -0.557067       -0.038867                  0.250436     -0.451997  \n",
       "44       -0.683270        1.287966                  1.010819     -1.102280  \n",
       "45        0.797511       -0.554857                 -0.231774     -0.821456  \n",
       "46        0.237170        1.140540                  0.945351     -0.773481  \n",
       "47        0.499672       -0.812853                 -0.562865      0.215414  \n",
       "48        0.285968       -0.628570                 -0.318072      0.050765  \n",
       "49       -0.286152        1.251110                  0.994775     -0.605689  \n",
       "50       -0.276897       -0.849709                 -0.617982      0.075836  \n",
       "51       -0.174252       -0.370575                 -0.038347      0.740912  \n",
       "52        0.504720        0.145416                  0.386676      1.385265  \n",
       "53        0.286810        1.914526                  1.257423     -0.177144  \n",
       "54       -6.443172       -1.660551                 -4.674781     -2.589195  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso7 = dfFinlab1.loc[dfFinlab1['curso']=='7']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene la columna con el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso7.reset_index()['mean(p$p2)'])\n",
    "\n",
    "# Se obtiene los datos del curso X para el lab Y normalizados Excluyendo la fila mean(p$p2)\n",
    "# ------------               Función que normaliza la data      , el nombre de las col a colocar en el DF  [desde cual columna hasta cual]\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso7),columns=dfLab1Curso7.columns)[dfLab1Curso7.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc7 = pd.concat([promCurso,DFnormalizado],axis=1)\n",
    "datalab1Normc7\n",
    "\n",
    "# Sentencia anterior\n",
    "#datalab1Normc7 = dfLab1Curso7[['mean(p$p2)']].join(pd.DataFrame(scaler1.fit_transform(dfLab1Curso7), columns=dfLab1Curso7.columns)[dfLab1Curso7.columns[1:26]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso13 = dfFinlab1.loc[dfFinlab1['curso']=='13']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso13.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso13),columns=dfLab1Curso13.columns)[dfLab1Curso13.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc13 = pd.concat([promCurso,DFnormalizado],axis=1)\n",
    "\n",
    "\n",
    "# Sentencia anterior\n",
    "#datalab1Normc13 = dfLab1Curso13[['mean(p$p2)']].join(pd.DataFrame(scaler2.fit_transform(dfLab1Curso13), columns=dfLab1Curso13.columns)[dfLab1Curso13.columns[1:26]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso19 = dfFinlab1.loc[dfFinlab1['curso']=='19']\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso19.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso19),columns=dfLab1Curso19.columns)[dfLab1Curso19.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc19 = pd.concat([promCurso,DFnormalizado],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso24 = dfFinlab1.loc[dfFinlab1['curso']=='24']\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso24.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso24),columns=dfLab1Curso24.columns)[dfLab1Curso24.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc24 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso30 = dfFinlab1.loc[dfFinlab1['curso']=='30']\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso30.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso30),columns=dfLab1Curso30.columns)[dfLab1Curso30.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc30 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso36 = dfFinlab1.loc[dfFinlab1['curso']=='36']\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso36.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso36),columns=dfLab1Curso36.columns)[dfLab1Curso36.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc36 = pd.concat([promCurso,DFnormalizado],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se unen los datos del laboratorio 1\n",
    "datalab1_norm = pd.concat([datalab1Normc7,datalab1Normc13,datalab1Normc19,datalab1Normc24,datalab1Normc30,datalab1Normc36],axis=0)\n",
    "datalab1_norm = datalab1_norm.reset_index(drop = True)\n",
    "#datalab1_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "13\n",
      "19\n",
      "24\n",
      "30\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "#LAB 2\n",
    "datalab2_all,cursoData = get_custom_dataframe(DATA, [2], ['p1p2','p2p2'], 'all', labels=True, index=None)\n",
    " \n",
    "datalab2 = copy.deepcopy(datalab2_all)\n",
    " \n",
    "# Remove questionsdifficulty\n",
    "remove_col(datalab2, 'qd?')\n",
    "# Group columns\n",
    "datalab2_all = apply(datalab2_all, ['p1p2','p2p2'], statistics.mean)\n",
    "datalab2 = apply(datalab2, ['p1p2','p2p2'], statistics.mean)\n",
    "datalab2 = apply(datalab2, 'dis_lab2', norm_log)\n",
    "datalab2 = apply(datalab2, 'qg?', statistics.mean)\n",
    "datalab2 = apply(datalab2, 'qat?', sum, replace=False)\n",
    "datalab2 = apply(datalab2, 'sum(qat$_lab2)', norm_log, replace=False)\n",
    "datalab2 = apply(datalab2, 'qat?', max)\n",
    "datalab2 = apply(datalab2, 'qact?', statistics.mean)\n",
    "datalab2 = apply(datalab2, 'qavt?', statistics.mean)\n",
    "datalab2 = apply(datalab2, 'qme?', max)\n",
    "datalab2 = apply(datalab2, 'qmce?', max)\n",
    "datalab2 = apply(datalab2, 'qmsr?', statistics.mean)\n",
    "datalab2 = apply(datalab2, 'qc?', statistics.mean)\n",
    "aux = datalab2['act_lab2'] / datalab2['sum(qat$_lab2)']\n",
    "for i in range(len(aux)):\n",
    "    if not aux[i] > 0:\n",
    "        aux[i] = 0\n",
    "datalab2['avgtime_lab2'] = aux\n",
    "datalab2 = datalab2.round(4)\n",
    "\n",
    "# Se transforma a dataframe la info de ese curso en particular\n",
    "cursoDF = pd.DataFrame(cursoData,columns=['curso'])\n",
    "\n",
    "# Se concatenan los dos dataframes \n",
    "dfFinlab2 = pd.concat([datalab2,cursoDF],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso7 = dfFinlab2.loc[dfFinlab2['curso']=='7']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso7.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso7),columns=dfLab2Curso7.columns)[dfLab2Curso7.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc7 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso13 = dfFinlab2.loc[dfFinlab2['curso']=='13']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso13.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso13),columns=dfLab2Curso13.columns)[dfLab2Curso13.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc13 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso19 = dfFinlab2.loc[dfFinlab2['curso']=='19']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso19.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso19),columns=dfLab2Curso19.columns)[dfLab2Curso19.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc19 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso24 = dfFinlab2.loc[dfFinlab2['curso']=='24']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso24.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso24),columns=dfLab2Curso24.columns)[dfLab2Curso24.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc24 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso30 = dfFinlab2.loc[dfFinlab2['curso']=='30']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso30.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso30),columns=dfLab2Curso30.columns)[dfLab2Curso30.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc30 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso36 = dfFinlab2.loc[dfFinlab2['curso']=='36']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso36.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso36),columns=dfLab2Curso36.columns)[dfLab2Curso36.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc36 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se unen los datos del laboratorio 2\n",
    "datalab2_norm = pd.concat([datalab2Normc7,datalab2Normc13,datalab2Normc19,datalab2Normc24,datalab2Normc30,datalab2Normc36],axis=0)\n",
    "datalab2_norm = datalab2_norm.reset_index(drop = True)\n",
    "#datalab2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#scaler1 = StandardScaler()\n",
    "#datalab2Normc7 = dfLab2Curso7[['mean(p$p2)']].join(pd.DataFrame(scaler1.fit_transform(dfLab2Curso7), columns=dfLab2Curso7.columns)[dfLab2Curso7.columns[1:26]]) \n",
    "#datalab2Normc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "13\n",
      "19\n",
      "24\n",
      "30\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "#LAB 3\n",
    "\n",
    "datalab3_all,cursoData = get_custom_dataframe(DATA, [3], ['p1p2','p2p2'], 'all', labels=True, index=None)\n",
    "\n",
    "datalab3 = copy.deepcopy(datalab3_all)\n",
    " \n",
    "# Remove questionsdifficulty\n",
    "remove_col(datalab3, 'qd?')\n",
    "# Group columns\n",
    "datalab3_all = apply(datalab3_all, ['p1p2','p2p2'], statistics.mean)\n",
    "datalab3 = apply(datalab3, ['p1p2','p2p2'], statistics.mean)\n",
    "datalab3 = apply(datalab3, 'dis_lab3', norm_log)\n",
    "datalab3 = apply(datalab3, 'qg?', statistics.mean)\n",
    "datalab3 = apply(datalab3, 'qat?', sum, replace=False)\n",
    "datalab3 = apply(datalab3, 'sum(qat$_lab3)', norm_log, replace=False)\n",
    "datalab3 = apply(datalab3, 'qat?', max)\n",
    "datalab3 = apply(datalab3, 'qact?', statistics.mean)\n",
    "datalab3 = apply(datalab3, 'qavt?', statistics.mean)\n",
    "datalab3 = apply(datalab3, 'qme?', max)\n",
    "datalab3 = apply(datalab3, 'qmce?', max)\n",
    "datalab3 = apply(datalab3, 'qmsr?', statistics.mean)\n",
    "datalab3 = apply(datalab3, 'qc?', statistics.mean)\n",
    "aux = datalab3['act_lab3'] / datalab3['sum(qat$_lab3)']\n",
    "for i in range(len(aux)):\n",
    "    if not aux[i] > 0:\n",
    "        aux[i] = 0\n",
    "datalab3['avgtime_lab3'] = aux\n",
    "datalab3 = datalab3.round(4)\n",
    "\n",
    "# Se transforma a dataframe la info de ese curso en particular\n",
    "cursoDF = pd.DataFrame(cursoData,columns=['curso'])\n",
    "\n",
    "# Se concatenan los dos dataframes \n",
    "dfFinlab3 = pd.concat([datalab3,cursoDF],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso7 = dfFinlab3.loc[dfFinlab3['curso']=='7']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso7.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso7),columns=dfLab3Curso7.columns)[dfLab3Curso7.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc7 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso13 = dfFinlab3.loc[dfFinlab3['curso']=='13']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso13.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso13),columns=dfLab3Curso13.columns)[dfLab3Curso13.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc13 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso19 = dfFinlab3.loc[dfFinlab3['curso']=='19']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso19.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso19),columns=dfLab3Curso19.columns)[dfLab3Curso19.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc19 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso24 = dfFinlab3.loc[dfFinlab3['curso']=='24']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso24.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso24),columns=dfLab3Curso24.columns)[dfLab3Curso24.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc24 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso30 = dfFinlab3.loc[dfFinlab3['curso']=='30']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso30.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso30),columns=dfLab3Curso30.columns)[dfLab3Curso30.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc30 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso36 = dfFinlab3.loc[dfFinlab3['curso']=='36']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso36.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso36),columns=dfLab3Curso36.columns)[dfLab3Curso36.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc36 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se unen los datos del laboratorio 3\n",
    "datalab3_norm = pd.concat([datalab3Normc7,datalab3Normc13,datalab3Normc19,datalab3Normc24,datalab3Normc30,datalab3Normc36],axis=0)\n",
    "datalab3_norm = datalab3_norm.reset_index(drop = True)\n",
    "#datalab3_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "13\n",
      "19\n",
      "24\n",
      "30\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "#LAB 4\n",
    "\n",
    "datalab4_all,cursoData = get_custom_dataframe(DATA, [4], ['p1p2','p2p2'], 'all', labels=True, index=None)\n",
    "datalab4 = copy.deepcopy(datalab4_all)\n",
    "\n",
    "# Remove questionsdifficulty\n",
    "remove_col(datalab4, 'qd?')\n",
    "# Group columns\n",
    "datalab4_all = apply(datalab4_all, ['p1p2','p2p2'], statistics.mean)\n",
    "datalab4 = apply(datalab4, ['p1p2','p2p2'], statistics.mean)\n",
    "datalab4 = apply(datalab4, 'dis_lab4', norm_log)\n",
    "datalab4 = apply(datalab4, 'qg?', statistics.mean)\n",
    "datalab4 = apply(datalab4, 'qat?', sum, replace=False)\n",
    "datalab4 = apply(datalab4, 'sum(qat$_lab4)', norm_log, replace=False)\n",
    "datalab4 = apply(datalab4, 'qat?', max)\n",
    "datalab4 = apply(datalab4, 'qact?', statistics.mean)\n",
    "datalab4 = apply(datalab4, 'qavt?', statistics.mean)\n",
    "datalab4 = apply(datalab4, 'qme?', max)\n",
    "datalab4 = apply(datalab4, 'qmce?', max)\n",
    "datalab4 = apply(datalab4, 'qmsr?', statistics.mean)\n",
    "datalab4 = apply(datalab4, 'qc?', statistics.mean)\n",
    "aux = datalab4['act_lab4'] / datalab4['sum(qat$_lab4)']\n",
    "for i in range(len(aux)):\n",
    "    if not aux[i] > 0:\n",
    "        aux[i] = 0\n",
    "datalab4['avgtime_lab4'] = aux\n",
    "datalab4 = datalab4.round(4)\n",
    "\n",
    "# Se transforma a dataframe la info de ese curso en particular\n",
    "cursoDF = pd.DataFrame(cursoData,columns=['curso'])\n",
    "\n",
    "# Se concatenan los dos dataframes \n",
    "dfFinlab4 = pd.concat([datalab4,cursoDF],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso7 = dfFinlab4.loc[dfFinlab4['curso']=='7']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso7.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso7),columns=dfLab4Curso7.columns)[dfLab4Curso7.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc7 = pd.concat([promCurso,DFnormalizado],axis=1)\n",
    "#datalab4Normc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso13 = dfFinlab4.loc[dfFinlab4['curso']=='13']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso13.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso13),columns=dfLab4Curso13.columns)[dfLab4Curso13.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc13 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso19 = dfFinlab4.loc[dfFinlab4['curso']=='19']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso19.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso19),columns=dfLab4Curso19.columns)[dfLab4Curso19.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc19 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso24 = dfFinlab4.loc[dfFinlab4['curso']=='24']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso24.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso24),columns=dfLab4Curso24.columns)[dfLab4Curso24.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc24 = pd.concat([promCurso,DFnormalizado],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso30 = dfFinlab4.loc[dfFinlab4['curso']=='30']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso30.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso30),columns=dfLab4Curso30.columns)[dfLab4Curso30.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc30 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso36 = dfFinlab4.loc[dfFinlab4['curso']=='36']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso36.reset_index()['mean(p$p2)'])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso36),columns=dfLab4Curso36.columns)[dfLab4Curso36.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc36 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se unen los datos del laboratorio 4\n",
    "datalab4_norm = pd.concat([datalab4Normc7,datalab4Normc13,datalab4Normc19,datalab4Normc24,datalab4Normc30,datalab4Normc36],axis=0)\n",
    "datalab4_norm = datalab4_norm.reset_index(drop = True)\n",
    "#datalab4_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Grid/Random-SearchCV process**   \n",
    " \n",
    "def run_process(dataset, grid_cv, target=TARGET):\n",
    "    X, y = dataset.drop(target, axis=1), np.array(dataset[target])\n",
    "   \n",
    "    grid_cv.fit(X,y)\n",
    "    print('R2:', max(grid_cv.cv_results_['mean_test_score']))\n",
    "    \n",
    "    try:\n",
    "        selected_features = X.columns[grid_cv.best_estimator_.steps[0][-1].get_support()]\n",
    "    except:\n",
    "        return list(dataset.columns[1:])\n",
    "    \n",
    "    return list(selected_features)\n",
    "    \n",
    "def run_process_obsolete(dataset, grid_cv, target=TARGET):\n",
    "    X, y = dataset.drop(target, axis=1), np.array(dataset[target])\n",
    "   \n",
    "    grid_cv.fit(X,y)\n",
    " \n",
    "    try:\n",
    "        print('R2-test-fit:', max(grid_cv.cv_results_['mean_test_score']))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        grid_cv.best_estimator_.fit(X_train, y_train)\n",
    "        print('R2-test', grid_cv.best_estimator_.score(X_test, y_test))\n",
    "        print('MSE-test', metrics.mean_squared_error(y_test,grid_cv.best_estimator_.predict(X_test)))\n",
    "\n",
    "        print('Best params:', grid_cv.best_params_)\n",
    "\n",
    "        selected_features = X.columns[grid_cv.best_estimator_.steps[0][-1].get_support()]\n",
    "        print('Selected features:', list(selected_features))\n",
    "\n",
    "        return list(selected_features)\n",
    "    except:\n",
    "        return list(dataset.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **SVR - Recursive Features Elimination**\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "sel_estimator = SVR(kernel='linear')\n",
    "selector = RFE(sel_estimator)\n",
    "estimator = SVR()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('sel', selector),\n",
    "    ('est', estimator)\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'sel__n_features_to_select' : [5],\n",
    "    'sel__step'                 : [1,2],\n",
    "    'est__C'                    : [0.01,0.1,1],\n",
    "    'est__gamma'                : ['scale','auto'],\n",
    "    'est__kernel'               : ['linear','poly','rbf']\n",
    "}\n",
    "\n",
    "grid_svr = GridSearchCV(estimator=pipe,\n",
    "                        param_grid=params,\n",
    "                        scoring='r2',\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True,\n",
    "                        cv=KFold(n_splits=10, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   33.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -0.04747534963116049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab1', 'norm_log(dis_lab1)', 'rt_lab1', 'err_lab1', 'mean(qact$_lab1)']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datalab1_shuffle = datalab1_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_svr_1 = run_process(datalab1_shuffle,grid_svr)\n",
    "selected_features_svr_1\n",
    "#R2: -0.04747534963116049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   25.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -0.018899532010106544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['g_lab2',\n",
       " 'act_lab2',\n",
       " 'rt_lab2',\n",
       " 'mean(qact$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab2_shuffle = datalab2_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_svr_2 = run_process(datalab2_shuffle,grid_svr)\n",
    "selected_features_svr_2\n",
    "#R2: -0.018899532010106544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   24.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.050022917363454586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab3', 'rt_lab3', 'actq1_lab3', 'mean(qg$_lab3)', 'mean(qmsr$_lab3)']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab3_shuffle = datalab3_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_svr_3 = run_process(datalab3_shuffle,grid_svr)\n",
    "selected_features_svr_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.12836350255649293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   26.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab4',\n",
       " 'rt_lab4',\n",
       " 'cer_lab4',\n",
       " 'mean(qact$_lab4)',\n",
       " 'norm_log(sum(qat$_lab4))']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab4_shuffle = datalab4_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_svr_4 = run_process(datalab4_shuffle,grid_svr)\n",
    "selected_features_svr_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVR()\n",
    "\n",
    "params = {\n",
    "    'C'         : [0.01,0.1,1],\n",
    "    'gamma'     : ['scale','auto'],\n",
    "    'kernel'    : ['linear','poly','rbf']\n",
    "}\n",
    "\n",
    "grid_svr = GridSearchCV(estimator=estimator,\n",
    "                        param_grid=params,\n",
    "                        scoring='r2',\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True,\n",
    "                        cv=KFold(n_splits=10, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    3.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -0.028746121318383222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab1',\n",
       " 'norm_log(dis_lab1)',\n",
       " 'rt_lab1',\n",
       " 'err_lab1',\n",
       " 'mean(qact$_lab1)',\n",
       " 'g_lab2',\n",
       " 'act_lab2',\n",
       " 'rt_lab2',\n",
       " 'mean(qact$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datalab1_norm[[TARGET] + selected_features_svr_1].join(datalab2_norm[selected_features_svr_2])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_svr)\n",
    "#R2: -0.028746121318383222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    3.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.031366296237040826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab1',\n",
       " 'norm_log(dis_lab1)',\n",
       " 'rt_lab1',\n",
       " 'err_lab1',\n",
       " 'mean(qact$_lab1)',\n",
       " 'g_lab2',\n",
       " 'act_lab2',\n",
       " 'rt_lab2',\n",
       " 'mean(qact$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))',\n",
       " 'act_lab3',\n",
       " 'rt_lab3',\n",
       " 'actq1_lab3',\n",
       " 'mean(qg$_lab3)',\n",
       " 'mean(qmsr$_lab3)']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datalab1_norm[[TARGET] + selected_features_svr_1].join(datalab2_norm[selected_features_svr_2]).join(datalab3_norm[selected_features_svr_3])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:    3.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.09571823127077915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab1',\n",
       " 'norm_log(dis_lab1)',\n",
       " 'rt_lab1',\n",
       " 'err_lab1',\n",
       " 'mean(qact$_lab1)',\n",
       " 'g_lab2',\n",
       " 'act_lab2',\n",
       " 'rt_lab2',\n",
       " 'mean(qact$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))',\n",
       " 'act_lab3',\n",
       " 'rt_lab3',\n",
       " 'actq1_lab3',\n",
       " 'mean(qg$_lab3)',\n",
       " 'mean(qmsr$_lab3)',\n",
       " 'act_lab4',\n",
       " 'rt_lab4',\n",
       " 'cer_lab4',\n",
       " 'mean(qact$_lab4)',\n",
       " 'norm_log(sum(qat$_lab4))']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datalab1_norm[[TARGET] + selected_features_svr_1].join(datalab2_norm[selected_features_svr_2]).join(datalab3_norm[selected_features_svr_3]).join(datalab4_norm[selected_features_svr_4])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Random Forest Regressor** \n",
    "\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    " \n",
    "# GradientBoostingRegressor / RandomForestRegressor / SVR(kernel='linear')\n",
    "sel_estimator = GradientBoostingRegressor(random_state=random_state)\n",
    " \n",
    "# RFE / SelectFromModel\n",
    "selector = RFE(sel_estimator)\n",
    "estimator = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n",
    " \n",
    "pipe = Pipeline([\n",
    "    ('sel', selector),\n",
    "    ('est', estimator)\n",
    "])\n",
    " \n",
    "params = {\n",
    "    'sel__estimator__learning_rate': [0.05,0.1,0.2],\n",
    "    'sel__n_features_to_select'    : [5],\n",
    "    'est__n_estimators'            : [50,100,200,400],\n",
    "    'est__criterion'               : ['mse','mae'],\n",
    "    'est__max_features'            : ['auto','sqrt','log2']\n",
    "}\n",
    " \n",
    "grid_rfr = GridSearchCV(estimator=pipe,\n",
    "                        param_grid=params,\n",
    "                        scoring='r2',\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True,\n",
    "                        cv=KFold(n_splits=5, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   36.5s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-bb346a13a819>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdatalab1_shuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatalab1_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mselected_features_rfr_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatalab1_shuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrid_rfr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-126-614349d97504>\u001b[0m in \u001b[0;36mrun_process\u001b[1;34m(dataset, grid_cv, target)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mgrid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'R2:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    710\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1153\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    689\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 691\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    552\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    553\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datalab1_shuffle = datalab1_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_rfr_1 = run_process(datalab1_shuffle,grid_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_rfr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-51d54b6d8ac8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdatalab2_shuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatalab2_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mselected_features_rfr_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatalab2_shuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrid_rfr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mselected_features_rfr_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_rfr' is not defined"
     ]
    }
   ],
   "source": [
    "datalab2_shuffle = datalab2_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_rfr_2 = run_process(datalab2_shuffle,grid_rfr)\n",
    "selected_features_rfr_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.15182166383214257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab3', 'cer_lab3', 'actq1_lab3', 'max(qat$_lab3)', 'avgtime_lab3']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab3_shuffle = datalab3_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_rfr_3 = run_process(datalab3_shuffle,grid_rfr)\n",
    "selected_features_rfr_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.1801737738899168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['g_lab4', 'ut_lab4', 'cer_lab4', 'mean(qc$_lab4)', 'norm_log(sum(qat$_lab4))']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab4_shuffle = datalab4_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_rfr_4 = run_process(datalab4_shuffle,grid_rfr)\n",
    "selected_features_rfr_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestRegressor(random_state=random_state, n_jobs=-1)\n",
    " \n",
    "params = {\n",
    "    'n_estimators'  : [50,100,200,400],\n",
    "    'criterion'     : ['mse','mae'],\n",
    "    'max_features'  : ['auto','sqrt','log2']\n",
    "}\n",
    " \n",
    "grid_rfr = GridSearchCV(estimator=estimator,\n",
    "                        param_grid=params,\n",
    "                        scoring='r2',\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True,\n",
    "                        cv=KFold(n_splits=5, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   12.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.11707190931551861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab1',\n",
       " 'ct_lab1',\n",
       " 'rtr_lab1',\n",
       " 'actq3_lab1',\n",
       " 'mean(qmsr$_lab1)',\n",
       " 'a_lab2',\n",
       " 'ut_lab2',\n",
       " 'actq2_lab2',\n",
       " 'mean(qmsr$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinación de laboratorios\n",
    "dataset = datalab1_shuffle[[TARGET] + selected_features_rfr_1].join(datalab2_shuffle[selected_features_rfr_2])\n",
    "run_process(dataset,grid_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    3.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.2903347814150217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   13.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab1',\n",
       " 'ct_lab1',\n",
       " 'rtr_lab1',\n",
       " 'actq3_lab1',\n",
       " 'mean(qmsr$_lab1)',\n",
       " 'a_lab2',\n",
       " 'ut_lab2',\n",
       " 'actq2_lab2',\n",
       " 'mean(qmsr$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))',\n",
       " 'a_lab3',\n",
       " 'cer_lab3',\n",
       " 'actq1_lab3',\n",
       " 'max(qat$_lab3)',\n",
       " 'avgtime_lab3']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datalab1_shuffle[[TARGET] + selected_features_rfr_1].join(datalab2_shuffle[selected_features_rfr_2]).join(datalab3_shuffle[selected_features_rfr_3])\n",
    "run_process(dataset,grid_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    3.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.34686963846954494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   14.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab1',\n",
       " 'ct_lab1',\n",
       " 'rtr_lab1',\n",
       " 'actq3_lab1',\n",
       " 'mean(qmsr$_lab1)',\n",
       " 'a_lab2',\n",
       " 'ut_lab2',\n",
       " 'actq2_lab2',\n",
       " 'mean(qmsr$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))',\n",
       " 'a_lab3',\n",
       " 'cer_lab3',\n",
       " 'actq1_lab3',\n",
       " 'max(qat$_lab3)',\n",
       " 'avgtime_lab3',\n",
       " 'g_lab4',\n",
       " 'ut_lab4',\n",
       " 'cer_lab4',\n",
       " 'mean(qc$_lab4)',\n",
       " 'norm_log(sum(qat$_lab4))']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datalab1_shuffle[[TARGET] + selected_features_rfr_1].join(datalab2_shuffle[selected_features_rfr_2]).join(datalab3_shuffle[selected_features_rfr_3]).join(datalab4_shuffle[selected_features_rfr_4])\n",
    "run_process(dataset,grid_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Linear Regression**\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    " \n",
    "# GradientBoostingRegressor / RandomForestRegressor / SVR(kernel='linear')\n",
    "sel_estimator = GradientBoostingRegressor(random_state=random_state)\n",
    " \n",
    "# RFE / SelectFromModel\n",
    "selector = RFE(sel_estimator)\n",
    "estimator = LinearRegression()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('sel', selector),\n",
    "    ('est', estimator)\n",
    "])\n",
    " \n",
    "# params = {'est__n_jobs': [-1], \n",
    "#           'est__normalize': [True], \n",
    "#           'sel__estimator__learning_rate': [0.1], \n",
    "#           'sel__estimator__n_estimators': [100], \n",
    "#           'sel__max_features': [10], \n",
    "#           'sel__prefit': [False]}\n",
    "\n",
    "params = {\n",
    "    'sel__n_features_to_select' : [5],\n",
    "    'sel__step'                 : [1,2],\n",
    "    'est__n_jobs'               : [-1],\n",
    "}\n",
    " \n",
    "grid_lr = GridSearchCV(estimator=pipe,\n",
    "                       param_grid=params,\n",
    "                       scoring='r2',\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1,\n",
    "                       return_train_score=True,\n",
    "                       cv=KFold(n_splits=5, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-f5a6d140f4d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdatalab1_shuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatalab1_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mselected_features_lr_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatalab1_shuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrid_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mselected_features_lr_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid_lr' is not defined"
     ]
    }
   ],
   "source": [
    "datalab1_shuffle = datalab1_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_lr_1 = run_process(datalab1_shuffle,grid_lr)\n",
    "selected_features_lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.7s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -0.034471938068203034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab2',\n",
       " 'ut_lab2',\n",
       " 'actq2_lab2',\n",
       " 'mean(qmsr$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab2_shuffle = datalab2_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_lr_2 = run_process(datalab2_shuffle,grid_lr)\n",
    "selected_features_lr_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.8s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.06342765309181922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab3', 'rtr_lab3', 'actq1_lab3', 'max(qat$_lab3)', 'avgtime_lab3']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab3_shuffle = datalab3_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_lr_3 = run_process(datalab3_shuffle,grid_lr)\n",
    "selected_features_lr_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    1.6s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.0645754385517409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['g_lab4', 'ut_lab4', 'cer_lab4', 'mean(qc$_lab4)', 'norm_log(sum(qat$_lab4))']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab4_shuffle = datalab4_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_lr_4 = run_process(datalab4_shuffle,grid_lr)\n",
    "selected_features_lr_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LinearRegression()\n",
    " \n",
    "params = {\n",
    "    'n_jobs'    : [-1],\n",
    "}\n",
    " \n",
    "grid_lr = GridSearchCV(estimator=estimator,\n",
    "                       param_grid=params,\n",
    "                       scoring='r2',\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1,\n",
    "                       return_train_score=True,\n",
    "                       cv=KFold(n_splits=5, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "R2: -0.016378643751367505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab1',\n",
       " 'ut_lab1',\n",
       " 'ct_lab1',\n",
       " 'rtr_lab1',\n",
       " 'actq3_lab1',\n",
       " 'a_lab2',\n",
       " 'ut_lab2',\n",
       " 'actq2_lab2',\n",
       " 'mean(qmsr$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinación de laboratorios 1 y 2\n",
    "dataset = datalab1_shuffle[[TARGET] + selected_features_lr_1].join(datalab2_shuffle[selected_features_lr_2])\n",
    "run_process(dataset,grid_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "R2: 0.04754742638434373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab1',\n",
       " 'ut_lab1',\n",
       " 'ct_lab1',\n",
       " 'rtr_lab1',\n",
       " 'actq3_lab1',\n",
       " 'a_lab2',\n",
       " 'ut_lab2',\n",
       " 'actq2_lab2',\n",
       " 'mean(qmsr$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))',\n",
       " 'a_lab3',\n",
       " 'rtr_lab3',\n",
       " 'actq1_lab3',\n",
       " 'max(qat$_lab3)',\n",
       " 'avgtime_lab3']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinación de laboratorios 1, 2 y 3\n",
    "dataset = datalab1_shuffle[[TARGET] + selected_features_lr_1].join(datalab2_shuffle[selected_features_lr_2]).join(datalab3_shuffle[selected_features_lr_3])\n",
    "run_process(dataset,grid_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.025138234260172254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.1s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab1',\n",
       " 'ut_lab1',\n",
       " 'ct_lab1',\n",
       " 'rtr_lab1',\n",
       " 'actq3_lab1',\n",
       " 'a_lab2',\n",
       " 'ut_lab2',\n",
       " 'actq2_lab2',\n",
       " 'mean(qmsr$_lab2)',\n",
       " 'norm_log(sum(qat$_lab2))',\n",
       " 'a_lab3',\n",
       " 'rtr_lab3',\n",
       " 'actq1_lab3',\n",
       " 'max(qat$_lab3)',\n",
       " 'avgtime_lab3',\n",
       " 'g_lab4',\n",
       " 'ut_lab4',\n",
       " 'cer_lab4',\n",
       " 'mean(qc$_lab4)',\n",
       " 'norm_log(sum(qat$_lab4))']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinación de laboratorios 1, 2, 3 y 4\n",
    "dataset = datalab1_shuffle[[TARGET] + selected_features_lr_1].join(datalab2_shuffle[selected_features_lr_2]).join(datalab3_shuffle[selected_features_lr_3]).join(datalab4_shuffle[selected_features_lr_4])\n",
    "run_process(dataset,grid_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **ANN**\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def create_model( nl1=1, nl2=1,  nl3=1, \n",
    "                 nn1=1000, nn2=500, nn3 = 200, lr=0.01, decay=0., l1=0.01, l2=0.01,\n",
    "                act = 'relu', dropout=0, input_shape=25, output_shape=1):\n",
    "    '''This is a model generating function so that we can search over neural net \n",
    "    parameters and architecture'''\n",
    "    \n",
    "    opt = 'SGD' # keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999,  decay=decay)\n",
    "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "                                                     \n",
    "    model = Sequential()\n",
    "    \n",
    "    # for the firt layer we need to specify the input dimensions\n",
    "    first=True\n",
    "    \n",
    "    for i in range(nl1):\n",
    "        if first:\n",
    "            model.add(Dense(nn1, input_dim=input_shape, activation=act, kernel_regularizer=reg))\n",
    "            first=False\n",
    "        else: \n",
    "            model.add(Dense(nn1, activation=act, kernel_regularizer=reg))\n",
    "        if dropout!=0:\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "    for i in range(nl2):\n",
    "        if first:\n",
    "            model.add(Dense(nn2, input_dim=input_shape, activation=act, kernel_regularizer=reg))\n",
    "            first=False\n",
    "        else: \n",
    "            model.add(Dense(nn2, activation=act, kernel_regularizer=reg))\n",
    "        if dropout!=0:\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "    for i in range(nl3):\n",
    "        if first:\n",
    "            model.add(Dense(nn3, input_dim=input_shape, activation=act, kernel_regularizer=reg))\n",
    "            first=False\n",
    "        else: \n",
    "            model.add(Dense(nn3, activation=act, kernel_regularizer=reg))\n",
    "        if dropout!=0:\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "    model.add(Dense(output_shape, activation='sigmoid'))\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=[coeff_determination])\n",
    "    return model\n",
    "\n",
    "# model class to use in the scikit random search CV \n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usaremos los detectador por RF\n",
    "selected_features_rfr_1 = ['a_lab1', 'ct_lab1', 'rtr_lab1', 'actq3_lab1', 'mean(qmsr$_lab1)']\n",
    "selected_features_rfr_2 = ['a_lab2', 'ut_lab2', 'actq2_lab2', 'mean(qmsr$_lab2)', 'norm_log(sum(qat$_lab2))']\n",
    "selected_features_rfr_3 = ['a_lab3', 'cer_lab3', 'actq1_lab3', 'max(qat$_lab3)', 'avgtime_lab3']\n",
    "selected_features_rfr_4 = ['g_lab4', 'ut_lab4', 'cer_lab4', 'mean(qc$_lab4)', 'norm_log(sum(qat$_lab4))']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253,)\n",
      "Score for fold: loss of 4.850094318389893; coeff_determination of -0.8982784748077393\n",
      "Score for fold: loss of 3.5494377613067627; coeff_determination of -1.2305452823638916\n",
      "Score for fold: loss of 6.49022912979126; coeff_determination of -1.2849454879760742\n",
      "Score for fold: loss of 7.1299052238464355; coeff_determination of -1.0491468906402588\n",
      "Score for fold: loss of 7.813864231109619; coeff_determination of -3.4904003143310547\n",
      "Score for fold: loss of 5.480053901672363; coeff_determination of -1.150977373123169\n",
      "Score for fold: loss of 6.1169819831848145; coeff_determination of -1.4866828918457031\n",
      "Score for fold: loss of 6.257479667663574; coeff_determination of -0.8293517827987671\n",
      "Score for fold: loss of 5.329455852508545; coeff_determination of -1.8746662139892578\n",
      "Score for fold: loss of 6.167933464050293; coeff_determination of -1.9908628463745117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.5285857558250426"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "dataset = datalab1_norm[[TARGET] + selected_features_rfr_1].join(datalab2_norm[selected_features_rfr_2]).join(datalab3_norm[selected_features_rfr_3]).join(datalab4_norm[selected_features_rfr_4])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "\n",
    "X, y = dataset_shuffle.drop(TARGET, axis=1).to_numpy(), np.array(dataset_shuffle[TARGET])\n",
    "print(y.shape)\n",
    "\n",
    "model = create_model( nl1=1, nl2=0,  nl3=0, \n",
    "                 nn1=10, nn2=0, nn3 = 0, lr=0.01, decay=0., l1=0.01, l2=0.01,\n",
    "                act = 'sigmoid', dropout=0, input_shape=20, output_shape=1)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "R = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    # Fit data to model\n",
    "    history = model.fit(X[train], y[train],\n",
    "              batch_size=50,\n",
    "              epochs=500,\n",
    "              verbose=0)\n",
    "    \n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    print(f'Score for fold: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]}')\n",
    "    R.append(scores[1])\n",
    "np.mean(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = model\n",
    "\n",
    "param_grid = {\n",
    "    'nl1':            [1],\n",
    "    'nl2':            [1],\n",
    "    'nl3':            [1],\n",
    "    'nn1':            [5,20,50],\n",
    "    'nn2':            [5,20,50],\n",
    "    'nn3':            [5,20,50],\n",
    "    #'act':           ['relu', 'sigmoid'],\n",
    "    #'l1':            [0, 0.01, 0.003, 0.001, 0.0001],\n",
    "    #'l2':            [0, 0.01, 0.003, 0.001, 0.0001],\n",
    "    #'lr':            [1e-2, 1e-3, 1e-4],\n",
    "    #'decay':         [1e-6,1e-9,0],\n",
    "    #'dropout':       [0, 0.1, 0.2, 0.3], \n",
    "    'input_shape':    [5,20,50,100],\n",
    "    'output_shape':   [1]\n",
    "}\n",
    "\n",
    "rand_cv = RandomizedSearchCV(estimator=estimator,\n",
    "                             param_distributions=param_grid,\n",
    "                             verbose=10,\n",
    "                             n_iter=70,\n",
    "                             scoring='neg_mean_squared_error',\n",
    "                             n_jobs=-1,\n",
    "                             return_train_score=True,\n",
    "                             cv=KFold(n_splits=10, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-f3f0b291060b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                  \u001b[0mnn1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 act = 'relu', dropout=0, input_shape=25, output_shape=1)\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = datalab1_norm[[TARGET] + selected_features_rfr_1].join(datalab2_norm[selected_features_rfr_2]).join(datalab3_norm[selected_features_rfr_3]).join(datalab4_norm[selected_features_rfr_4])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "#dataset_shuffle\n",
    "run_process(dataset_shuffle,rand_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
