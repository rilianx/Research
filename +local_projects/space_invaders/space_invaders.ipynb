{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3orbAEQrYvqR",
    "outputId": "21067476-2f2a-4ff2-9634-5ae28b20cdb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gym-spaceinvaders'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sbaezamella/gym-spaceinvaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPf-w8SJYxfn",
    "outputId": "5e1c0389-576a-4382-d3e1-c4320e9a635d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Documents\\local_projects\\space_invaders\\gym-spaceinvaders\n"
     ]
    }
   ],
   "source": [
    "cd gym-spaceinvaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting atari_py\n",
      "  Using cached atari-py-0.2.9.tar.gz (540 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from atari_py) (1.19.2)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\appdata\\roaming\\python\\python38\\site-packages (from atari_py) (1.15.0)\n",
      "Using legacy 'setup.py install' for atari-py, since package 'wheel' is not installed.\n",
      "Installing collected packages: atari-py\n",
      "    Running setup.py install for atari-py: started\n",
      "    Running setup.py install for atari-py: finished with status 'done'\n",
      "Successfully installed atari-py-0.2.9\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RPdaZmdob7XP",
    "outputId": "2e03015a-111c-43ad-b53e-213e664cac06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python38-32\\python.exe: No module named atari_py.import_roms\n"
     ]
    }
   ],
   "source": [
    "!python -m atari_py.import_roms gym-spaceinvaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vaB9SOgY54H",
    "outputId": "2ab49c80-8145-44a5-a67a-f9cd980ceb40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/gym-spaceinvaders\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from gym-spaceinvaders==0.0.1) (0.17.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-spaceinvaders==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->gym-spaceinvaders==0.0.1) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->gym-spaceinvaders==0.0.1) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-spaceinvaders==0.0.1) (1.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-spaceinvaders==0.0.1) (0.16.0)\n",
      "Installing collected packages: gym-spaceinvaders\n",
      "  Found existing installation: gym-spaceinvaders 0.0.1\n",
      "    Can't uninstall 'gym-spaceinvaders'. No files were found to uninstall.\n",
      "  Running setup.py develop for gym-spaceinvaders\n",
      "Successfully installed gym-spaceinvaders\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gc27jsQcY8Li"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import gym\n",
    "import gym_spaceinvaders\n",
    "#importlib.reload(gym)\n",
    "#importlib.reload(gym_spaceinvaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iYV-hUsdR--",
    "outputId": "f201e7cf-daee-4ff9-f226-5bfe12ed5bb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
      "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.2)\n",
      "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (0.3)\n",
      "Requirement already satisfied: piglet in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
      "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.7/dist-packages (from piglet) (1.2.0)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (21.2.0)\n",
      "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.4.7)\n",
      "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.36.2)\n"
     ]
    }
   ],
   "source": [
    "!apt-get install python-opengl -y\n",
    "\n",
    "!apt install xvfb -y\n",
    "\n",
    "!pip install pyvirtualdisplay\n",
    "\n",
    "!pip install piglet\n",
    "\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "Display().start()\n",
    "\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fh_nl4eFhsyq",
    "outputId": "4739f6bf-f486-41ea-8cb2-b6c3a928d781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.5.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.19.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.5.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.12.4)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.34.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.36.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.12.1)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.5.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.12.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow->keras-rl2) (57.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (2.23.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.8.0)\n",
      "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow->keras-rl2) (1.5.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2) (4.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2) (2.10)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GKgbts-Rdbu0"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import gym\n",
    "import gym_spaceinvaders\n",
    "\n",
    "def calculate_enemies_centroids(self, agent_centroid, enemies_centroids):\n",
    "    agent_x, agent_y = agent_centroid\n",
    "    coords = enemies_centroids.copy()\n",
    "    coords[:, 0], coords[:, 1] = coords[:, 0] - agent_x, agent_y - coords[:, 1]\n",
    "\n",
    "    distances = np.square(coords)\n",
    "    distances = np.sqrt(np.sum(distances, axis=1, keepdims=True))\n",
    "\n",
    "    coords = np.append(coords, distances, axis=1)\n",
    "\n",
    "    sorted_enemy_coords = coords[np.argsort(coords[:, 2])]\n",
    "    print(\"hola\")\n",
    "    system.exit\n",
    "\n",
    "    nearest_enemies = sorted_enemy_coords[:5, :2].reshape(\n",
    "        10,\n",
    "    )\n",
    "    return nearest_enemies\n",
    "\n",
    "gym_spaceinvaders.envs.spaceinvaders_env._calculate_enemies_centroids = calculate_enemies_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X3ApblRwg_Mi",
    "outputId": "64428c62-bb4e-40d9-8cc7-ede1543d0a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 102       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,350\n",
      "Trainable params: 1,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   420/100000: episode: 1, duration: 11.850s, episode steps: 420, steps per second:  35, episode reward: 63.000, mean reward:  0.150 [-0.100, 29.900], mean action: 2.467 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "   960/100000: episode: 2, duration: 14.388s, episode steps: 540, steps per second:  38, episode reward: 31.000, mean reward:  0.057 [-0.100, 29.900], mean action: 2.376 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  1467/100000: episode: 3, duration: 13.484s, episode steps: 507, steps per second:  38, episode reward: 84.300, mean reward:  0.166 [-0.100, 29.900], mean action: 2.473 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  2154/100000: episode: 4, duration: 18.353s, episode steps: 687, steps per second:  37, episode reward: 46.300, mean reward:  0.067 [-0.100, 29.900], mean action: 2.325 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  2536/100000: episode: 5, duration: 10.237s, episode steps: 382, steps per second:  37, episode reward: 26.800, mean reward:  0.070 [-0.100, 19.900], mean action: 2.348 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  3128/100000: episode: 6, duration: 15.758s, episode steps: 592, steps per second:  38, episode reward: 30.800, mean reward:  0.052 [-0.100, 24.900], mean action: 2.493 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  3600/100000: episode: 7, duration: 12.705s, episode steps: 472, steps per second:  37, episode reward: -27.200, mean reward: -0.058 [-0.100,  9.900], mean action: 2.439 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  4454/100000: episode: 8, duration: 22.881s, episode steps: 854, steps per second:  37, episode reward: 129.600, mean reward:  0.152 [-0.100, 29.900], mean action: 2.429 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  4816/100000: episode: 9, duration: 9.791s, episode steps: 362, steps per second:  37, episode reward: 58.800, mean reward:  0.162 [-0.100, 24.900], mean action: 2.442 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  5657/100000: episode: 10, duration: 22.620s, episode steps: 841, steps per second:  37, episode reward: 125.900, mean reward:  0.150 [-0.100, 29.900], mean action: 2.440 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  6597/100000: episode: 11, duration: 25.157s, episode steps: 940, steps per second:  37, episode reward: 366.000, mean reward:  0.389 [-0.100, 199.900], mean action: 2.421 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  7005/100000: episode: 12, duration: 10.970s, episode steps: 408, steps per second:  37, episode reward:  9.200, mean reward:  0.023 [-0.100, 19.900], mean action: 2.392 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  7633/100000: episode: 13, duration: 16.798s, episode steps: 628, steps per second:  37, episode reward: 17.200, mean reward:  0.027 [-0.100, 24.900], mean action: 2.379 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  8453/100000: episode: 14, duration: 21.876s, episode steps: 820, steps per second:  37, episode reward: 303.000, mean reward:  0.370 [-0.100, 199.900], mean action: 2.352 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  8827/100000: episode: 15, duration: 9.960s, episode steps: 374, steps per second:  38, episode reward: 62.600, mean reward:  0.167 [-0.100, 19.900], mean action: 2.481 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  9302/100000: episode: 16, duration: 12.605s, episode steps: 475, steps per second:  38, episode reward: 57.500, mean reward:  0.121 [-0.100, 29.900], mean action: 2.417 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n",
      "  9790/100000: episode: 17, duration: 12.966s, episode steps: 488, steps per second:  38, episode reward: 106.200, mean reward:  0.218 [-0.100, 29.900], mean action: 2.461 [0.000, 5.000],  loss: --, accuracy: --, mse: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10477/100000: episode: 18, duration: 24.622s, episode steps: 687, steps per second:  28, episode reward: 56.300, mean reward:  0.082 [-0.100, 24.900], mean action: 2.266 [0.000, 5.000],  loss: 8.955983, accuracy: 0.161962, mse: 3.125109, mae: 0.311388, mean_q: 0.999311, mean_eps: 0.907854\n",
      " 11352/100000: episode: 19, duration: 34.038s, episode steps: 875, steps per second:  26, episode reward: 82.500, mean reward:  0.094 [-0.100, 19.900], mean action: 2.242 [0.000, 5.000],  loss: 4.429815, accuracy: 0.167643, mse: 1.615504, mae: 0.301511, mean_q: 1.000000, mean_eps: 0.901774\n",
      " 11857/100000: episode: 20, duration: 19.688s, episode steps: 505, steps per second:  26, episode reward: 14.500, mean reward:  0.029 [-0.100, 29.900], mean action: 2.200 [0.000, 5.000],  loss: 11.018647, accuracy: 0.175248, mse: 3.810568, mae: 0.311144, mean_q: 1.000000, mean_eps: 0.895564\n",
      " 12841/100000: episode: 21, duration: 38.329s, episode steps: 984, steps per second:  26, episode reward: 341.600, mean reward:  0.347 [-0.100, 199.900], mean action: 2.250 [0.000, 5.000],  loss: 5.008120, accuracy: 0.179719, mse: 1.806245, mae: 0.301534, mean_q: 1.000000, mean_eps: 0.888863\n",
      " 13722/100000: episode: 22, duration: 34.341s, episode steps: 881, steps per second:  26, episode reward: 251.900, mean reward:  0.286 [-0.100, 29.900], mean action: 2.308 [0.000, 5.000],  loss: 11.699221, accuracy: 0.191047, mse: 4.034713, mae: 0.308444, mean_q: 1.000000, mean_eps: 0.880471\n",
      " 14656/100000: episode: 23, duration: 36.581s, episode steps: 934, steps per second:  26, episode reward: 286.600, mean reward:  0.307 [-0.100, 199.900], mean action: 2.203 [0.000, 5.000],  loss: 8.115046, accuracy: 0.197705, mse: 2.838904, mae: 0.303702, mean_q: 1.000000, mean_eps: 0.872304\n",
      " 15059/100000: episode: 24, duration: 15.858s, episode steps: 403, steps per second:  25, episode reward: 39.700, mean reward:  0.099 [-0.100, 29.900], mean action: 2.141 [0.000, 5.000],  loss: 16.460210, accuracy: 0.205800, mse: 5.619258, mae: 0.312078, mean_q: 1.000000, mean_eps: 0.866287\n",
      " 16368/100000: episode: 25, duration: 50.993s, episode steps: 1309, steps per second:  26, episode reward: 529.100, mean reward:  0.404 [-0.100, 199.900], mean action: 2.149 [0.000, 5.000],  loss: 11.756844, accuracy: 0.213402, mse: 4.050163, mae: 0.304425, mean_q: 1.000000, mean_eps: 0.858583\n",
      " 17104/100000: episode: 26, duration: 28.793s, episode steps: 736, steps per second:  26, episode reward: 51.400, mean reward:  0.070 [-0.100, 29.900], mean action: 2.103 [0.000, 5.000],  loss: 12.326339, accuracy: 0.227114, mse: 4.237707, mae: 0.304790, mean_q: 1.000000, mean_eps: 0.849381\n",
      " 17736/100000: episode: 27, duration: 24.895s, episode steps: 632, steps per second:  25, episode reward: 96.800, mean reward:  0.153 [-0.100, 29.900], mean action: 2.206 [0.000, 5.000],  loss: 11.853627, accuracy: 0.232298, mse: 4.079159, mae: 0.302552, mean_q: 1.000000, mean_eps: 0.843224\n",
      " 18370/100000: episode: 28, duration: 24.854s, episode steps: 634, steps per second:  26, episode reward: 56.600, mean reward:  0.089 [-0.100, 29.900], mean action: 2.057 [0.000, 5.000],  loss: 11.700250, accuracy: 0.241769, mse: 4.026480, mae: 0.297532, mean_q: 1.000000, mean_eps: 0.837527\n",
      " 19060/100000: episode: 29, duration: 27.049s, episode steps: 690, steps per second:  26, episode reward: 41.000, mean reward:  0.059 [-0.100, 29.900], mean action: 2.029 [0.000, 5.000],  loss: 10.022473, accuracy: 0.258152, mse: 3.464511, mae: 0.289091, mean_q: 1.000000, mean_eps: 0.831570\n",
      " 19457/100000: episode: 30, duration: 15.509s, episode steps: 397, steps per second:  26, episode reward: 40.300, mean reward:  0.102 [-0.100, 19.900], mean action: 1.972 [0.000, 5.000],  loss: 7.797930, accuracy: 0.259761, mse: 2.722657, mae: 0.288693, mean_q: 1.000000, mean_eps: 0.826678\n",
      " 20309/100000: episode: 31, duration: 33.355s, episode steps: 852, steps per second:  26, episode reward: 129.800, mean reward:  0.152 [-0.100, 29.900], mean action: 1.969 [0.000, 5.000],  loss: 12.953354, accuracy: 0.276188, mse: 4.438401, mae: 0.287544, mean_q: 1.000000, mean_eps: 0.821057\n",
      " 21213/100000: episode: 32, duration: 35.323s, episode steps: 904, steps per second:  26, episode reward: 139.600, mean reward:  0.154 [-0.100, 29.900], mean action: 2.095 [0.000, 5.000],  loss: 5.573521, accuracy: 0.285295, mse: 1.976998, mae: 0.273112, mean_q: 1.000000, mean_eps: 0.813156\n",
      " 21853/100000: episode: 33, duration: 25.191s, episode steps: 640, steps per second:  25, episode reward: 41.000, mean reward:  0.064 [-0.100, 29.900], mean action: 1.956 [0.000, 5.000],  loss: 11.755994, accuracy: 0.289355, mse: 4.037219, mae: 0.285761, mean_q: 1.000000, mean_eps: 0.806207\n",
      " 22812/100000: episode: 34, duration: 37.754s, episode steps: 959, steps per second:  25, episode reward: 314.100, mean reward:  0.328 [-0.100, 199.900], mean action: 1.980 [0.000, 5.000],  loss: 7.373020, accuracy: 0.295686, mse: 2.575146, mae: 0.273402, mean_q: 1.000000, mean_eps: 0.799012\n",
      " 23611/100000: episode: 35, duration: 31.317s, episode steps: 799, steps per second:  26, episode reward: 175.100, mean reward:  0.219 [-0.100, 29.900], mean action: 1.892 [0.000, 5.000],  loss: 9.419970, accuracy: 0.306868, mse: 3.255623, mae: 0.277591, mean_q: 1.000000, mean_eps: 0.791101\n",
      " 24063/100000: episode: 36, duration: 17.833s, episode steps: 452, steps per second:  25, episode reward: 29.800, mean reward:  0.066 [-0.100, 24.900], mean action: 1.985 [0.000, 5.000],  loss: 5.576561, accuracy: 0.309942, mse: 1.973886, mae: 0.267346, mean_q: 1.000000, mean_eps: 0.785471\n",
      " 24439/100000: episode: 37, duration: 14.785s, episode steps: 376, steps per second:  25, episode reward: 47.400, mean reward:  0.126 [-0.100, 19.900], mean action: 1.965 [0.000, 5.000],  loss: 9.575082, accuracy: 0.314661, mse: 3.306000, mae: 0.274940, mean_q: 1.000000, mean_eps: 0.781746\n",
      " 25156/100000: episode: 38, duration: 28.156s, episode steps: 717, steps per second:  25, episode reward: 48.300, mean reward:  0.067 [-0.100, 29.900], mean action: 2.013 [0.000, 5.000],  loss: 10.962848, accuracy: 0.320040, mse: 3.767689, mae: 0.276701, mean_q: 1.000000, mean_eps: 0.776827\n",
      " 26204/100000: episode: 39, duration: 40.856s, episode steps: 1048, steps per second:  26, episode reward: 260.200, mean reward:  0.248 [-0.100, 29.900], mean action: 1.971 [0.000, 5.000],  loss: 7.267736, accuracy: 0.320998, mse: 2.535820, mae: 0.270315, mean_q: 1.000000, mean_eps: 0.768885\n",
      " 26802/100000: episode: 40, duration: 23.345s, episode steps: 598, steps per second:  26, episode reward: 45.200, mean reward:  0.076 [-0.100, 24.900], mean action: 1.881 [0.000, 5.000],  loss: 2.296948, accuracy: 0.332985, mse: 0.876879, mae: 0.250240, mean_q: 1.000000, mean_eps: 0.761478\n",
      " 27779/100000: episode: 41, duration: 37.991s, episode steps: 977, steps per second:  26, episode reward: 142.300, mean reward:  0.146 [-0.100, 29.900], mean action: 1.886 [0.000, 5.000],  loss: 5.111939, accuracy: 0.336937, mse: 1.814506, mae: 0.256846, mean_q: 1.000000, mean_eps: 0.754390\n",
      " 28433/100000: episode: 42, duration: 25.434s, episode steps: 654, steps per second:  26, episode reward: 149.600, mean reward:  0.229 [-0.100, 29.900], mean action: 1.752 [0.000, 5.000],  loss: 5.486357, accuracy: 0.345709, mse: 1.937977, mae: 0.255259, mean_q: 1.000000, mean_eps: 0.747051\n",
      " 28829/100000: episode: 43, duration: 15.504s, episode steps: 396, steps per second:  26, episode reward: 15.400, mean reward:  0.039 [-0.100, 19.900], mean action: 1.725 [0.000, 5.000],  loss: 4.094347, accuracy: 0.352588, mse: 1.472829, mae: 0.249672, mean_q: 1.000000, mean_eps: 0.742326\n",
      " 29746/100000: episode: 44, duration: 35.913s, episode steps: 917, steps per second:  26, episode reward: 148.300, mean reward:  0.162 [-0.100, 29.900], mean action: 1.874 [0.000, 5.000],  loss: 4.182310, accuracy: 0.356393, mse: 1.501507, mae: 0.251942, mean_q: 1.000000, mean_eps: 0.736417\n",
      " 30404/100000: episode: 45, duration: 25.907s, episode steps: 658, steps per second:  25, episode reward: -50.800, mean reward: -0.077 [-0.100,  9.900], mean action: 1.862 [0.000, 5.000],  loss: 2.815855, accuracy: 0.356810, mse: 1.045864, mae: 0.248371, mean_q: 1.000000, mean_eps: 0.729330\n",
      " 31181/100000: episode: 46, duration: 30.424s, episode steps: 777, steps per second:  26, episode reward: 102.300, mean reward:  0.132 [-0.100, 29.900], mean action: 1.862 [0.000, 5.000],  loss: 3.216062, accuracy: 0.363497, mse: 1.178245, mae: 0.242931, mean_q: 1.000000, mean_eps: 0.722872\n",
      " 32119/100000: episode: 47, duration: 36.652s, episode steps: 938, steps per second:  26, episode reward: 166.200, mean reward:  0.177 [-0.100, 29.900], mean action: 1.738 [0.000, 5.000],  loss: 6.196516, accuracy: 0.370103, mse: 2.170466, mae: 0.252436, mean_q: 1.000000, mean_eps: 0.715155\n",
      " 32896/100000: episode: 48, duration: 30.103s, episode steps: 777, steps per second:  26, episode reward: 202.300, mean reward:  0.260 [-0.100, 29.900], mean action: 1.819 [0.000, 5.000],  loss: 2.655308, accuracy: 0.373834, mse: 0.989497, mae: 0.242621, mean_q: 1.000000, mean_eps: 0.707437\n",
      " 33753/100000: episode: 49, duration: 33.176s, episode steps: 857, steps per second:  26, episode reward: 179.300, mean reward:  0.209 [-0.100, 29.900], mean action: 1.777 [0.000, 5.000],  loss: 2.914574, accuracy: 0.372958, mse: 1.076068, mae: 0.246232, mean_q: 1.000000, mean_eps: 0.700084\n",
      " 34545/100000: episode: 50, duration: 30.793s, episode steps: 792, steps per second:  26, episode reward: 75.800, mean reward:  0.096 [-0.100, 29.900], mean action: 1.624 [0.000, 5.000],  loss: 2.725032, accuracy: 0.385811, mse: 1.010762, mae: 0.239040, mean_q: 1.000000, mean_eps: 0.692663\n",
      " 35593/100000: episode: 51, duration: 40.386s, episode steps: 1048, steps per second:  26, episode reward: 180.200, mean reward:  0.172 [-0.100, 29.900], mean action: 1.635 [0.000, 5.000],  loss: 2.814175, accuracy: 0.395694, mse: 1.038816, mae: 0.236690, mean_q: 1.000000, mean_eps: 0.684384\n"
     ]
    }
   ],
   "source": [
    "#importlib.reload(gym)\n",
    "#importlib.reload(gym_spaceinvaders)\n",
    "#importlib.reload(gym_spaceinvaders.envs.spaceinvaders_env)\n",
    "import importlib\n",
    "import gym\n",
    "import gym_spaceinvaders\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from rl.agents.dqn import DQNAgent\n",
    "\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.policy import LinearAnnealedPolicy\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.policy import SoftmaxPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "ENV_NAME = 'CustomSpaceInvaders-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in tensorflow.keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=100000)\n",
    "#policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10000,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(learning_rate=.025), metrics=['accuracy','mse','mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "\n",
    "train_history = dqn.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights(f'dqn_{ENV_NAME}_weights.h5f', overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "#dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJYVknel2aG6",
    "outputId": "5875c12e-2975-4999-bcf9-a45ab78229f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105.0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znLUV_2ZtXV6",
    "outputId": "3bdea06f-7f71-437e-d8a7-452e97fe4871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 270.000, steps: 715\n",
      "Episode 2: reward: 270.000, steps: 715\n",
      "Episode 3: reward: 270.000, steps: 729\n",
      "Episode 4: reward: 270.000, steps: 719\n",
      "Episode 5: reward: 270.000, steps: 713\n",
      "270.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=5, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de TesisV3 final (profe)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
