## Agentes para juegos

- [Boulder Dash](https://www.boulder-dash.com/boulder-dash-online-game/)

### TODO

Preparar explicación/presentación **actor-critic** código.
  -  Explicar paso a paso el algoritmo
  - Dónde se recolectan las observaciones
  - ¿Qué se almacena en el buffer?
  - ¿Cómo se calcula el advantage_buffer?
  - ¿En qué momento se entrena el actor y el crítico? ¿Al final de las épocas?
  - ¿Cuáles son los inputs?
  - ¿Cómo se calcula el loss en ambos modelos?
  - Otros detalles importantes


### Idea: Simulador automático para entrenar agentes

[track de trayectoria](https://github.com/deepankarkotnala/Object-trajectory-tracking-OpenCV)

- Recompensa es **sparse**. Augmentation of trajectories.

![GeneralFramework](https://i.imgur.com/byclaVc.png)

![GeneralFramework](https://i.imgur.com/YVsLgZf.png)

![TrainedSimulator](https://i.imgur.com/LLd7NTS.png)

**Versión 2.0**

![simulador2.0](https://i.imgur.com/N9IUB0m.png)


**Versión 3.0**

**Simulator**: Red que aprende a simular observaciones a partir de estado+observación
**Critic**: Red que predice retorno/value a partir de una observación
**Actor**: Red que predice retornos para cada acción

Algoritmo de entrenamiento:

1. Recopila trayectorias (s_t,action,reward) corriendo la política (actor) en el ambiente.
2. Calcula los rewards-to-go ($R_t$) para cada estado t
3. $s_t \rightarrow R_{t}$ se usaa para ajustar el crítico (predictor de valores)
4. $(s_t, a_t) \rightarrow s_{t+1}$ se usa para ajustar el simulador.
5. Para ajustar al actor se calculan los valores de ventaja (advantage values). Para un estado s y una acción a, el valor de ventaja se obtiene:
$A(s,a)  = Q(s,a) - V(s)$, es decir cuánto mejoró el estado el hecho de aplicar la acción.
También lo podemos denotar: $A(s,a)=r+\gamma V(s_{​+1} )−V(s)$.
$r+\gamma V(s_{​+1})$ se conoce como TD (temporal difference) target y es una especie de estimación 
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTY1NDkxMTEyMSwxMDE3NDUyMzEsLTE0Mj
I0NjYxMjAsMTI4NDcyMjg2NiwtODMxNTc0MTIyLC00NDg2ODgy
OTAsMjA3MDY2ODUxMSwtNzA1NDYzODMsMTY2NjE0OTQ4OV19
-->