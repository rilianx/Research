# Agentes para juegos

## Giselle

- [Boulder Dash](https://www.boulder-dash.com/boulder-dash-online-game/)

### TODO

- Guardar y cargar modelos (keras). Actor/Critic
- Crear un buffer2 para almacenar observation, act, observation_new, reward.
- Entrenar el simulador:
![image](https://i.imgur.com/eSHiB0Q.png)

Algo así es como se podría continuar:
![image](https://i.imgur.com/goINpvr.png)


### Idea: Simulador automático para entrenar agentes

[track de trayectoria](https://github.com/deepankarkotnala/Object-trajectory-tracking-OpenCV)

- Recompensa es **sparse**. Augmentation of trajectories.

![GeneralFramework](https://i.imgur.com/byclaVc.png)

![GeneralFramework](https://i.imgur.com/YVsLgZf.png)

![TrainedSimulator](https://i.imgur.com/LLd7NTS.png)

**Versión 2.0**

![simulador2.0](https://i.imgur.com/N9IUB0m.png)


**Versión 3.0**

**Simulator (S)**: Red que a partir de estado+acción, predice estado siguiente+reward
**Critic (V)**: Red que predice retorno/value a partir de una observación
**Actor (A)**: Red que predice retornos (advantage values) para cada acción

Algoritmo de entrenamiento:

1. Recopila trayectorias (s_t,action,reward) corriendo la política (actor) en el ambiente.
2. Calcula los rewards-to-go ($R_t$) para cada estado t
3. $s_t \rightarrow R_{t}$ se usa para ajustar el crítico (predictor de valores)
![image](https://i.imgur.com/BZdScFy.png)
5. $(s_t, a_t) \rightarrow s_{t+1},r_{t+1}$ se usa para entrenar el simulador.
6. Para ajustar al actor se calculan los valores de ventaja (advantage values). Para un estado s y una acción a, el valor de ventaja se obtiene:
$A(s,a)  = Q(s,a) - V(s)$, es decir cuánto mejoró el estado el hecho de aplicar la acción.
También lo podemos denotar: $A(s,a)=r+\gamma V(s_{​+1} )−V(s)$.
$r+\gamma V(s_{​+1})$ se conoce como TD (temporal difference) target y es una estimación del $Q(s,a)$.
Ayudados del simulador S, podemos calcular $V(s_{+1})$ para las acciones que **no fueron realizadas**. $s_{+1}(s,a) =S(s,a)$.
El actor se entrena usando $s \rightarrow A(s,a_i)$.
7. Además, una vez bien entrenado el simulador, se puede comenzar a usar en lugar del ambiente para aumentar trayectorias sin tener que jugar y *gastar dinero en fichas*.


## Tifa

[diapos](https://docs.google.com/presentation/d/1ra5IoLmwwnK9jTnuz7_BbRMeBPZpv5epmtEhe9S5nb8/edit#slide=id.p)
[Red neuronal from scratch](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/)
[doc-tifa](https://docs.google.com/document/d/1jdXJ1CsSC8C8bL685cekD8fC06I-Q1YpVABQ6YBcDzQ/edit#heading=h.oxpxc9pk337l)
[ppo](https://keras.io/examples/rl/ppo_cartpole/)
[ejemplo red básica](https://colab.research.google.com/drive/1KYZnwVrdcEEd5dsFew5KbyZ_DPXeoGGv?usp=sharing#scrollTo=witAIKqinkOX)


### Tutorial

[Reinforcement Learning](https://theaisummer.com/Reinforcement_learning/)
[Deep Q-Learning](https://theaisummer.com/Deep_Q_Learning/)
[Deep Q-Learning2](https://theaisummer.com/Taking_Deep_Q_Networks_a_step_further/)
[Actor Critic](https://theaisummer.com/Actor_critics/)
[PPO](https://theaisummer.com/TRPO_PPO/)

### TODO

- Comenzar a implementar red en keras
- Ambiente gym
- Implementar algoritmos: dqn, actor-crítico

---

- Terminar la implementación (backward)
- Estudiar algoritmo de backward (con dibujo)
- Estudiar funciones de activación
	- Cuando conviene usarlas
- **Algoritmo de optimización:** adam
- **Funciones de pérdida (loss):** mse, error absoluto medio








<!--stackedit_data:
eyJoaXN0b3J5IjpbNzcxMzI3NjY5LC0zNzE5ODY2MzAsLTU4MD
U2MDI5Myw3NTc2MTcxNywtNjE1NDk2NDYyLDIwMzQwOTI4MjUs
MTY3MzgyNDA4NywxODQ4NTk5MzU1LDE2MTA3OTI4MTAsLTEwND
AxMjQ5NDMsMTAyOTE3NjczOSwtMzg2ODI2NTE4LDEwMDExMTY5
NTUsLTUzMzE1OTkxMCwxMDE3NDUyMzEsLTE0MjI0NjYxMjAsMT
I4NDcyMjg2NiwtODMxNTc0MTIyLC00NDg2ODgyOTAsMjA3MDY2
ODUxMV19
-->