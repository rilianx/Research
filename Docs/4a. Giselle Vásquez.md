## Agentes para juegos

- [Boulder Dash](https://www.boulder-dash.com/boulder-dash-online-game/)

### TODO

Preparar explicación/presentación **actor-critic** código.
  -  Explicar paso a paso el algoritmo
  - Dónde se recolectan las observaciones
  - ¿Qué se almacena en el buffer?
  - ¿Cómo se calcula el advantage_buffer?
  - ¿En qué momento se entrena el actor y el crítico? ¿Al final de las épocas?
  - ¿Cuáles son los inputs?
  - ¿Cómo se calcula el loss en ambos modelos?
  - Otros detalles importantes


### Idea: Simulador automático para entrenar agentes

[track de trayectoria](https://github.com/deepankarkotnala/Object-trajectory-tracking-OpenCV)

- Recompensa es **sparse**. Augmentation of trajectories.

![GeneralFramework](https://i.imgur.com/byclaVc.png)

![GeneralFramework](https://i.imgur.com/YVsLgZf.png)

![TrainedSimulator](https://i.imgur.com/LLd7NTS.png)

**Versión 2.0**

![simulador2.0](https://i.imgur.com/N9IUB0m.png)


**Versión 3.0**

**Simulator**: Red que aprende a simular observaciones a partir de estado+observación
**Critic**: Red que predice retorno/value a partir de una observación
**Actor**: Red que predice retornos para cada acción

Algoritmo de entrenamiento:

1. Recopila trayectorias (s_t,action,reward) corriendo la política (actor) en el ambiente.
2. Calcula los rewards-to-go (Rt) para cada estado t
3. Los Rt se usan para ajustar el crítico (predictor de valores)
4. 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTUzMjcyMzIyNiwtMTQyMjQ2NjEyMCwxMj
g0NzIyODY2LC04MzE1NzQxMjIsLTQ0ODY4ODI5MCwyMDcwNjY4
NTExLC03MDU0NjM4MywxNjY2MTQ5NDg5XX0=
-->