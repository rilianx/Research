
[**Playing Atari with Six Neurons**](https://arxiv.org/pdf/1806.01363.pdf)

[**Rotation, Translation, and Cropping for Zero-Shot Generalization**](https://arxiv.org/pdf/2001.09908.pdf)

[Github repo](https://github.com/giuse/DNE/tree/six_neurons)

By separating the image processing (learning features) from decision-making, one could better understand the complexity of each task, as well as potentially find **smaller policy representations** that are easier for humans to understand and may generalize better.

The dictionary trained by IDVQ is then used by DRSC to produce a compact code for each observation. This code will be used in turn by the neural network (policy) as input to select the next action. The code is a binary string: a value of ‘1’ indicates that the corresponding centroid contains information also present in the image, and a limited number of centroids are used to represent the totality of the information.

![image](https://i.imgur.com/daRVdGv.png)

**Learning Features from images**

State representations are generated by an **encoder** based on two novel algorithms: 
**Increasing Dictionary Vector Quantization** makes the encoder capable of growing its dictionary size over time 
**Direct Residuals Sparse Coding** encodes observations by disregarding reconstruction error minimization, and aiming instead for highest information inclusion.

As the training progresses and more sophisticated policies are learned, complex interactions with the environment result in increasingly novel observations; the dictionary reflects this by growing in size, including centroids that account for newly discovered features. A larger dictionary corresponds to a larger code, forcing the neural network to grow in input size. This is handled using a specialized version of **Exponential Natural Evolution Strategy** which adapts the dimensionality of the underlying multivariate Gaussian.

### Compressor (Vector Quantization)

The standard VQ algorithm [24] is a dictionary-based encoding technique with applications in dimensionality reduction and compression.

Representative elements in the space (called singularly centroids and collectively called a dictionary) act as references for a surrounding volume, in a manner akin to k-means. The code of an element in the space is then a vector where each position corresponds to a centroid in the dictionary. Its values are traditionally set to zeros, except for the position corresponding to the closest representative centroid in the space.

The original can be reconstructed as a vector product between the code and the dictionary. The difference between the original and its reconstruction is called **reconstruction error**, and quantifies the information lost in the compression/decompression process. The dictionary is trained by adapting the centroids to minimize reconstruction error over a training set.

**Increasing Dictionary VQ**

We introduce Increasing Dictionary VQ (IDVQ, Algorithm 1), a new compressor based on VQ which automatically increases the size of its dictionary over successive training iterations, specifically tailored for online learning. Rather than having a fixed-size dictionary, IDVQ starts with an empty dictionary, thus requiring no initialization, and adds new centroids as the learning progresses.

This is done by **building new centroids** from the positive part of the **reconstruction error**.

Centroids added to the dictionary are not further refined. Each centroid is purposely constructed to represents one particular feature, which was found in an actual observation and was not available in the dictionary before.

**Coding**

The performance of algorithms based on dictionary approaches depends more on the **choice of encoding** than on the dictionary training.

The classic way to construct a sparse code is through an iterative approach [28, 29] where at each step (i) few centroids are selected, (ii) a corresponding code is built and (iii) the code quality is evaluated based on the reconstruction error, with the l_1 norm of the code as a regularization term. This process is repeated over different combinations of centroids to incrementally reduce the reconstruction error, at the cost of the algorithm’s performance.

In our case though the focus is in **differentiating states in order to support the decision maker**, rather than perfecting the reconstruction of the original input. The encoding algorithm will be called on each and **every observation** coming from the environment, prioritizing **distinction** over **precision**.

**Direct Residuals Sparse Coding**

Its key characteristics are: (i) it utilizes centroids constructed as residual images from IDVQ, thus avoiding the centroid-train phase; (ii) it produces binary encodings, reducing the reconstruction process to an unweighted sum over the centroids corresponding to the code’s nonzero coefficients; and (iii) it produces the code in a single pass, terminating early after a small number of centroids are selected. The result is an algorithm with linear performance over dictionary size, which disassembles an observation into its consecutive most similar components as found in the dictionary.

### Controller (ANN)

In order to ensure continuity in training (i.e. the change needs to be transparent to the training algorithm), it is necessary to define an invariance across this change, where the network with expanded weights is equivalent to the previous one. This is done by setting the weights of all new connections to zero, making the new network mathematically equivalent to the previous one, as any input on the new connections cancels out. The same principle can be ported to any neural network application.

### Optimizer

A variation of Exponential Natural Evolution Strategy(XNES; [30]) tailored for evolving networks with dynamic varying size.



## Idea nueva

![image](https://i.imgur.com/EEW0kDY.png)Compresor para la imagen (para pasarle el output a una red multilayer normal). Partir con un juego específico y luego tratar de generalizarlo para juegos similares.
El compresor haría algo como esto:
1. Identificar ubicación del jugador dentro de la imagen
2. Identificar objetos y obtener sus distancias y ángulo con respecto al jugador
3. Si los objetos son muy numerosos, seleccionar subconjunto representativo.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1MzY5NTgyNzUsMTAxMjE0MjkyOSwtMT
MwNDE3MjI1OF19
-->