{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from b1 import *\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import statistics\n",
    "import sklearn.metrics as metrics\n",
    " \n",
    "# Evitar truncar data mostrada al usar jupyter notebook\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    " \n",
    "# Constante que aloja el diccionario JSON con toda la data\n",
    "DATA = None\n",
    "\n",
    "# Obtener data JSON\n",
    "if os.path.exists('./out/dataout.json'):\n",
    "    DATA = json.load(open('./out/dataout.json', 'r'))\n",
    "else:\n",
    "    data_url = urlopen('http://nutriexcel.cl/UMDU/dataout_v2.json')\n",
    "    DATA = json.loads(data_url.read())\n",
    " \n",
    "# Labels base de las columnas\n",
    "LABELS_BASE = {\n",
    "    # Parámetros del alumno (Target)\n",
    "    'p1':                            ['p1'],\n",
    "    'p2':                            ['p2'],\n",
    "    'np':                            ['np'],\n",
    "    'p1p2':                          ['p1p2'], # Promedio p1p2 y p2p2\n",
    "    'p2p2':                          ['p2p2'],\n",
    "    \n",
    "    # Parámetros del laboratorio (Features)\n",
    "    'grade':                         ['g_lab#'],\n",
    "    'attempts':                      ['a_lab#'],\n",
    "    'usedtime':                      ['ut_lab#'],\n",
    "    'activetime':                    ['act_lab#'],\n",
    "    'disconnections':                ['dis_lab#'],      # log\n",
    "    'compilationtime':               ['ct_lab#'],\n",
    "    'runtimedebuggingtime':          ['rt_lab#'],\n",
    "    'compilationtimeratio':          ['ctr_lab#'],\n",
    "    'runtimedebuggingtimeratio':     ['rtr_lab#'],\n",
    "    'errorsreductionratio':          ['err_lab#'],\n",
    "    'compilationerrorsratio':        ['cer_lab#'],\n",
    "    'activequartiles':               ['actq1_lab#','actq2_lab#','actq3_lab#'],\n",
    "    'questionsdifficulty':           ['qd$_lab#'],\n",
    "    'questionsgrades':               ['qg$_lab#'],      # Promedio\n",
    "    'questionsattempts':             ['qat$_lab#'],     # Sumar - Max   # log\n",
    "    'questionsactivetime':           ['qact$_lab#'],    # Promedio\n",
    "    'questionsavgtime':              ['qavt$_lab#'],    # Promedio\n",
    "    'questionsmaxerrors':            ['qme$_lab#'],     # Max\n",
    "    'questionsmaxconsecutiveerrors': ['qmce$_lab#'],    # Max\n",
    "    'questionsmaxsimilarityratio':   ['qmsr$_lab#'],    # Promedio\n",
    "    'questionscorrectness':          ['qc$_lab#']       # Promedio\n",
    "}\n",
    " \n",
    " \n",
    "# Cantidad de preguntas por lab\n",
    "LABS_LENGTHS = {\n",
    "    '1': 7,\n",
    "    '2': 6,\n",
    "    '3': 6,\n",
    "    '4': 5,\n",
    "    '5': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curso  7 : 55\n",
      "curso  13 : 22\n",
      "curso  19 : 54\n",
      "curso  24 : 28\n",
      "curso  30 : 53\n",
      "curso  36 : 41\n",
      "total: 253\n"
     ]
    }
   ],
   "source": [
    "total=0\n",
    "for id in DATA[\"courses\"]:\n",
    "    students=len(DATA[\"courses\"][id][\"students\"])\n",
    "    total+=students\n",
    "    print(\"curso \",id,\":\",students)\n",
    "print(\"total:\",total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Parameters**\n",
    "\n",
    "# Objective vector\n",
    "TARGET = 'mean(p$)'\n",
    "NORM_TYPE = 'col'\n",
    "N_FEATURES = 5\n",
    " \n",
    " \n",
    "# Import needed libraries ----------------------------------------\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import KFold\n",
    " \n",
    "random_state = None # Random state for train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "13\n",
      "19\n",
      "24\n",
      "30\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# CursoData retorna el curso de los alumnos del lab Correspondiente\n",
    "datalab1_all,cursoData = get_custom_dataframe(DATA, [1], ['p1','p2'], 'all', labels=True, index=None)\n",
    "#@title **Data preparation**\n",
    "\n",
    "datalab1 = copy.deepcopy(datalab1_all)\n",
    "\n",
    "# Remove questionsdifficulty\n",
    "remove_col(datalab1, 'qd?')\n",
    "# Group columns\n",
    "datalab1_all = apply(datalab1_all, ['p1','p2'], statistics.mean)\n",
    "datalab1 = apply(datalab1, ['p1','p2'], statistics.mean)\n",
    "datalab1 = apply(datalab1, 'dis_lab1', norm_log)\n",
    "datalab1 = apply(datalab1, 'qg?', statistics.mean)\n",
    "datalab1 = apply(datalab1, 'qat?', sum, replace=False)\n",
    "datalab1 = apply(datalab1, 'sum(qat$_lab1)', norm_log, replace=False)\n",
    "datalab1 = apply(datalab1, 'qat?', max)\n",
    "datalab1 = apply(datalab1, 'qact?', statistics.mean)\n",
    "datalab1 = apply(datalab1, 'qavt?', statistics.mean)\n",
    "datalab1 = apply(datalab1, 'qme?', max)\n",
    "datalab1 = apply(datalab1, 'qmce?', max)\n",
    "datalab1 = apply(datalab1, 'qmsr?', statistics.mean)\n",
    "datalab1 = apply(datalab1, 'qc?', statistics.mean)\n",
    "aux = datalab1['act_lab1'] / datalab1['sum(qat$_lab1)']\n",
    "for i in range(len(aux)):\n",
    "    if not aux[i] > 0:\n",
    "        aux[i] = 0\n",
    "datalab1['avgtime_lab1'] = aux\n",
    "datalab1 = datalab1.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se transforma a dataframe la info de ese curso en particular\n",
    "cursoDF = pd.DataFrame(cursoData,columns=['curso'])\n",
    "\n",
    "# Se concatenan los dos dataframes \n",
    "dfFinlab1 = pd.concat([datalab1,cursoDF],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean(p$)</th>\n",
       "      <th>a_lab1</th>\n",
       "      <th>ut_lab1</th>\n",
       "      <th>act_lab1</th>\n",
       "      <th>norm_log(dis_lab1)</th>\n",
       "      <th>ct_lab1</th>\n",
       "      <th>rt_lab1</th>\n",
       "      <th>ctr_lab1</th>\n",
       "      <th>rtr_lab1</th>\n",
       "      <th>err_lab1</th>\n",
       "      <th>cer_lab1</th>\n",
       "      <th>actq1_lab1</th>\n",
       "      <th>actq2_lab1</th>\n",
       "      <th>actq3_lab1</th>\n",
       "      <th>max(qat$_lab1)</th>\n",
       "      <th>mean(qact$_lab1)</th>\n",
       "      <th>mean(qavt$_lab1)</th>\n",
       "      <th>max(qme$_lab1)</th>\n",
       "      <th>max(qmce$_lab1)</th>\n",
       "      <th>mean(qmsr$_lab1)</th>\n",
       "      <th>mean(qc$_lab1)</th>\n",
       "      <th>sum(qat$_lab1)</th>\n",
       "      <th>norm_log(sum(qat$_lab1))</th>\n",
       "      <th>avgtime_lab1</th>\n",
       "      <th>curso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.90</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.455061</td>\n",
       "      <td>-0.761629</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.049073</td>\n",
       "      <td>-0.890975</td>\n",
       "      <td>0.786036</td>\n",
       "      <td>-0.874950</td>\n",
       "      <td>-0.637584</td>\n",
       "      <td>0.383103</td>\n",
       "      <td>-0.514311</td>\n",
       "      <td>-0.665163</td>\n",
       "      <td>-0.709985</td>\n",
       "      <td>-0.489600</td>\n",
       "      <td>-0.852848</td>\n",
       "      <td>-0.743363</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.475535</td>\n",
       "      <td>0.930445</td>\n",
       "      <td>-0.554857</td>\n",
       "      <td>-0.231774</td>\n",
       "      <td>-0.390801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.15</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.817620</td>\n",
       "      <td>-0.125169</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>-0.603989</td>\n",
       "      <td>-0.385012</td>\n",
       "      <td>-0.690473</td>\n",
       "      <td>-0.336087</td>\n",
       "      <td>0.553319</td>\n",
       "      <td>-0.195346</td>\n",
       "      <td>-1.196970</td>\n",
       "      <td>-1.260748</td>\n",
       "      <td>-1.289504</td>\n",
       "      <td>-0.350438</td>\n",
       "      <td>-0.503732</td>\n",
       "      <td>0.468175</td>\n",
       "      <td>-0.805294</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>-0.975358</td>\n",
       "      <td>0.456763</td>\n",
       "      <td>-0.370575</td>\n",
       "      <td>-0.038347</td>\n",
       "      <td>0.348930</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.016240</td>\n",
       "      <td>2.141457</td>\n",
       "      <td>1.727227</td>\n",
       "      <td>-0.647569</td>\n",
       "      <td>0.618115</td>\n",
       "      <td>-1.033409</td>\n",
       "      <td>-0.374360</td>\n",
       "      <td>1.189007</td>\n",
       "      <td>-0.304597</td>\n",
       "      <td>0.559647</td>\n",
       "      <td>0.838540</td>\n",
       "      <td>0.771844</td>\n",
       "      <td>0.275796</td>\n",
       "      <td>0.436110</td>\n",
       "      <td>0.508278</td>\n",
       "      <td>1.954690</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>-0.207310</td>\n",
       "      <td>-0.793488</td>\n",
       "      <td>0.366554</td>\n",
       "      <td>0.533396</td>\n",
       "      <td>1.668700</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.15</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.911666</td>\n",
       "      <td>-0.877879</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.537167</td>\n",
       "      <td>0.542220</td>\n",
       "      <td>-0.117179</td>\n",
       "      <td>3.331465</td>\n",
       "      <td>-0.191185</td>\n",
       "      <td>-0.496710</td>\n",
       "      <td>-1.108546</td>\n",
       "      <td>-1.255053</td>\n",
       "      <td>-1.286561</td>\n",
       "      <td>-0.837508</td>\n",
       "      <td>-0.257596</td>\n",
       "      <td>1.066704</td>\n",
       "      <td>-0.673866</td>\n",
       "      <td>-0.631388</td>\n",
       "      <td>0.653245</td>\n",
       "      <td>0.519023</td>\n",
       "      <td>-0.739140</td>\n",
       "      <td>-0.459359</td>\n",
       "      <td>-0.220591</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.885970</td>\n",
       "      <td>-0.428801</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>0.253080</td>\n",
       "      <td>-0.496104</td>\n",
       "      <td>0.812065</td>\n",
       "      <td>-0.275336</td>\n",
       "      <td>-0.042385</td>\n",
       "      <td>0.195740</td>\n",
       "      <td>1.020738</td>\n",
       "      <td>0.854618</td>\n",
       "      <td>0.777731</td>\n",
       "      <td>-0.350438</td>\n",
       "      <td>-0.519575</td>\n",
       "      <td>-0.314631</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.578996</td>\n",
       "      <td>0.505561</td>\n",
       "      <td>-0.481144</td>\n",
       "      <td>-0.150910</td>\n",
       "      <td>0.074525</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.30</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.223061</td>\n",
       "      <td>0.834033</td>\n",
       "      <td>0.802062</td>\n",
       "      <td>1.102885</td>\n",
       "      <td>-0.547800</td>\n",
       "      <td>0.596023</td>\n",
       "      <td>-0.989770</td>\n",
       "      <td>-0.457405</td>\n",
       "      <td>0.725633</td>\n",
       "      <td>-0.137240</td>\n",
       "      <td>-0.287981</td>\n",
       "      <td>0.113180</td>\n",
       "      <td>0.484540</td>\n",
       "      <td>0.449124</td>\n",
       "      <td>0.280275</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>-0.127584</td>\n",
       "      <td>-0.066559</td>\n",
       "      <td>0.440267</td>\n",
       "      <td>0.578680</td>\n",
       "      <td>0.190098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.30</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.525164</td>\n",
       "      <td>1.814468</td>\n",
       "      <td>1.834012</td>\n",
       "      <td>3.475076</td>\n",
       "      <td>0.554319</td>\n",
       "      <td>1.625480</td>\n",
       "      <td>-0.308141</td>\n",
       "      <td>-0.973142</td>\n",
       "      <td>2.074644</td>\n",
       "      <td>-0.264286</td>\n",
       "      <td>-0.396178</td>\n",
       "      <td>0.761379</td>\n",
       "      <td>4.311521</td>\n",
       "      <td>2.609458</td>\n",
       "      <td>0.865472</td>\n",
       "      <td>0.246129</td>\n",
       "      <td>4.344975</td>\n",
       "      <td>0.193146</td>\n",
       "      <td>0.299430</td>\n",
       "      <td>3.241359</td>\n",
       "      <td>1.662132</td>\n",
       "      <td>-0.970884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.60</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.259429</td>\n",
       "      <td>0.711943</td>\n",
       "      <td>1.007758</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>-0.317917</td>\n",
       "      <td>0.142463</td>\n",
       "      <td>-0.698772</td>\n",
       "      <td>-0.682122</td>\n",
       "      <td>1.538945</td>\n",
       "      <td>-0.123689</td>\n",
       "      <td>-0.268217</td>\n",
       "      <td>-0.290390</td>\n",
       "      <td>1.528262</td>\n",
       "      <td>1.082286</td>\n",
       "      <td>-0.103934</td>\n",
       "      <td>-0.016727</td>\n",
       "      <td>2.722248</td>\n",
       "      <td>0.455451</td>\n",
       "      <td>0.113491</td>\n",
       "      <td>1.435392</td>\n",
       "      <td>1.073311</td>\n",
       "      <td>-0.787474</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.60</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.911666</td>\n",
       "      <td>-0.877879</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.470344</td>\n",
       "      <td>-0.636893</td>\n",
       "      <td>0.038346</td>\n",
       "      <td>-0.051771</td>\n",
       "      <td>1.214819</td>\n",
       "      <td>-0.448682</td>\n",
       "      <td>-0.939152</td>\n",
       "      <td>-1.083546</td>\n",
       "      <td>-1.122386</td>\n",
       "      <td>-0.698345</td>\n",
       "      <td>-0.886232</td>\n",
       "      <td>-0.533004</td>\n",
       "      <td>-0.016727</td>\n",
       "      <td>-0.631388</td>\n",
       "      <td>-0.011951</td>\n",
       "      <td>0.042817</td>\n",
       "      <td>-0.849709</td>\n",
       "      <td>-0.617982</td>\n",
       "      <td>0.102401</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.00</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>2.519935</td>\n",
       "      <td>1.185957</td>\n",
       "      <td>-0.329437</td>\n",
       "      <td>3.794682</td>\n",
       "      <td>-0.881138</td>\n",
       "      <td>1.618888</td>\n",
       "      <td>-0.240785</td>\n",
       "      <td>0.224240</td>\n",
       "      <td>-0.280886</td>\n",
       "      <td>0.404077</td>\n",
       "      <td>0.386915</td>\n",
       "      <td>1.180355</td>\n",
       "      <td>2.582864</td>\n",
       "      <td>0.545034</td>\n",
       "      <td>1.034696</td>\n",
       "      <td>-0.090479</td>\n",
       "      <td>-0.273038</td>\n",
       "      <td>-0.788439</td>\n",
       "      <td>1.361679</td>\n",
       "      <td>1.042388</td>\n",
       "      <td>0.534182</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.60</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.909336</td>\n",
       "      <td>-0.739865</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.946816</td>\n",
       "      <td>-0.714988</td>\n",
       "      <td>-1.092625</td>\n",
       "      <td>-0.445439</td>\n",
       "      <td>2.339420</td>\n",
       "      <td>-1.288383</td>\n",
       "      <td>-1.218991</td>\n",
       "      <td>-1.361240</td>\n",
       "      <td>-1.390234</td>\n",
       "      <td>-0.976671</td>\n",
       "      <td>-1.111997</td>\n",
       "      <td>-0.934640</td>\n",
       "      <td>-1.068150</td>\n",
       "      <td>-0.847752</td>\n",
       "      <td>-3.661702</td>\n",
       "      <td>0.711693</td>\n",
       "      <td>-1.070848</td>\n",
       "      <td>-1.009106</td>\n",
       "      <td>1.611617</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.30</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.210770</td>\n",
       "      <td>-1.174611</td>\n",
       "      <td>0.558591</td>\n",
       "      <td>-0.521187</td>\n",
       "      <td>-0.774383</td>\n",
       "      <td>0.401454</td>\n",
       "      <td>0.067909</td>\n",
       "      <td>-0.191185</td>\n",
       "      <td>-0.107735</td>\n",
       "      <td>1.026159</td>\n",
       "      <td>0.854953</td>\n",
       "      <td>0.772498</td>\n",
       "      <td>-0.211275</td>\n",
       "      <td>-0.958658</td>\n",
       "      <td>-0.739180</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>-0.415025</td>\n",
       "      <td>0.606992</td>\n",
       "      <td>0.881646</td>\n",
       "      <td>-0.260006</td>\n",
       "      <td>0.065289</td>\n",
       "      <td>-1.483413</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.60</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.448061</td>\n",
       "      <td>-0.285478</td>\n",
       "      <td>0.802062</td>\n",
       "      <td>0.011938</td>\n",
       "      <td>0.656612</td>\n",
       "      <td>0.273260</td>\n",
       "      <td>1.691789</td>\n",
       "      <td>-0.339985</td>\n",
       "      <td>0.407908</td>\n",
       "      <td>-0.820915</td>\n",
       "      <td>-0.674207</td>\n",
       "      <td>-0.718488</td>\n",
       "      <td>0.067051</td>\n",
       "      <td>0.236938</td>\n",
       "      <td>0.661192</td>\n",
       "      <td>-0.279583</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>-0.212179</td>\n",
       "      <td>-0.313075</td>\n",
       "      <td>0.145416</td>\n",
       "      <td>0.386676</td>\n",
       "      <td>-0.680124</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.10</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.889654</td>\n",
       "      <td>-1.018548</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.691148</td>\n",
       "      <td>-1.173654</td>\n",
       "      <td>-0.350792</td>\n",
       "      <td>-1.574197</td>\n",
       "      <td>-0.191185</td>\n",
       "      <td>0.169879</td>\n",
       "      <td>-1.204423</td>\n",
       "      <td>-1.341142</td>\n",
       "      <td>-1.371265</td>\n",
       "      <td>-0.837508</td>\n",
       "      <td>-1.242704</td>\n",
       "      <td>-1.227669</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>-0.415025</td>\n",
       "      <td>0.753663</td>\n",
       "      <td>0.772270</td>\n",
       "      <td>-0.960278</td>\n",
       "      <td>-0.798859</td>\n",
       "      <td>0.098353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.40</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.186075</td>\n",
       "      <td>-0.547175</td>\n",
       "      <td>1.611097</td>\n",
       "      <td>0.129604</td>\n",
       "      <td>-0.451007</td>\n",
       "      <td>0.776275</td>\n",
       "      <td>-0.052379</td>\n",
       "      <td>0.138806</td>\n",
       "      <td>0.738828</td>\n",
       "      <td>0.696179</td>\n",
       "      <td>0.569220</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>-0.072112</td>\n",
       "      <td>-0.185170</td>\n",
       "      <td>0.120644</td>\n",
       "      <td>0.771840</td>\n",
       "      <td>-0.306843</td>\n",
       "      <td>0.202275</td>\n",
       "      <td>0.192578</td>\n",
       "      <td>-0.149436</td>\n",
       "      <td>0.161162</td>\n",
       "      <td>-0.677507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.95</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.905267</td>\n",
       "      <td>-0.855585</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>-0.820435</td>\n",
       "      <td>-0.349814</td>\n",
       "      <td>-0.786782</td>\n",
       "      <td>0.719162</td>\n",
       "      <td>0.652519</td>\n",
       "      <td>-0.983853</td>\n",
       "      <td>0.310977</td>\n",
       "      <td>0.151171</td>\n",
       "      <td>0.084401</td>\n",
       "      <td>-0.907089</td>\n",
       "      <td>-0.812674</td>\n",
       "      <td>0.219207</td>\n",
       "      <td>-0.936722</td>\n",
       "      <td>-0.847752</td>\n",
       "      <td>-0.975358</td>\n",
       "      <td>-0.879306</td>\n",
       "      <td>-1.181417</td>\n",
       "      <td>-1.260238</td>\n",
       "      <td>2.065196</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.10</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.193974</td>\n",
       "      <td>1.365390</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.528451</td>\n",
       "      <td>1.681737</td>\n",
       "      <td>-0.893502</td>\n",
       "      <td>0.895948</td>\n",
       "      <td>-0.697306</td>\n",
       "      <td>0.075933</td>\n",
       "      <td>-0.506180</td>\n",
       "      <td>-0.649754</td>\n",
       "      <td>-0.685130</td>\n",
       "      <td>0.554121</td>\n",
       "      <td>1.012124</td>\n",
       "      <td>-0.050041</td>\n",
       "      <td>0.377556</td>\n",
       "      <td>-0.198661</td>\n",
       "      <td>0.083599</td>\n",
       "      <td>0.065534</td>\n",
       "      <td>0.513980</td>\n",
       "      <td>0.622541</td>\n",
       "      <td>0.617778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.45</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.895874</td>\n",
       "      <td>-0.883718</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.371564</td>\n",
       "      <td>-1.212151</td>\n",
       "      <td>0.276514</td>\n",
       "      <td>-1.702382</td>\n",
       "      <td>1.777120</td>\n",
       "      <td>0.360936</td>\n",
       "      <td>1.020738</td>\n",
       "      <td>0.858638</td>\n",
       "      <td>0.774787</td>\n",
       "      <td>-0.976671</td>\n",
       "      <td>-1.143684</td>\n",
       "      <td>-0.764273</td>\n",
       "      <td>3.794680</td>\n",
       "      <td>-0.631388</td>\n",
       "      <td>0.274090</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-1.070848</td>\n",
       "      <td>-1.009106</td>\n",
       "      <td>1.090601</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.60</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.257496</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.558591</td>\n",
       "      <td>0.574117</td>\n",
       "      <td>0.280439</td>\n",
       "      <td>0.702743</td>\n",
       "      <td>0.550274</td>\n",
       "      <td>-0.444245</td>\n",
       "      <td>-0.448682</td>\n",
       "      <td>-0.440456</td>\n",
       "      <td>-0.589793</td>\n",
       "      <td>-0.452603</td>\n",
       "      <td>-0.350438</td>\n",
       "      <td>0.299179</td>\n",
       "      <td>1.096272</td>\n",
       "      <td>2.480402</td>\n",
       "      <td>-0.306843</td>\n",
       "      <td>-0.119672</td>\n",
       "      <td>-0.220526</td>\n",
       "      <td>-0.038867</td>\n",
       "      <td>0.250436</td>\n",
       "      <td>-0.030432</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.75</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.338202</td>\n",
       "      <td>0.105210</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>-0.513924</td>\n",
       "      <td>0.043957</td>\n",
       "      <td>-0.648176</td>\n",
       "      <td>0.134735</td>\n",
       "      <td>-0.488784</td>\n",
       "      <td>0.294963</td>\n",
       "      <td>-0.213806</td>\n",
       "      <td>0.841554</td>\n",
       "      <td>0.765630</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>-0.079360</td>\n",
       "      <td>-0.155502</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>0.666793</td>\n",
       "      <td>0.523614</td>\n",
       "      <td>0.061327</td>\n",
       "      <td>0.550837</td>\n",
       "      <td>0.644018</td>\n",
       "      <td>-0.652784</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.05</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.242991</td>\n",
       "      <td>0.082915</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.458723</td>\n",
       "      <td>-0.343215</td>\n",
       "      <td>-0.575294</td>\n",
       "      <td>-0.417493</td>\n",
       "      <td>-1.118905</td>\n",
       "      <td>1.316749</td>\n",
       "      <td>-0.141644</td>\n",
       "      <td>-0.291666</td>\n",
       "      <td>-0.345333</td>\n",
       "      <td>2.224077</td>\n",
       "      <td>-0.043147</td>\n",
       "      <td>-0.567184</td>\n",
       "      <td>-0.805294</td>\n",
       "      <td>3.154975</td>\n",
       "      <td>0.897901</td>\n",
       "      <td>0.334767</td>\n",
       "      <td>1.214253</td>\n",
       "      <td>0.978473</td>\n",
       "      <td>-1.116212</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.05</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.256895</td>\n",
       "      <td>-1.705436</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.845130</td>\n",
       "      <td>-1.127457</td>\n",
       "      <td>0.965638</td>\n",
       "      <td>-0.424176</td>\n",
       "      <td>2.339420</td>\n",
       "      <td>-2.128084</td>\n",
       "      <td>0.366199</td>\n",
       "      <td>0.842894</td>\n",
       "      <td>0.770209</td>\n",
       "      <td>-0.976671</td>\n",
       "      <td>-1.284575</td>\n",
       "      <td>-1.317190</td>\n",
       "      <td>0.771840</td>\n",
       "      <td>-0.847752</td>\n",
       "      <td>0.389114</td>\n",
       "      <td>-0.142280</td>\n",
       "      <td>-1.255130</td>\n",
       "      <td>-1.459747</td>\n",
       "      <td>-1.565690</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.40</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.461146</td>\n",
       "      <td>-0.098097</td>\n",
       "      <td>0.802062</td>\n",
       "      <td>-0.930837</td>\n",
       "      <td>0.508123</td>\n",
       "      <td>-1.130368</td>\n",
       "      <td>1.097035</td>\n",
       "      <td>-0.191185</td>\n",
       "      <td>-1.763387</td>\n",
       "      <td>-0.510246</td>\n",
       "      <td>-0.660473</td>\n",
       "      <td>-0.694614</td>\n",
       "      <td>-0.628763</td>\n",
       "      <td>-0.427345</td>\n",
       "      <td>0.613374</td>\n",
       "      <td>-0.805294</td>\n",
       "      <td>-0.739570</td>\n",
       "      <td>0.809654</td>\n",
       "      <td>0.749554</td>\n",
       "      <td>-0.739140</td>\n",
       "      <td>-0.459359</td>\n",
       "      <td>1.586931</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.25</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.886042</td>\n",
       "      <td>0.639751</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.275688</td>\n",
       "      <td>1.350661</td>\n",
       "      <td>-0.553169</td>\n",
       "      <td>1.256203</td>\n",
       "      <td>-0.081357</td>\n",
       "      <td>0.070656</td>\n",
       "      <td>1.021755</td>\n",
       "      <td>0.858303</td>\n",
       "      <td>0.782309</td>\n",
       "      <td>0.206214</td>\n",
       "      <td>0.849165</td>\n",
       "      <td>0.578307</td>\n",
       "      <td>1.166123</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.416501</td>\n",
       "      <td>0.261569</td>\n",
       "      <td>0.808832</td>\n",
       "      <td>0.784528</td>\n",
       "      <td>-0.392760</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.55</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.125654</td>\n",
       "      <td>0.410965</td>\n",
       "      <td>0.802062</td>\n",
       "      <td>3.005867</td>\n",
       "      <td>-1.069162</td>\n",
       "      <td>2.893105</td>\n",
       "      <td>-1.520736</td>\n",
       "      <td>-0.881534</td>\n",
       "      <td>1.614418</td>\n",
       "      <td>0.695501</td>\n",
       "      <td>1.226106</td>\n",
       "      <td>1.142056</td>\n",
       "      <td>0.554121</td>\n",
       "      <td>0.245426</td>\n",
       "      <td>-0.164137</td>\n",
       "      <td>-0.411011</td>\n",
       "      <td>1.748611</td>\n",
       "      <td>0.504748</td>\n",
       "      <td>-0.076655</td>\n",
       "      <td>0.550837</td>\n",
       "      <td>0.644018</td>\n",
       "      <td>-0.357477</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.20</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>2.163171</td>\n",
       "      <td>-0.150648</td>\n",
       "      <td>1.343199</td>\n",
       "      <td>2.122651</td>\n",
       "      <td>-0.722687</td>\n",
       "      <td>3.025203</td>\n",
       "      <td>-0.881633</td>\n",
       "      <td>0.138806</td>\n",
       "      <td>0.725633</td>\n",
       "      <td>-0.990309</td>\n",
       "      <td>-0.444749</td>\n",
       "      <td>-0.449660</td>\n",
       "      <td>-0.628763</td>\n",
       "      <td>0.269191</td>\n",
       "      <td>1.016213</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>-0.198661</td>\n",
       "      <td>0.498053</td>\n",
       "      <td>0.348228</td>\n",
       "      <td>-0.260006</td>\n",
       "      <td>0.065289</td>\n",
       "      <td>0.078115</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.95</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.894458</td>\n",
       "      <td>-0.257875</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>-0.646116</td>\n",
       "      <td>0.196846</td>\n",
       "      <td>-0.705440</td>\n",
       "      <td>0.813934</td>\n",
       "      <td>0.652519</td>\n",
       "      <td>-0.848741</td>\n",
       "      <td>1.024465</td>\n",
       "      <td>0.856293</td>\n",
       "      <td>0.779039</td>\n",
       "      <td>-0.837508</td>\n",
       "      <td>-0.442622</td>\n",
       "      <td>1.845819</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.337992</td>\n",
       "      <td>0.389455</td>\n",
       "      <td>-0.997135</td>\n",
       "      <td>-0.865233</td>\n",
       "      <td>2.696585</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.00</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.327727</td>\n",
       "      <td>1.612754</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>0.251627</td>\n",
       "      <td>1.504650</td>\n",
       "      <td>-0.391137</td>\n",
       "      <td>0.565461</td>\n",
       "      <td>-0.534841</td>\n",
       "      <td>0.708216</td>\n",
       "      <td>-0.514311</td>\n",
       "      <td>-0.655783</td>\n",
       "      <td>-0.692325</td>\n",
       "      <td>1.389099</td>\n",
       "      <td>1.501565</td>\n",
       "      <td>-0.211464</td>\n",
       "      <td>-0.411011</td>\n",
       "      <td>0.991339</td>\n",
       "      <td>0.158456</td>\n",
       "      <td>-0.089275</td>\n",
       "      <td>2.504229</td>\n",
       "      <td>1.453049</td>\n",
       "      <td>-0.787902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.55</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.726221</td>\n",
       "      <td>0.209251</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.717296</td>\n",
       "      <td>0.487224</td>\n",
       "      <td>-0.907818</td>\n",
       "      <td>0.660233</td>\n",
       "      <td>0.419703</td>\n",
       "      <td>0.031072</td>\n",
       "      <td>-0.508891</td>\n",
       "      <td>-0.657458</td>\n",
       "      <td>-0.699193</td>\n",
       "      <td>0.484540</td>\n",
       "      <td>-0.091808</td>\n",
       "      <td>-0.489942</td>\n",
       "      <td>-0.411011</td>\n",
       "      <td>-0.415025</td>\n",
       "      <td>0.535786</td>\n",
       "      <td>0.362531</td>\n",
       "      <td>0.108559</td>\n",
       "      <td>0.360541</td>\n",
       "      <td>-0.043074</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.80</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.907453</td>\n",
       "      <td>-1.196375</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>-0.978775</td>\n",
       "      <td>-0.845878</td>\n",
       "      <td>-1.094578</td>\n",
       "      <td>-0.175704</td>\n",
       "      <td>2.339420</td>\n",
       "      <td>-1.796109</td>\n",
       "      <td>-1.192227</td>\n",
       "      <td>-1.337457</td>\n",
       "      <td>-1.370938</td>\n",
       "      <td>-1.046252</td>\n",
       "      <td>-1.191779</td>\n",
       "      <td>-0.840162</td>\n",
       "      <td>-1.068150</td>\n",
       "      <td>-0.847752</td>\n",
       "      <td>-0.096545</td>\n",
       "      <td>0.683087</td>\n",
       "      <td>-1.181417</td>\n",
       "      <td>-1.260238</td>\n",
       "      <td>0.546071</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.00</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.495115</td>\n",
       "      <td>0.697611</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>0.826880</td>\n",
       "      <td>0.804001</td>\n",
       "      <td>0.440498</td>\n",
       "      <td>0.573359</td>\n",
       "      <td>-0.444245</td>\n",
       "      <td>0.031072</td>\n",
       "      <td>-0.506180</td>\n",
       "      <td>-0.650424</td>\n",
       "      <td>-0.690690</td>\n",
       "      <td>-0.698345</td>\n",
       "      <td>0.643769</td>\n",
       "      <td>2.462647</td>\n",
       "      <td>-0.805294</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.380594</td>\n",
       "      <td>-0.284469</td>\n",
       "      <td>-0.333719</td>\n",
       "      <td>-0.002896</td>\n",
       "      <td>1.591752</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.45</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>2.302640</td>\n",
       "      <td>0.252248</td>\n",
       "      <td>1.483889</td>\n",
       "      <td>0.667087</td>\n",
       "      <td>0.622514</td>\n",
       "      <td>0.614894</td>\n",
       "      <td>0.796924</td>\n",
       "      <td>-0.659853</td>\n",
       "      <td>0.800578</td>\n",
       "      <td>1.831120</td>\n",
       "      <td>1.658558</td>\n",
       "      <td>1.559035</td>\n",
       "      <td>-0.141693</td>\n",
       "      <td>0.234675</td>\n",
       "      <td>-0.480760</td>\n",
       "      <td>0.377556</td>\n",
       "      <td>0.450430</td>\n",
       "      <td>-1.878515</td>\n",
       "      <td>-1.002143</td>\n",
       "      <td>0.108559</td>\n",
       "      <td>0.360541</td>\n",
       "      <td>0.008835</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.70</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.065956</td>\n",
       "      <td>-0.415530</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>-0.176907</td>\n",
       "      <td>-0.603896</td>\n",
       "      <td>0.104720</td>\n",
       "      <td>-0.500723</td>\n",
       "      <td>0.989596</td>\n",
       "      <td>0.383103</td>\n",
       "      <td>1.014979</td>\n",
       "      <td>0.847919</td>\n",
       "      <td>0.768573</td>\n",
       "      <td>-0.767926</td>\n",
       "      <td>-0.451110</td>\n",
       "      <td>0.529429</td>\n",
       "      <td>-0.411011</td>\n",
       "      <td>-0.415025</td>\n",
       "      <td>0.429890</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>-0.554857</td>\n",
       "      <td>-0.231774</td>\n",
       "      <td>0.277739</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.30</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>2.195294</td>\n",
       "      <td>1.230029</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>1.108695</td>\n",
       "      <td>1.476052</td>\n",
       "      <td>0.365664</td>\n",
       "      <td>0.815756</td>\n",
       "      <td>-0.536360</td>\n",
       "      <td>0.306574</td>\n",
       "      <td>1.030563</td>\n",
       "      <td>0.869357</td>\n",
       "      <td>0.830057</td>\n",
       "      <td>0.206214</td>\n",
       "      <td>1.259956</td>\n",
       "      <td>1.358651</td>\n",
       "      <td>0.114701</td>\n",
       "      <td>0.125884</td>\n",
       "      <td>-0.517694</td>\n",
       "      <td>-0.076655</td>\n",
       "      <td>0.808832</td>\n",
       "      <td>0.784528</td>\n",
       "      <td>0.117781</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.55</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>2.112675</td>\n",
       "      <td>-0.661303</td>\n",
       "      <td>0.802062</td>\n",
       "      <td>-1.026713</td>\n",
       "      <td>-0.470805</td>\n",
       "      <td>-1.256610</td>\n",
       "      <td>0.052721</td>\n",
       "      <td>0.532062</td>\n",
       "      <td>-0.672461</td>\n",
       "      <td>-1.049936</td>\n",
       "      <td>0.893475</td>\n",
       "      <td>1.659110</td>\n",
       "      <td>-0.350438</td>\n",
       "      <td>-0.784948</td>\n",
       "      <td>-0.911407</td>\n",
       "      <td>-0.673866</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.771921</td>\n",
       "      <td>-0.069924</td>\n",
       "      <td>-0.554857</td>\n",
       "      <td>-0.231774</td>\n",
       "      <td>-0.197006</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6.90</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.502599</td>\n",
       "      <td>-0.065716</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>-0.865467</td>\n",
       "      <td>0.087954</td>\n",
       "      <td>-1.047725</td>\n",
       "      <td>0.375918</td>\n",
       "      <td>-0.191185</td>\n",
       "      <td>-0.983853</td>\n",
       "      <td>-0.505503</td>\n",
       "      <td>-0.654443</td>\n",
       "      <td>-0.695595</td>\n",
       "      <td>-0.698345</td>\n",
       "      <td>-0.419423</td>\n",
       "      <td>0.196641</td>\n",
       "      <td>2.217546</td>\n",
       "      <td>-0.739570</td>\n",
       "      <td>0.205927</td>\n",
       "      <td>-0.394686</td>\n",
       "      <td>-0.702283</td>\n",
       "      <td>-0.410581</td>\n",
       "      <td>1.498481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.75</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.927485</td>\n",
       "      <td>-1.814787</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-1.026713</td>\n",
       "      <td>-1.212151</td>\n",
       "      <td>-1.256610</td>\n",
       "      <td>-1.702382</td>\n",
       "      <td>-2.721790</td>\n",
       "      <td>-2.607838</td>\n",
       "      <td>1.983574</td>\n",
       "      <td>1.799248</td>\n",
       "      <td>1.691160</td>\n",
       "      <td>-1.185415</td>\n",
       "      <td>-1.398873</td>\n",
       "      <td>-1.815898</td>\n",
       "      <td>-1.331006</td>\n",
       "      <td>-0.955934</td>\n",
       "      <td>1.085348</td>\n",
       "      <td>0.466018</td>\n",
       "      <td>-1.402556</td>\n",
       "      <td>-1.984394</td>\n",
       "      <td>-1.886085</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.70</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.375164</td>\n",
       "      <td>0.763433</td>\n",
       "      <td>0.558591</td>\n",
       "      <td>-0.608347</td>\n",
       "      <td>0.533421</td>\n",
       "      <td>-0.883090</td>\n",
       "      <td>0.219179</td>\n",
       "      <td>-0.238760</td>\n",
       "      <td>0.197851</td>\n",
       "      <td>-0.515667</td>\n",
       "      <td>-0.663488</td>\n",
       "      <td>-0.703444</td>\n",
       "      <td>0.693284</td>\n",
       "      <td>0.454216</td>\n",
       "      <td>-0.469507</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>0.558612</td>\n",
       "      <td>0.734797</td>\n",
       "      <td>1.080205</td>\n",
       "      <td>1.251110</td>\n",
       "      <td>0.994775</td>\n",
       "      <td>-0.635672</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.55</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.375779</td>\n",
       "      <td>0.821824</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>0.398345</td>\n",
       "      <td>1.214271</td>\n",
       "      <td>-0.011760</td>\n",
       "      <td>0.911743</td>\n",
       "      <td>0.277483</td>\n",
       "      <td>0.087017</td>\n",
       "      <td>2.114685</td>\n",
       "      <td>1.940607</td>\n",
       "      <td>1.839310</td>\n",
       "      <td>-0.141693</td>\n",
       "      <td>0.932342</td>\n",
       "      <td>2.876394</td>\n",
       "      <td>-0.016727</td>\n",
       "      <td>-0.198661</td>\n",
       "      <td>0.336775</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>0.071703</td>\n",
       "      <td>0.333888</td>\n",
       "      <td>0.766382</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6.00</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.887744</td>\n",
       "      <td>0.538894</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.364300</td>\n",
       "      <td>0.047257</td>\n",
       "      <td>-0.610433</td>\n",
       "      <td>-0.188461</td>\n",
       "      <td>0.599376</td>\n",
       "      <td>0.175156</td>\n",
       "      <td>-1.074328</td>\n",
       "      <td>-1.212512</td>\n",
       "      <td>-1.241756</td>\n",
       "      <td>0.414959</td>\n",
       "      <td>-0.168761</td>\n",
       "      <td>-0.985537</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>-0.198661</td>\n",
       "      <td>-0.498828</td>\n",
       "      <td>-0.355984</td>\n",
       "      <td>0.366554</td>\n",
       "      <td>0.533396</td>\n",
       "      <td>-0.019804</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6.80</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.768818</td>\n",
       "      <td>-0.818427</td>\n",
       "      <td>1.007758</td>\n",
       "      <td>-0.479060</td>\n",
       "      <td>-0.358614</td>\n",
       "      <td>-0.052106</td>\n",
       "      <td>0.612240</td>\n",
       "      <td>0.268879</td>\n",
       "      <td>-0.385348</td>\n",
       "      <td>-0.940168</td>\n",
       "      <td>-1.089241</td>\n",
       "      <td>-1.063845</td>\n",
       "      <td>0.067051</td>\n",
       "      <td>-0.621990</td>\n",
       "      <td>-0.945704</td>\n",
       "      <td>2.348974</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.040997</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>-0.260006</td>\n",
       "      <td>0.065289</td>\n",
       "      <td>-0.940237</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.75</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.340774</td>\n",
       "      <td>0.039387</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>-0.165286</td>\n",
       "      <td>0.149549</td>\n",
       "      <td>-0.200473</td>\n",
       "      <td>0.356477</td>\n",
       "      <td>-0.494858</td>\n",
       "      <td>-0.124096</td>\n",
       "      <td>1.013285</td>\n",
       "      <td>0.847919</td>\n",
       "      <td>0.770863</td>\n",
       "      <td>-0.211275</td>\n",
       "      <td>-0.224778</td>\n",
       "      <td>0.656359</td>\n",
       "      <td>0.640412</td>\n",
       "      <td>0.450430</td>\n",
       "      <td>0.537612</td>\n",
       "      <td>0.797511</td>\n",
       "      <td>0.219128</td>\n",
       "      <td>0.437394</td>\n",
       "      <td>-0.385855</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.10</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.880628</td>\n",
       "      <td>-0.763221</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.830604</td>\n",
       "      <td>-1.015266</td>\n",
       "      <td>-0.845998</td>\n",
       "      <td>-1.194502</td>\n",
       "      <td>1.495717</td>\n",
       "      <td>-1.024492</td>\n",
       "      <td>1.339877</td>\n",
       "      <td>1.169160</td>\n",
       "      <td>1.078610</td>\n",
       "      <td>-0.628763</td>\n",
       "      <td>-1.110300</td>\n",
       "      <td>-1.105338</td>\n",
       "      <td>-0.673866</td>\n",
       "      <td>-0.631388</td>\n",
       "      <td>-0.224959</td>\n",
       "      <td>-0.187713</td>\n",
       "      <td>-0.923422</td>\n",
       "      <td>-0.735720</td>\n",
       "      <td>0.703780</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.25</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>2.187353</td>\n",
       "      <td>-0.276985</td>\n",
       "      <td>1.483889</td>\n",
       "      <td>1.142106</td>\n",
       "      <td>-1.187953</td>\n",
       "      <td>1.921563</td>\n",
       "      <td>-1.658641</td>\n",
       "      <td>-0.272670</td>\n",
       "      <td>1.590668</td>\n",
       "      <td>1.835863</td>\n",
       "      <td>1.659898</td>\n",
       "      <td>1.561324</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>-0.090111</td>\n",
       "      <td>-0.078546</td>\n",
       "      <td>-0.016727</td>\n",
       "      <td>0.774975</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>-0.557067</td>\n",
       "      <td>-0.038867</td>\n",
       "      <td>0.250436</td>\n",
       "      <td>-0.451997</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4.55</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.537822</td>\n",
       "      <td>0.152984</td>\n",
       "      <td>-0.123236</td>\n",
       "      <td>-0.110085</td>\n",
       "      <td>-0.121031</td>\n",
       "      <td>-0.194616</td>\n",
       "      <td>-0.144113</td>\n",
       "      <td>-0.744881</td>\n",
       "      <td>1.152609</td>\n",
       "      <td>-0.514311</td>\n",
       "      <td>-0.662483</td>\n",
       "      <td>-0.703772</td>\n",
       "      <td>0.345377</td>\n",
       "      <td>0.351801</td>\n",
       "      <td>-0.441093</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>0.666793</td>\n",
       "      <td>-0.722791</td>\n",
       "      <td>-0.683270</td>\n",
       "      <td>1.287966</td>\n",
       "      <td>1.010819</td>\n",
       "      <td>-1.102280</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.451735</td>\n",
       "      <td>-0.984575</td>\n",
       "      <td>1.185957</td>\n",
       "      <td>-0.689696</td>\n",
       "      <td>-0.997667</td>\n",
       "      <td>-0.380725</td>\n",
       "      <td>-1.015286</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>-0.320959</td>\n",
       "      <td>-0.824980</td>\n",
       "      <td>-0.666168</td>\n",
       "      <td>-0.709004</td>\n",
       "      <td>-0.767926</td>\n",
       "      <td>-1.006187</td>\n",
       "      <td>-0.709677</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>-0.523207</td>\n",
       "      <td>0.810263</td>\n",
       "      <td>0.797511</td>\n",
       "      <td>-0.554857</td>\n",
       "      <td>-0.231774</td>\n",
       "      <td>-0.821456</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6.65</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.528498</td>\n",
       "      <td>0.481565</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>1.217645</td>\n",
       "      <td>1.206572</td>\n",
       "      <td>0.984509</td>\n",
       "      <td>1.275643</td>\n",
       "      <td>-0.462466</td>\n",
       "      <td>1.003246</td>\n",
       "      <td>1.025142</td>\n",
       "      <td>0.861988</td>\n",
       "      <td>0.784599</td>\n",
       "      <td>0.345377</td>\n",
       "      <td>1.031362</td>\n",
       "      <td>0.818497</td>\n",
       "      <td>0.903268</td>\n",
       "      <td>0.342248</td>\n",
       "      <td>0.664808</td>\n",
       "      <td>0.237170</td>\n",
       "      <td>1.140540</td>\n",
       "      <td>0.945351</td>\n",
       "      <td>-0.773481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6.15</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.910124</td>\n",
       "      <td>-0.786577</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.631589</td>\n",
       "      <td>-0.712788</td>\n",
       "      <td>-0.412611</td>\n",
       "      <td>-0.387118</td>\n",
       "      <td>1.706769</td>\n",
       "      <td>-0.772212</td>\n",
       "      <td>-1.217975</td>\n",
       "      <td>-1.362580</td>\n",
       "      <td>-1.391869</td>\n",
       "      <td>-0.698345</td>\n",
       "      <td>-0.988081</td>\n",
       "      <td>-0.917824</td>\n",
       "      <td>-0.279583</td>\n",
       "      <td>-0.739570</td>\n",
       "      <td>-0.375283</td>\n",
       "      <td>0.499672</td>\n",
       "      <td>-0.812853</td>\n",
       "      <td>-0.562865</td>\n",
       "      <td>0.215414</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.15</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.907382</td>\n",
       "      <td>-0.624145</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-0.694054</td>\n",
       "      <td>-0.148529</td>\n",
       "      <td>-0.636463</td>\n",
       "      <td>0.742247</td>\n",
       "      <td>0.170186</td>\n",
       "      <td>-0.723128</td>\n",
       "      <td>-0.516683</td>\n",
       "      <td>-0.668177</td>\n",
       "      <td>-0.711948</td>\n",
       "      <td>-0.141693</td>\n",
       "      <td>-0.666124</td>\n",
       "      <td>-0.780922</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>-0.739570</td>\n",
       "      <td>0.468840</td>\n",
       "      <td>0.285968</td>\n",
       "      <td>-0.628570</td>\n",
       "      <td>-0.318072</td>\n",
       "      <td>0.050765</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6.65</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.883263</td>\n",
       "      <td>0.804307</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>0.848669</td>\n",
       "      <td>1.544247</td>\n",
       "      <td>0.392344</td>\n",
       "      <td>1.286578</td>\n",
       "      <td>-0.681110</td>\n",
       "      <td>0.933051</td>\n",
       "      <td>-1.072973</td>\n",
       "      <td>-1.210502</td>\n",
       "      <td>-1.237178</td>\n",
       "      <td>1.041192</td>\n",
       "      <td>1.401980</td>\n",
       "      <td>0.449744</td>\n",
       "      <td>-0.542439</td>\n",
       "      <td>0.017702</td>\n",
       "      <td>0.216882</td>\n",
       "      <td>-0.286152</td>\n",
       "      <td>1.251110</td>\n",
       "      <td>0.994775</td>\n",
       "      <td>-0.605689</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6.45</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.868295</td>\n",
       "      <td>-0.887965</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>0.428851</td>\n",
       "      <td>-0.832679</td>\n",
       "      <td>2.164285</td>\n",
       "      <td>-0.602785</td>\n",
       "      <td>0.392879</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>-1.022494</td>\n",
       "      <td>-1.169635</td>\n",
       "      <td>-1.204473</td>\n",
       "      <td>-0.628763</td>\n",
       "      <td>-0.636701</td>\n",
       "      <td>-0.737941</td>\n",
       "      <td>-0.673866</td>\n",
       "      <td>-0.198661</td>\n",
       "      <td>-0.266344</td>\n",
       "      <td>-0.276897</td>\n",
       "      <td>-0.849709</td>\n",
       "      <td>-0.617982</td>\n",
       "      <td>0.075836</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4.75</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>-0.889223</td>\n",
       "      <td>0.111580</td>\n",
       "      <td>-0.664505</td>\n",
       "      <td>0.080213</td>\n",
       "      <td>-0.732586</td>\n",
       "      <td>0.052011</td>\n",
       "      <td>-1.003135</td>\n",
       "      <td>-0.301013</td>\n",
       "      <td>0.709800</td>\n",
       "      <td>1.025481</td>\n",
       "      <td>0.860983</td>\n",
       "      <td>0.781001</td>\n",
       "      <td>0.136633</td>\n",
       "      <td>-0.021080</td>\n",
       "      <td>-0.561847</td>\n",
       "      <td>-0.016727</td>\n",
       "      <td>0.342248</td>\n",
       "      <td>-0.111152</td>\n",
       "      <td>-0.174252</td>\n",
       "      <td>-0.370575</td>\n",
       "      <td>-0.038347</td>\n",
       "      <td>0.740912</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.25</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>1.173240</td>\n",
       "      <td>1.460938</td>\n",
       "      <td>0.260792</td>\n",
       "      <td>2.507605</td>\n",
       "      <td>-0.203525</td>\n",
       "      <td>1.244151</td>\n",
       "      <td>-0.822704</td>\n",
       "      <td>-0.842057</td>\n",
       "      <td>1.054441</td>\n",
       "      <td>-1.033335</td>\n",
       "      <td>-1.167625</td>\n",
       "      <td>-1.173077</td>\n",
       "      <td>0.345377</td>\n",
       "      <td>1.846155</td>\n",
       "      <td>2.451319</td>\n",
       "      <td>0.508984</td>\n",
       "      <td>0.234066</td>\n",
       "      <td>0.087251</td>\n",
       "      <td>0.504720</td>\n",
       "      <td>0.145416</td>\n",
       "      <td>0.386676</td>\n",
       "      <td>1.385265</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3.85</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.725759</td>\n",
       "      <td>2.137741</td>\n",
       "      <td>1.185957</td>\n",
       "      <td>-0.083937</td>\n",
       "      <td>1.963316</td>\n",
       "      <td>-0.701536</td>\n",
       "      <td>0.603735</td>\n",
       "      <td>-0.528768</td>\n",
       "      <td>0.439047</td>\n",
       "      <td>-0.498388</td>\n",
       "      <td>0.515624</td>\n",
       "      <td>0.459518</td>\n",
       "      <td>2.641565</td>\n",
       "      <td>2.293160</td>\n",
       "      <td>0.229569</td>\n",
       "      <td>-0.148155</td>\n",
       "      <td>0.666793</td>\n",
       "      <td>0.118289</td>\n",
       "      <td>0.286810</td>\n",
       "      <td>1.914526</td>\n",
       "      <td>1.257423</td>\n",
       "      <td>-0.177144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3.95</td>\n",
       "      <td>-7.348469</td>\n",
       "      <td>-0.928919</td>\n",
       "      <td>-1.899719</td>\n",
       "      <td>-1.589670</td>\n",
       "      <td>-1.026713</td>\n",
       "      <td>-1.212151</td>\n",
       "      <td>-1.256610</td>\n",
       "      <td>-1.702382</td>\n",
       "      <td>-2.721790</td>\n",
       "      <td>-2.607838</td>\n",
       "      <td>2.163471</td>\n",
       "      <td>1.976784</td>\n",
       "      <td>1.864166</td>\n",
       "      <td>-1.254997</td>\n",
       "      <td>-1.398873</td>\n",
       "      <td>-1.815898</td>\n",
       "      <td>-1.331006</td>\n",
       "      <td>-0.955934</td>\n",
       "      <td>-4.988441</td>\n",
       "      <td>-6.443172</td>\n",
       "      <td>-1.660551</td>\n",
       "      <td>-4.674781</td>\n",
       "      <td>-2.589195</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean(p$)    a_lab1   ut_lab1  act_lab1  norm_log(dis_lab1)   ct_lab1  \\\n",
       "0       3.90  0.136083 -0.455061 -0.761629           -0.123236 -0.049073   \n",
       "1       4.15  0.136083 -0.817620 -0.125169           -0.664505 -0.603989   \n",
       "2       4.95  0.136083  1.016240  2.141457            1.727227 -0.647569   \n",
       "3       5.15  0.136083 -0.911666 -0.877879           -1.589670 -0.537167   \n",
       "4       3.80  0.136083 -0.885970 -0.428801            0.260792  0.253080   \n",
       "5       4.30  0.136083 -0.223061  0.834033            0.802062  1.102885   \n",
       "6       4.30  0.136083  0.525164  1.814468            1.834012  3.475076   \n",
       "7       4.60  0.136083  0.259429  0.711943            1.007758  0.510200   \n",
       "8       6.60  0.136083 -0.911666 -0.877879           -1.589670 -0.470344   \n",
       "9       5.00  0.136083  0.963889  2.519935            1.185957 -0.329437   \n",
       "10      6.60  0.136083 -0.909336 -0.739865           -1.589670 -0.946816   \n",
       "11      2.30  0.136083  0.210770 -1.174611            0.558591 -0.521187   \n",
       "12      5.60  0.136083 -0.448061 -0.285478            0.802062  0.011938   \n",
       "13      4.10  0.136083 -0.889654 -1.018548           -0.123236 -0.691148   \n",
       "14      4.40  0.136083  1.186075 -0.547175            1.611097  0.129604   \n",
       "15      5.95  0.136083 -0.905267 -0.855585           -0.664505 -0.820435   \n",
       "16      6.10  0.136083  0.193974  1.365390           -0.123236 -0.528451   \n",
       "17      5.45  0.136083 -0.895874 -0.883718           -0.123236 -0.371564   \n",
       "18      3.60  0.136083 -0.257496  0.043103            0.558591  0.574117   \n",
       "19      4.75  0.136083  0.338202  0.105210           -0.664505 -0.513924   \n",
       "20      4.05  0.136083  0.242991  0.082915           -0.123236 -0.458723   \n",
       "21      6.05  0.136083 -0.256895 -1.705436           -0.123236 -0.845130   \n",
       "22      4.40  0.136083 -0.461146 -0.098097            0.802062 -0.930837   \n",
       "23      5.25  0.136083 -0.886042  0.639751           -1.589670 -0.275688   \n",
       "24      4.55  0.136083 -0.125654  0.410965            0.802062  3.005867   \n",
       "25      4.20  0.136083  2.163171 -0.150648            1.343199  2.122651   \n",
       "26      3.95  0.136083 -0.894458 -0.257875           -0.664505 -0.646116   \n",
       "27      5.00  0.136083 -0.327727  1.612754           -0.123236  0.251627   \n",
       "28      4.55  0.136083  1.726221  0.209251           -0.123236 -0.717296   \n",
       "29      3.80  0.136083 -0.907453 -1.196375           -0.664505 -0.978775   \n",
       "30      5.00  0.136083 -0.495115  0.697611           -0.664505  0.826880   \n",
       "31      3.45  0.136083  2.302640  0.252248            1.483889  0.667087   \n",
       "32      3.70  0.136083  1.065956 -0.415530            0.260792 -0.176907   \n",
       "33      5.30  0.136083  2.195294  1.230029            0.260792  1.108695   \n",
       "34      4.55  0.136083  2.112675 -0.661303            0.802062 -1.026713   \n",
       "35      6.90  0.136083 -0.502599 -0.065716            0.260792 -0.865467   \n",
       "36      4.75  0.136083 -0.927485 -1.814787           -1.589670 -1.026713   \n",
       "37      4.70  0.136083  0.375164  0.763433            0.558591 -0.608347   \n",
       "38      4.55  0.136083  1.375779  0.821824           -0.123236  0.398345   \n",
       "39      6.00  0.136083 -0.887744  0.538894           -1.589670 -0.364300   \n",
       "40      6.80  0.136083 -0.768818 -0.818427            1.007758 -0.479060   \n",
       "41      4.75  0.136083  0.340774  0.039387            0.260792 -0.165286   \n",
       "42      4.10  0.136083 -0.880628 -0.763221           -0.123236 -0.830604   \n",
       "43      4.25  0.136083  2.187353 -0.276985            1.483889  1.142106   \n",
       "44      4.55  0.136083 -0.537822  0.152984           -0.123236 -0.110085   \n",
       "45      4.95  0.136083 -0.451735 -0.984575            1.185957 -0.689696   \n",
       "46      6.65  0.136083  0.528498  0.481565           -0.664505  1.217645   \n",
       "47      6.15  0.136083 -0.910124 -0.786577           -1.589670 -0.631589   \n",
       "48      5.15  0.136083 -0.907382 -0.624145           -1.589670 -0.694054   \n",
       "49      6.65  0.136083 -0.883263  0.804307           -1.589670  0.848669   \n",
       "50      6.45  0.136083 -0.868295 -0.887965            0.260792  0.428851   \n",
       "51      4.75  0.136083 -0.889223  0.111580           -0.664505  0.080213   \n",
       "52      6.25  0.136083  1.173240  1.460938            0.260792  2.507605   \n",
       "53      3.85  0.136083  0.725759  2.137741            1.185957 -0.083937   \n",
       "54      3.95 -7.348469 -0.928919 -1.899719           -1.589670 -1.026713   \n",
       "\n",
       "     rt_lab1  ctr_lab1  rtr_lab1  err_lab1  cer_lab1  actq1_lab1  actq2_lab1  \\\n",
       "0  -0.890975  0.786036 -0.874950 -0.637584  0.383103   -0.514311   -0.665163   \n",
       "1  -0.385012 -0.690473 -0.336087  0.553319 -0.195346   -1.196970   -1.260748   \n",
       "2   0.618115 -1.033409 -0.374360  1.189007 -0.304597    0.559647    0.838540   \n",
       "3   0.542220 -0.117179  3.331465 -0.191185 -0.496710   -1.108546   -1.255053   \n",
       "4  -0.496104  0.812065 -0.275336 -0.042385  0.195740    1.020738    0.854618   \n",
       "5  -0.547800  0.596023 -0.989770 -0.457405  0.725633   -0.137240   -0.287981   \n",
       "6   0.554319  1.625480 -0.308141 -0.973142  2.074644   -0.264286   -0.396178   \n",
       "7  -0.317917  0.142463 -0.698772 -0.682122  1.538945   -0.123689   -0.268217   \n",
       "8  -0.636893  0.038346 -0.051771  1.214819 -0.448682   -0.939152   -1.083546   \n",
       "9   3.794682 -0.881138  1.618888 -0.240785  0.224240   -0.280886    0.404077   \n",
       "10 -0.714988 -1.092625 -0.445439  2.339420 -1.288383   -1.218991   -1.361240   \n",
       "11 -0.774383  0.401454  0.067909 -0.191185 -0.107735    1.026159    0.854953   \n",
       "12  0.656612  0.273260  1.691789 -0.339985  0.407908   -0.820915   -0.674207   \n",
       "13 -1.173654 -0.350792 -1.574197 -0.191185  0.169879   -1.204423   -1.341142   \n",
       "14 -0.451007  0.776275 -0.052379  0.138806  0.738828    0.696179    0.569220   \n",
       "15 -0.349814 -0.786782  0.719162  0.652519 -0.983853    0.310977    0.151171   \n",
       "16  1.681737 -0.893502  0.895948 -0.697306  0.075933   -0.506180   -0.649754   \n",
       "17 -1.212151  0.276514 -1.702382  1.777120  0.360936    1.020738    0.858638   \n",
       "18  0.280439  0.702743  0.550274 -0.444245 -0.448682   -0.440456   -0.589793   \n",
       "19  0.043957 -0.648176  0.134735 -0.488784  0.294963   -0.213806    0.841554   \n",
       "20 -0.343215 -0.575294 -0.417493 -1.118905  1.316749   -0.141644   -0.291666   \n",
       "21 -1.127457  0.965638 -0.424176  2.339420 -2.128084    0.366199    0.842894   \n",
       "22  0.508123 -1.130368  1.097035 -0.191185 -1.763387   -0.510246   -0.660473   \n",
       "23  1.350661 -0.553169  1.256203 -0.081357  0.070656    1.021755    0.858303   \n",
       "24 -1.069162  2.893105 -1.520736 -0.881534  1.614418    0.695501    1.226106   \n",
       "25 -0.722687  3.025203 -0.881633  0.138806  0.725633   -0.990309   -0.444749   \n",
       "26  0.196846 -0.705440  0.813934  0.652519 -0.848741    1.024465    0.856293   \n",
       "27  1.504650 -0.391137  0.565461 -0.534841  0.708216   -0.514311   -0.655783   \n",
       "28  0.487224 -0.907818  0.660233  0.419703  0.031072   -0.508891   -0.657458   \n",
       "29 -0.845878 -1.094578 -0.175704  2.339420 -1.796109   -1.192227   -1.337457   \n",
       "30  0.804001  0.440498  0.573359 -0.444245  0.031072   -0.506180   -0.650424   \n",
       "31  0.622514  0.614894  0.796924 -0.659853  0.800578    1.831120    1.658558   \n",
       "32 -0.603896  0.104720 -0.500723  0.989596  0.383103    1.014979    0.847919   \n",
       "33  1.476052  0.365664  0.815756 -0.536360  0.306574    1.030563    0.869357   \n",
       "34 -0.470805 -1.256610  0.052721  0.532062 -0.672461   -1.049936    0.893475   \n",
       "35  0.087954 -1.047725  0.375918 -0.191185 -0.983853   -0.505503   -0.654443   \n",
       "36 -1.212151 -1.256610 -1.702382 -2.721790 -2.607838    1.983574    1.799248   \n",
       "37  0.533421 -0.883090  0.219179 -0.238760  0.197851   -0.515667   -0.663488   \n",
       "38  1.214271 -0.011760  0.911743  0.277483  0.087017    2.114685    1.940607   \n",
       "39  0.047257 -0.610433 -0.188461  0.599376  0.175156   -1.074328   -1.212512   \n",
       "40 -0.358614 -0.052106  0.612240  0.268879 -0.385348   -0.940168   -1.089241   \n",
       "41  0.149549 -0.200473  0.356477 -0.494858 -0.124096    1.013285    0.847919   \n",
       "42 -1.015266 -0.845998 -1.194502  1.495717 -1.024492    1.339877    1.169160   \n",
       "43 -1.187953  1.921563 -1.658641 -0.272670  1.590668    1.835863    1.659898   \n",
       "44 -0.121031 -0.194616 -0.144113 -0.744881  1.152609   -0.514311   -0.662483   \n",
       "45 -0.997667 -0.380725 -1.015286  0.039100 -0.320959   -0.824980   -0.666168   \n",
       "46  1.206572  0.984509  1.275643 -0.462466  1.003246    1.025142    0.861988   \n",
       "47 -0.712788 -0.412611 -0.387118  1.706769 -0.772212   -1.217975   -1.362580   \n",
       "48 -0.148529 -0.636463  0.742247  0.170186 -0.723128   -0.516683   -0.668177   \n",
       "49  1.544247  0.392344  1.286578 -0.681110  0.933051   -1.072973   -1.210502   \n",
       "50 -0.832679  2.164285 -0.602785  0.392879  0.510826   -1.022494   -1.169635   \n",
       "51 -0.732586  0.052011 -1.003135 -0.301013  0.709800    1.025481    0.860983   \n",
       "52 -0.203525  1.244151 -0.822704 -0.842057  1.054441   -1.033335   -1.167625   \n",
       "53  1.963316 -0.701536  0.603735 -0.528768  0.439047   -0.498388    0.515624   \n",
       "54 -1.212151 -1.256610 -1.702382 -2.721790 -2.607838    2.163471    1.976784   \n",
       "\n",
       "    actq3_lab1  max(qat$_lab1)  mean(qact$_lab1)  mean(qavt$_lab1)  \\\n",
       "0    -0.709985       -0.489600         -0.852848         -0.743363   \n",
       "1    -1.289504       -0.350438         -0.503732          0.468175   \n",
       "2     0.771844        0.275796          0.436110          0.508278   \n",
       "3    -1.286561       -0.837508         -0.257596          1.066704   \n",
       "4     0.777731       -0.350438         -0.519575         -0.314631   \n",
       "5     0.113180        0.484540          0.449124          0.280275   \n",
       "6     0.761379        4.311521          2.609458          0.865472   \n",
       "7    -0.290390        1.528262          1.082286         -0.103934   \n",
       "8    -1.122386       -0.698345         -0.886232         -0.533004   \n",
       "9     0.386915        1.180355          2.582864          0.545034   \n",
       "10   -1.390234       -0.976671         -1.111997         -0.934640   \n",
       "11    0.772498       -0.211275         -0.958658         -0.739180   \n",
       "12   -0.718488        0.067051          0.236938          0.661192   \n",
       "13   -1.371265       -0.837508         -1.242704         -1.227669   \n",
       "14    0.765957       -0.072112         -0.185170          0.120644   \n",
       "15    0.084401       -0.907089         -0.812674          0.219207   \n",
       "16   -0.685130        0.554121          1.012124         -0.050041   \n",
       "17    0.774787       -0.976671         -1.143684         -0.764273   \n",
       "18   -0.452603       -0.350438          0.299179          1.096272   \n",
       "19    0.765630       -0.002530         -0.079360         -0.155502   \n",
       "20   -0.345333        2.224077         -0.043147         -0.567184   \n",
       "21    0.770209       -0.976671         -1.284575         -1.317190   \n",
       "22   -0.694614       -0.628763         -0.427345          0.613374   \n",
       "23    0.782309        0.206214          0.849165          0.578307   \n",
       "24    1.142056        0.554121          0.245426         -0.164137   \n",
       "25   -0.449660       -0.628763          0.269191          1.016213   \n",
       "26    0.779039       -0.837508         -0.442622          1.845819   \n",
       "27   -0.692325        1.389099          1.501565         -0.211464   \n",
       "28   -0.699193        0.484540         -0.091808         -0.489942   \n",
       "29   -1.370938       -1.046252         -1.191779         -0.840162   \n",
       "30   -0.690690       -0.698345          0.643769          2.462647   \n",
       "31    1.559035       -0.141693          0.234675         -0.480760   \n",
       "32    0.768573       -0.767926         -0.451110          0.529429   \n",
       "33    0.830057        0.206214          1.259956          1.358651   \n",
       "34    1.659110       -0.350438         -0.784948         -0.911407   \n",
       "35   -0.695595       -0.698345         -0.419423          0.196641   \n",
       "36    1.691160       -1.185415         -1.398873         -1.815898   \n",
       "37   -0.703444        0.693284          0.454216         -0.469507   \n",
       "38    1.839310       -0.141693          0.932342          2.876394   \n",
       "39   -1.241756        0.414959         -0.168761         -0.985537   \n",
       "40   -1.063845        0.067051         -0.621990         -0.945704   \n",
       "41    0.770863       -0.211275         -0.224778          0.656359   \n",
       "42    1.078610       -0.628763         -1.110300         -1.105338   \n",
       "43    1.561324       -0.002530         -0.090111         -0.078546   \n",
       "44   -0.703772        0.345377          0.351801         -0.441093   \n",
       "45   -0.709004       -0.767926         -1.006187         -0.709677   \n",
       "46    0.784599        0.345377          1.031362          0.818497   \n",
       "47   -1.391869       -0.698345         -0.988081         -0.917824   \n",
       "48   -0.711948       -0.141693         -0.666124         -0.780922   \n",
       "49   -1.237178        1.041192          1.401980          0.449744   \n",
       "50   -1.204473       -0.628763         -0.636701         -0.737941   \n",
       "51    0.781001        0.136633         -0.021080         -0.561847   \n",
       "52   -1.173077        0.345377          1.846155          2.451319   \n",
       "53    0.459518        2.641565          2.293160          0.229569   \n",
       "54    1.864166       -1.254997         -1.398873         -1.815898   \n",
       "\n",
       "    max(qme$_lab1)  max(qmce$_lab1)  mean(qmsr$_lab1)  mean(qc$_lab1)  \\\n",
       "0        -0.148155         0.017702          0.475535        0.930445   \n",
       "1        -0.805294        -0.523207         -0.975358        0.456763   \n",
       "2         1.954690        -0.523207         -0.207310       -0.793488   \n",
       "3        -0.673866        -0.631388          0.653245        0.519023   \n",
       "4        -0.148155         0.017702          0.578996        0.505561   \n",
       "5        -0.148155         0.017702         -0.127584       -0.066559   \n",
       "6         0.246129         4.344975          0.193146        0.299430   \n",
       "7        -0.016727         2.722248          0.455451        0.113491   \n",
       "8        -0.016727        -0.631388         -0.011951        0.042817   \n",
       "9         1.034696        -0.090479         -0.273038       -0.788439   \n",
       "10       -1.068150        -0.847752         -3.661702        0.711693   \n",
       "11       -0.542439        -0.415025          0.606992        0.881646   \n",
       "12       -0.279583         0.017702         -0.212179       -0.313075   \n",
       "13       -0.542439        -0.415025          0.753663        0.772270   \n",
       "14        0.771840        -0.306843          0.202275        0.192578   \n",
       "15       -0.936722        -0.847752         -0.975358       -0.879306   \n",
       "16        0.377556        -0.198661          0.083599        0.065534   \n",
       "17        3.794680        -0.631388          0.274090       -0.000092   \n",
       "18        2.480402        -0.306843         -0.119672       -0.220526   \n",
       "19       -0.542439         0.666793          0.523614        0.061327   \n",
       "20       -0.805294         3.154975          0.897901        0.334767   \n",
       "21        0.771840        -0.847752          0.389114       -0.142280   \n",
       "22       -0.805294        -0.739570          0.809654        0.749554   \n",
       "23        1.166123        -0.523207          0.416501        0.261569   \n",
       "24       -0.411011         1.748611          0.504748       -0.076655   \n",
       "25       -0.148155        -0.198661          0.498053        0.348228   \n",
       "26       -0.542439        -0.523207          0.337992        0.389455   \n",
       "27       -0.411011         0.991339          0.158456       -0.089275   \n",
       "28       -0.411011        -0.415025          0.535786        0.362531   \n",
       "29       -1.068150        -0.847752         -0.096545        0.683087   \n",
       "30       -0.805294        -0.523207          0.380594       -0.284469   \n",
       "31        0.377556         0.450430         -1.878515       -1.002143   \n",
       "32       -0.411011        -0.415025          0.429890        0.008322   \n",
       "33        0.114701         0.125884         -0.517694       -0.076655   \n",
       "34       -0.673866        -0.523207          0.771921       -0.069924   \n",
       "35        2.217546        -0.739570          0.205927       -0.394686   \n",
       "36       -1.331006        -0.955934          1.085348        0.466018   \n",
       "37       -0.542439         0.558612          0.734797        1.080205   \n",
       "38       -0.016727        -0.198661          0.336775       -0.005981   \n",
       "39       -0.542439        -0.198661         -0.498828       -0.355984   \n",
       "40        2.348974        -0.523207          0.040997        0.522388   \n",
       "41        0.640412         0.450430          0.537612        0.797511   \n",
       "42       -0.673866        -0.631388         -0.224959       -0.187713   \n",
       "43       -0.016727         0.774975          0.005699       -0.557067   \n",
       "44       -0.542439         0.666793         -0.722791       -0.683270   \n",
       "45       -0.148155        -0.523207          0.810263        0.797511   \n",
       "46        0.903268         0.342248          0.664808        0.237170   \n",
       "47       -0.279583        -0.739570         -0.375283        0.499672   \n",
       "48       -0.542439        -0.739570          0.468840        0.285968   \n",
       "49       -0.542439         0.017702          0.216882       -0.286152   \n",
       "50       -0.673866        -0.198661         -0.266344       -0.276897   \n",
       "51       -0.016727         0.342248         -0.111152       -0.174252   \n",
       "52        0.508984         0.234066          0.087251        0.504720   \n",
       "53       -0.148155         0.666793          0.118289        0.286810   \n",
       "54       -1.331006        -0.955934         -4.988441       -6.443172   \n",
       "\n",
       "    sum(qat$_lab1)  norm_log(sum(qat$_lab1))  avgtime_lab1  curso  \n",
       "0        -0.554857                 -0.231774     -0.390801    0.0  \n",
       "1        -0.370575                 -0.038347      0.348930    0.0  \n",
       "2         0.366554                  0.533396      1.668700    0.0  \n",
       "3        -0.739140                 -0.459359     -0.220591    0.0  \n",
       "4        -0.481144                 -0.150910      0.074525    0.0  \n",
       "5         0.440267                  0.578680      0.190098    0.0  \n",
       "6         3.241359                  1.662132     -0.970884    0.0  \n",
       "7         1.435392                  1.073311     -0.787474    0.0  \n",
       "8        -0.849709                 -0.617982      0.102401    0.0  \n",
       "9         1.361679                  1.042388      0.534182    0.0  \n",
       "10       -1.070848                 -1.009106      1.611617    0.0  \n",
       "11       -0.260006                  0.065289     -1.483413    0.0  \n",
       "12        0.145416                  0.386676     -0.680124    0.0  \n",
       "13       -0.960278                 -0.798859      0.098353    0.0  \n",
       "14       -0.149436                  0.161162     -0.677507    0.0  \n",
       "15       -1.181417                 -1.260238      2.065196    0.0  \n",
       "16        0.513980                  0.622541      0.617778    0.0  \n",
       "17       -1.070848                 -1.009106      1.090601    0.0  \n",
       "18       -0.038867                  0.250436     -0.030432    0.0  \n",
       "19        0.550837                  0.644018     -0.652784    0.0  \n",
       "20        1.214253                  0.978473     -1.116212    0.0  \n",
       "21       -1.255130                 -1.459747     -1.565690    0.0  \n",
       "22       -0.739140                 -0.459359      1.586931    0.0  \n",
       "23        0.808832                  0.784528     -0.392760    0.0  \n",
       "24        0.550837                  0.644018     -0.357477    0.0  \n",
       "25       -0.260006                  0.065289      0.078115    0.0  \n",
       "26       -0.997135                 -0.865233      2.696585    0.0  \n",
       "27        2.504229                  1.453049     -0.787902    0.0  \n",
       "28        0.108559                  0.360541     -0.043074    0.0  \n",
       "29       -1.181417                 -1.260238      0.546071    0.0  \n",
       "30       -0.333719                 -0.002896      1.591752    0.0  \n",
       "31        0.108559                  0.360541      0.008835    0.0  \n",
       "32       -0.554857                 -0.231774      0.277739    0.0  \n",
       "33        0.808832                  0.784528      0.117781    0.0  \n",
       "34       -0.554857                 -0.231774     -0.197006    0.0  \n",
       "35       -0.702283                 -0.410581      1.498481    0.0  \n",
       "36       -1.402556                 -1.984394     -1.886085    0.0  \n",
       "37        1.251110                  0.994775     -0.635672    0.0  \n",
       "38        0.071703                  0.333888      0.766382    0.0  \n",
       "39        0.366554                  0.533396     -0.019804    0.0  \n",
       "40       -0.260006                  0.065289     -0.940237    0.0  \n",
       "41        0.219128                  0.437394     -0.385855    0.0  \n",
       "42       -0.923422                 -0.735720      0.703780    0.0  \n",
       "43       -0.038867                  0.250436     -0.451997    0.0  \n",
       "44        1.287966                  1.010819     -1.102280    0.0  \n",
       "45       -0.554857                 -0.231774     -0.821456    0.0  \n",
       "46        1.140540                  0.945351     -0.773481    0.0  \n",
       "47       -0.812853                 -0.562865      0.215414    0.0  \n",
       "48       -0.628570                 -0.318072      0.050765    0.0  \n",
       "49        1.251110                  0.994775     -0.605689    0.0  \n",
       "50       -0.849709                 -0.617982      0.075836    0.0  \n",
       "51       -0.370575                 -0.038347      0.740912    0.0  \n",
       "52        0.145416                  0.386676      1.385265    0.0  \n",
       "53        1.914526                  1.257423     -0.177144    0.0  \n",
       "54       -1.660551                 -4.674781     -2.589195    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso7 = dfFinlab1.loc[dfFinlab1['curso']=='7']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene la columna con el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso7.reset_index()[TARGET])\n",
    "\n",
    "# Se obtiene los datos del curso X para el lab Y normalizados Excluyendo la fila mean(p$p2)\n",
    "# ------------               Función que normaliza la data      , el nombre de las col a colocar en el DF  [desde cual columna hasta cual]\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso7),columns=dfLab1Curso7.columns)[dfLab1Curso7.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc7 = pd.concat([promCurso,DFnormalizado],axis=1)\n",
    "datalab1Normc7\n",
    "\n",
    "# Sentencia anterior\n",
    "#datalab1Normc7 = dfLab1Curso7[['mean(p$p2)']].join(pd.DataFrame(scaler1.fit_transform(dfLab1Curso7), columns=dfLab1Curso7.columns)[dfLab1Curso7.columns[1:26]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso13 = dfFinlab1.loc[dfFinlab1['curso']=='13']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso13.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso13),columns=dfLab1Curso13.columns)[dfLab1Curso13.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc13 = pd.concat([promCurso,DFnormalizado],axis=1)\n",
    "\n",
    "\n",
    "# Sentencia anterior\n",
    "#datalab1Normc13 = dfLab1Curso13[['mean(p$p2)']].join(pd.DataFrame(scaler2.fit_transform(dfLab1Curso13), columns=dfLab1Curso13.columns)[dfLab1Curso13.columns[1:26]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso19 = dfFinlab1.loc[dfFinlab1['curso']=='19']\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso19.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso19),columns=dfLab1Curso19.columns)[dfLab1Curso19.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc19 = pd.concat([promCurso,DFnormalizado],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso24 = dfFinlab1.loc[dfFinlab1['curso']=='24']\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso24.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso24),columns=dfLab1Curso24.columns)[dfLab1Curso24.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc24 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso30 = dfFinlab1.loc[dfFinlab1['curso']=='30']\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso30.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso30),columns=dfLab1Curso30.columns)[dfLab1Curso30.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc30 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab1Curso36 = dfFinlab1.loc[dfFinlab1['curso']=='36']\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab1Curso36.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab1Curso36),columns=dfLab1Curso36.columns)[dfLab1Curso36.columns[1:26]]\n",
    "                       \n",
    "datalab1Normc36 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se unen los datos del laboratorio 1\n",
    "datalab1_norm = pd.concat([datalab1Normc7,datalab1Normc13,datalab1Normc19,datalab1Normc24,datalab1Normc30,datalab1Normc36],axis=0)\n",
    "datalab1_norm = datalab1_norm.reset_index(drop = True)\n",
    "#datalab1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "13\n",
      "19\n",
      "24\n",
      "30\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "#LAB 2\n",
    "datalab2_all,cursoData = get_custom_dataframe(DATA, [2], ['p1','p2'], 'all', labels=True, index=None)\n",
    " \n",
    "datalab2 = copy.deepcopy(datalab2_all)\n",
    " \n",
    "# Remove questionsdifficulty\n",
    "remove_col(datalab2, 'qd?')\n",
    "# Group columns\n",
    "datalab2_all = apply(datalab2_all, ['p1','p2'], statistics.mean)\n",
    "datalab2 = apply(datalab2, ['p1','p2'], statistics.mean)\n",
    "datalab2 = apply(datalab2, 'dis_lab2', norm_log)\n",
    "datalab2 = apply(datalab2, 'qg?', statistics.mean)\n",
    "datalab2 = apply(datalab2, 'qat?', sum, replace=False)\n",
    "datalab2 = apply(datalab2, 'sum(qat$_lab2)', norm_log, replace=False)\n",
    "datalab2 = apply(datalab2, 'qat?', max)\n",
    "datalab2 = apply(datalab2, 'qact?', statistics.mean)\n",
    "datalab2 = apply(datalab2, 'qavt?', statistics.mean)\n",
    "datalab2 = apply(datalab2, 'qme?', max)\n",
    "datalab2 = apply(datalab2, 'qmce?', max)\n",
    "datalab2 = apply(datalab2, 'qmsr?', statistics.mean)\n",
    "datalab2 = apply(datalab2, 'qc?', statistics.mean)\n",
    "aux = datalab2['act_lab2'] / datalab2['sum(qat$_lab2)']\n",
    "for i in range(len(aux)):\n",
    "    if not aux[i] > 0:\n",
    "        aux[i] = 0\n",
    "datalab2['avgtime_lab2'] = aux\n",
    "datalab2 = datalab2.round(4)\n",
    "\n",
    "# Se transforma a dataframe la info de ese curso en particular\n",
    "cursoDF = pd.DataFrame(cursoData,columns=['curso'])\n",
    "\n",
    "# Se concatenan los dos dataframes \n",
    "dfFinlab2 = pd.concat([datalab2,cursoDF],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso7 = dfFinlab2.loc[dfFinlab2['curso']=='7']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso7.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso7),columns=dfLab2Curso7.columns)[dfLab2Curso7.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc7 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso13 = dfFinlab2.loc[dfFinlab2['curso']=='13']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso13.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso13),columns=dfLab2Curso13.columns)[dfLab2Curso13.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc13 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso19 = dfFinlab2.loc[dfFinlab2['curso']=='19']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso19.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso19),columns=dfLab2Curso19.columns)[dfLab2Curso19.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc19 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso24 = dfFinlab2.loc[dfFinlab2['curso']=='24']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso24.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso24),columns=dfLab2Curso24.columns)[dfLab2Curso24.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc24 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso30 = dfFinlab2.loc[dfFinlab2['curso']=='30']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso30.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso30),columns=dfLab2Curso30.columns)[dfLab2Curso30.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc30 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab2Curso36 = dfFinlab2.loc[dfFinlab2['curso']=='36']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab2Curso36.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab2Curso36),columns=dfLab2Curso36.columns)[dfLab2Curso36.columns[1:26]]\n",
    "                       \n",
    "datalab2Normc36 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se unen los datos del laboratorio 2\n",
    "datalab2_norm = pd.concat([datalab2Normc7,datalab2Normc13,datalab2Normc19,datalab2Normc24,datalab2Normc30,datalab2Normc36],axis=0)\n",
    "datalab2_norm = datalab2_norm.reset_index(drop = True)\n",
    "#datalab2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#scaler1 = StandardScaler()\n",
    "#datalab2Normc7 = dfLab2Curso7[['mean(p$p2)']].join(pd.DataFrame(scaler1.fit_transform(dfLab2Curso7), columns=dfLab2Curso7.columns)[dfLab2Curso7.columns[1:26]]) \n",
    "#datalab2Normc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "13\n",
      "19\n",
      "24\n",
      "30\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "#LAB 3\n",
    "\n",
    "datalab3_all,cursoData = get_custom_dataframe(DATA, [3], ['p1','p2'], 'all', labels=True, index=None)\n",
    "\n",
    "datalab3 = copy.deepcopy(datalab3_all)\n",
    " \n",
    "# Remove questionsdifficulty\n",
    "remove_col(datalab3, 'qd?')\n",
    "# Group columns\n",
    "datalab3_all = apply(datalab3_all, ['p1','p2'], statistics.mean)\n",
    "datalab3 = apply(datalab3, ['p1','p2'], statistics.mean)\n",
    "datalab3 = apply(datalab3, 'dis_lab3', norm_log)\n",
    "datalab3 = apply(datalab3, 'qg?', statistics.mean)\n",
    "datalab3 = apply(datalab3, 'qat?', sum, replace=False)\n",
    "datalab3 = apply(datalab3, 'sum(qat$_lab3)', norm_log, replace=False)\n",
    "datalab3 = apply(datalab3, 'qat?', max)\n",
    "datalab3 = apply(datalab3, 'qact?', statistics.mean)\n",
    "datalab3 = apply(datalab3, 'qavt?', statistics.mean)\n",
    "datalab3 = apply(datalab3, 'qme?', max)\n",
    "datalab3 = apply(datalab3, 'qmce?', max)\n",
    "datalab3 = apply(datalab3, 'qmsr?', statistics.mean)\n",
    "datalab3 = apply(datalab3, 'qc?', statistics.mean)\n",
    "aux = datalab3['act_lab3'] / datalab3['sum(qat$_lab3)']\n",
    "for i in range(len(aux)):\n",
    "    if not aux[i] > 0:\n",
    "        aux[i] = 0\n",
    "datalab3['avgtime_lab3'] = aux\n",
    "datalab3 = datalab3.round(4)\n",
    "\n",
    "# Se transforma a dataframe la info de ese curso en particular\n",
    "cursoDF = pd.DataFrame(cursoData,columns=['curso'])\n",
    "\n",
    "# Se concatenan los dos dataframes \n",
    "dfFinlab3 = pd.concat([datalab3,cursoDF],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso7 = dfFinlab3.loc[dfFinlab3['curso']=='7']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso7.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso7),columns=dfLab3Curso7.columns)[dfLab3Curso7.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc7 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso13 = dfFinlab3.loc[dfFinlab3['curso']=='13']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso13.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso13),columns=dfLab3Curso13.columns)[dfLab3Curso13.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc13 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso19 = dfFinlab3.loc[dfFinlab3['curso']=='19']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso19.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso19),columns=dfLab3Curso19.columns)[dfLab3Curso19.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc19 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso24 = dfFinlab3.loc[dfFinlab3['curso']=='24']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso24.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso24),columns=dfLab3Curso24.columns)[dfLab3Curso24.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc24 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso30 = dfFinlab3.loc[dfFinlab3['curso']=='30']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso30.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso30),columns=dfLab3Curso30.columns)[dfLab3Curso30.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc30 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab3Curso36 = dfFinlab3.loc[dfFinlab3['curso']=='36']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab3Curso36.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab3Curso36),columns=dfLab3Curso36.columns)[dfLab3Curso36.columns[1:26]]\n",
    "                       \n",
    "datalab3Normc36 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se unen los datos del laboratorio 3\n",
    "datalab3_norm = pd.concat([datalab3Normc7,datalab3Normc13,datalab3Normc19,datalab3Normc24,datalab3Normc30,datalab3Normc36],axis=0)\n",
    "datalab3_norm = datalab3_norm.reset_index(drop = True)\n",
    "#datalab3_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "13\n",
      "19\n",
      "24\n",
      "30\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "#LAB 4\n",
    "\n",
    "datalab4_all,cursoData = get_custom_dataframe(DATA, [4], ['p1','p2'], 'all', labels=True, index=None)\n",
    "datalab4 = copy.deepcopy(datalab4_all)\n",
    "\n",
    "# Remove questionsdifficulty\n",
    "remove_col(datalab4, 'qd?')\n",
    "# Group columns\n",
    "datalab4_all = apply(datalab4_all, ['p1','p2'], statistics.mean)\n",
    "datalab4 = apply(datalab4, ['p1','p2'], statistics.mean)\n",
    "datalab4 = apply(datalab4, 'dis_lab4', norm_log)\n",
    "datalab4 = apply(datalab4, 'qg?', statistics.mean)\n",
    "datalab4 = apply(datalab4, 'qat?', sum, replace=False)\n",
    "datalab4 = apply(datalab4, 'sum(qat$_lab4)', norm_log, replace=False)\n",
    "datalab4 = apply(datalab4, 'qat?', max)\n",
    "datalab4 = apply(datalab4, 'qact?', statistics.mean)\n",
    "datalab4 = apply(datalab4, 'qavt?', statistics.mean)\n",
    "datalab4 = apply(datalab4, 'qme?', max)\n",
    "datalab4 = apply(datalab4, 'qmce?', max)\n",
    "datalab4 = apply(datalab4, 'qmsr?', statistics.mean)\n",
    "datalab4 = apply(datalab4, 'qc?', statistics.mean)\n",
    "aux = datalab4['act_lab4'] / datalab4['sum(qat$_lab4)']\n",
    "for i in range(len(aux)):\n",
    "    if not aux[i] > 0:\n",
    "        aux[i] = 0\n",
    "datalab4['avgtime_lab4'] = aux\n",
    "datalab4 = datalab4.round(4)\n",
    "\n",
    "# Se transforma a dataframe la info de ese curso en particular\n",
    "cursoDF = pd.DataFrame(cursoData,columns=['curso'])\n",
    "\n",
    "# Se concatenan los dos dataframes \n",
    "dfFinlab4 = pd.concat([datalab4,cursoDF],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso7 = dfFinlab4.loc[dfFinlab4['curso']=='7']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso7.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso7),columns=dfLab4Curso7.columns)[dfLab4Curso7.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc7 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso13 = dfFinlab4.loc[dfFinlab4['curso']=='13']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso13.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso13),columns=dfLab4Curso13.columns)[dfLab4Curso13.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc13 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso19 = dfFinlab4.loc[dfFinlab4['curso']=='19']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso19.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso19),columns=dfLab4Curso19.columns)[dfLab4Curso19.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc19 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso24 = dfFinlab4.loc[dfFinlab4['curso']=='24']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso24.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso24),columns=dfLab4Curso24.columns)[dfLab4Curso24.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc24 = pd.concat([promCurso,DFnormalizado],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso30 = dfFinlab4.loc[dfFinlab4['curso']=='30']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso30.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso30),columns=dfLab4Curso30.columns)[dfLab4Curso30.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc30 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Filtrar por curso\n",
    "dfLab4Curso36 = dfFinlab4.loc[dfFinlab4['curso']=='36']\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "# Se obtiene el promedio del curso X\n",
    "promCurso = pd.DataFrame(dfLab4Curso36.reset_index()[TARGET])\n",
    "\n",
    "DFnormalizado = pd.DataFrame(scaler1.fit_transform(dfLab4Curso36),columns=dfLab4Curso36.columns)[dfLab4Curso36.columns[1:26]]\n",
    "                       \n",
    "datalab4Normc36 = pd.concat([promCurso,DFnormalizado],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se unen los datos del laboratorio 4\n",
    "datalab4_norm = pd.concat([datalab4Normc7,datalab4Normc13,datalab4Normc19,datalab4Normc24,datalab4Normc30,datalab4Normc36],axis=0)\n",
    "datalab4_norm = datalab4_norm.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Grid/Random-SearchCV process**   \n",
    " \n",
    "def run_process(dataset, grid_cv, target=TARGET):\n",
    "    X, y = dataset.drop(target, axis=1), np.array(dataset[target])\n",
    "   \n",
    "    grid_cv.fit(X,y)\n",
    "    print('R2:', max(grid_cv.cv_results_['mean_test_score']))\n",
    "    \n",
    "    try:\n",
    "        selected_features = X.columns[grid_cv.best_estimator_.steps[0][-1].get_support()]\n",
    "    except:\n",
    "        return list(dataset.columns[1:])\n",
    "    \n",
    "    return list(selected_features)\n",
    "    \n",
    "def run_process_obsolete(dataset, grid_cv, target=TARGET):\n",
    "    X, y = dataset.drop(target, axis=1), np.array(dataset[target])\n",
    "   \n",
    "    grid_cv.fit(X,y)\n",
    " \n",
    "    try:\n",
    "        print('R2-test-fit:', max(grid_cv.cv_results_['mean_test_score']))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        grid_cv.best_estimator_.fit(X_train, y_train)\n",
    "        print('R2-test', grid_cv.best_estimator_.score(X_test, y_test))\n",
    "        print('MSE-test', metrics.mean_squared_error(y_test,grid_cv.best_estimator_.predict(X_test)))\n",
    "\n",
    "        print('Best params:', grid_cv.best_params_)\n",
    "\n",
    "        selected_features = X.columns[grid_cv.best_estimator_.steps[0][-1].get_support()]\n",
    "        print('Selected features:', list(selected_features))\n",
    "\n",
    "        return list(selected_features)\n",
    "    except:\n",
    "        return list(dataset.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **SVR - Recursive Features Elimination**\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "sel_estimator = SVR(kernel='linear')\n",
    "selector = RFE(sel_estimator)\n",
    "estimator = SVR()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('sel', selector),\n",
    "    ('est', estimator)\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'sel__n_features_to_select' : [5],\n",
    "    'sel__step'                 : [1,2],\n",
    "    'est__C'                    : [0.01,0.1,1],\n",
    "    'est__gamma'                : ['scale','auto'],\n",
    "    'est__kernel'               : ['linear','poly','rbf']\n",
    "}\n",
    "\n",
    "grid_svr = GridSearchCV(estimator=pipe,\n",
    "                        param_grid=params,\n",
    "                        scoring='r2',\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True,\n",
    "                        cv=KFold(n_splits=10, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   27.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -0.027847394166631255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab1',\n",
       " 'norm_log(dis_lab1)',\n",
       " 'actq3_lab1',\n",
       " 'sum(qat$_lab1)',\n",
       " 'avgtime_lab1']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab1_shuffle = datalab1_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_svr_1 = run_process(datalab1_shuffle,grid_svr)\n",
    "selected_features_svr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   28.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.0024818705440738053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab2', 'act_lab2', 'ctr_lab2', 'mean(qact$_lab2)', 'mean(qmsr$_lab2)']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab2_shuffle = datalab2_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_svr_2 = run_process(datalab2_shuffle,grid_svr)\n",
    "selected_features_svr_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   30.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.09243672203962304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab3', 'rtr_lab3', 'actq1_lab3', 'sum(qat$_lab3)', 'avgtime_lab3']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab3_shuffle = datalab3_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_svr_3 = run_process(datalab3_shuffle,grid_svr)\n",
    "selected_features_svr_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   30.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.14205962118987578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rt_lab4',\n",
       " 'max(qat$_lab4)',\n",
       " 'mean(qact$_lab4)',\n",
       " 'mean(qavt$_lab4)',\n",
       " 'norm_log(sum(qat$_lab4))']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab4_shuffle = datalab4_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_svr_4 = run_process(datalab4_shuffle,grid_svr)\n",
    "selected_features_svr_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVR()\n",
    "\n",
    "params = {\n",
    "    'C'         : [0.01,0.1,1],\n",
    "    'gamma'     : ['scale','auto'],\n",
    "    'kernel'    : ['linear','poly','rbf']\n",
    "}\n",
    "\n",
    "grid_svr = GridSearchCV(estimator=estimator,\n",
    "                        param_grid=params,\n",
    "                        scoring='r2',\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True,\n",
    "                        cv=KFold(n_splits=10, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    4.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.02613049022784335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab1',\n",
       " 'norm_log(dis_lab1)',\n",
       " 'actq3_lab1',\n",
       " 'sum(qat$_lab1)',\n",
       " 'avgtime_lab1',\n",
       " 'a_lab2',\n",
       " 'act_lab2',\n",
       " 'ctr_lab2',\n",
       " 'mean(qact$_lab2)',\n",
       " 'mean(qmsr$_lab2)']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combiación lab 1 y 2\n",
    "dataset = datalab1_norm[[TARGET] + selected_features_svr_1].join(datalab2_norm[selected_features_svr_2])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    3.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.09411965728290814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab1',\n",
       " 'norm_log(dis_lab1)',\n",
       " 'actq3_lab1',\n",
       " 'sum(qat$_lab1)',\n",
       " 'avgtime_lab1',\n",
       " 'a_lab2',\n",
       " 'act_lab2',\n",
       " 'ctr_lab2',\n",
       " 'mean(qact$_lab2)',\n",
       " 'mean(qmsr$_lab2)',\n",
       " 'act_lab3',\n",
       " 'rtr_lab3',\n",
       " 'actq1_lab3',\n",
       " 'sum(qat$_lab3)',\n",
       " 'avgtime_lab3']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinación lab 1, 2 y 3\n",
    "dataset = datalab1_norm[[TARGET] + selected_features_svr_1].join(datalab2_norm[selected_features_svr_2]).join(datalab3_norm[selected_features_svr_3])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    3.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.14302263421043446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['act_lab1',\n",
       " 'norm_log(dis_lab1)',\n",
       " 'actq3_lab1',\n",
       " 'sum(qat$_lab1)',\n",
       " 'avgtime_lab1',\n",
       " 'a_lab2',\n",
       " 'act_lab2',\n",
       " 'ctr_lab2',\n",
       " 'mean(qact$_lab2)',\n",
       " 'mean(qmsr$_lab2)',\n",
       " 'act_lab3',\n",
       " 'rtr_lab3',\n",
       " 'actq1_lab3',\n",
       " 'sum(qat$_lab3)',\n",
       " 'avgtime_lab3',\n",
       " 'rt_lab4',\n",
       " 'max(qat$_lab4)',\n",
       " 'mean(qact$_lab4)',\n",
       " 'mean(qavt$_lab4)',\n",
       " 'norm_log(sum(qat$_lab4))']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinación lab 1, 2, 3 y 4\n",
    "dataset = datalab1_norm[[TARGET] + selected_features_svr_1].join(datalab2_norm[selected_features_svr_2]).join(datalab3_norm[selected_features_svr_3]).join(datalab4_norm[selected_features_svr_4])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Random Forest Regressor** \n",
    "\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    " \n",
    "# GradientBoostingRegressor / RandomForestRegressor / SVR(kernel='linear')\n",
    "sel_estimator = GradientBoostingRegressor(random_state=1)\n",
    " \n",
    "# RFE / SelectFromModel\n",
    "selector = RFE(sel_estimator)\n",
    "estimator = RandomForestRegressor(random_state=1, n_jobs=-1)\n",
    " \n",
    "pipe = Pipeline([\n",
    "    ('sel', selector),\n",
    "    ('est', estimator)\n",
    "])\n",
    " \n",
    "params = {\n",
    "    'sel__estimator__learning_rate': [0.05,0.1,0.2],\n",
    "    'sel__n_features_to_select'    : [5],\n",
    "    'est__n_estimators'            : [50,100,200,400],\n",
    "    'est__criterion'               : ['mse','mae'],\n",
    "    'est__max_features'            : ['auto','sqrt','log2']\n",
    "}\n",
    " \n",
    "grid_rfr = GridSearchCV(estimator=pipe,\n",
    "                        param_grid=params,\n",
    "                        scoring='r2',\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True,\n",
    "                        cv=KFold(n_splits=10, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -0.12825433801322306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab1', 'actq1_lab1', 'actq3_lab1', 'max(qat$_lab1)', 'mean(qmsr$_lab1)']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab1_shuffle = datalab1_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_rfr_1 = run_process(datalab1_shuffle,grid_rfr)\n",
    "selected_features_rfr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 12.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.030681338979879347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab2', 'ctr_lab2', 'max(qat$_lab2)', 'mean(qc$_lab2)', 'avgtime_lab2']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab2_shuffle = datalab2_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_rfr_2 = run_process(datalab2_shuffle,grid_rfr)\n",
    "selected_features_rfr_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 12.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.04908655011401486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rtr_lab3', 'cer_lab3', 'actq1_lab3', 'max(qat$_lab3)', 'avgtime_lab3']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab3_shuffle = datalab3_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_rfr_3 = run_process(datalab3_shuffle,grid_rfr)\n",
    "selected_features_rfr_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.09488695772769623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a_lab4', 'act_lab4', 'cer_lab4', 'mean(qc$_lab4)', 'avgtime_lab4']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab4_shuffle = datalab4_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_rfr_4 = run_process(datalab4_shuffle,grid_rfr)\n",
    "selected_features_rfr_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestRegressor(random_state=1, n_jobs=-1)\n",
    " \n",
    "params = {\n",
    "    'n_estimators'  : [50,100,200,400],\n",
    "    'criterion'     : ['mse','mae'],\n",
    "    'max_features'  : ['auto','sqrt','log2']\n",
    "}\n",
    " \n",
    "grid_rfr = GridSearchCV(estimator=estimator,\n",
    "                        param_grid=params,\n",
    "                        scoring='r2',\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        return_train_score=True,\n",
    "                        cv=KFold(n_splits=10, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.13065170464265768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab1',\n",
       " 'actq1_lab1',\n",
       " 'actq3_lab1',\n",
       " 'max(qat$_lab1)',\n",
       " 'mean(qmsr$_lab1)',\n",
       " 'ut_lab2',\n",
       " 'ctr_lab2',\n",
       " 'max(qat$_lab2)',\n",
       " 'mean(qc$_lab2)',\n",
       " 'avgtime_lab2']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinación de laboratorios\n",
    "dataset = datalab1_norm[[TARGET] + selected_features_rfr_1].join(datalab2_norm[selected_features_rfr_2])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.16624134155579337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab1',\n",
       " 'actq1_lab1',\n",
       " 'actq3_lab1',\n",
       " 'max(qat$_lab1)',\n",
       " 'mean(qmsr$_lab1)',\n",
       " 'ut_lab2',\n",
       " 'ctr_lab2',\n",
       " 'max(qat$_lab2)',\n",
       " 'mean(qc$_lab2)',\n",
       " 'avgtime_lab2',\n",
       " 'rtr_lab3',\n",
       " 'cer_lab3',\n",
       " 'actq1_lab3',\n",
       " 'max(qat$_lab3)',\n",
       " 'avgtime_lab3']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datalab1_norm[[TARGET] + selected_features_rfr_1].join(datalab2_norm[selected_features_rfr_2]).join(datalab3_norm[selected_features_rfr_3])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.2680787959710579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab1',\n",
       " 'actq1_lab1',\n",
       " 'actq3_lab1',\n",
       " 'max(qat$_lab1)',\n",
       " 'mean(qmsr$_lab1)',\n",
       " 'ut_lab2',\n",
       " 'ctr_lab2',\n",
       " 'max(qat$_lab2)',\n",
       " 'mean(qc$_lab2)',\n",
       " 'avgtime_lab2',\n",
       " 'rtr_lab3',\n",
       " 'cer_lab3',\n",
       " 'actq1_lab3',\n",
       " 'max(qat$_lab3)',\n",
       " 'avgtime_lab3',\n",
       " 'a_lab4',\n",
       " 'act_lab4',\n",
       " 'cer_lab4',\n",
       " 'mean(qc$_lab4)',\n",
       " 'avgtime_lab4']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datalab1_norm[[TARGET] + selected_features_rfr_1].join(datalab2_norm[selected_features_rfr_2]).join(datalab3_norm[selected_features_rfr_3]).join(datalab4_norm[selected_features_rfr_4])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24741547457664903"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = dataset_shuffle.drop(TARGET, axis=1), np.array(dataset_shuffle[TARGET])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "grid_rfr.best_estimator_.fit(X_train,y_train)\n",
    "ypred = grid_rfr.best_estimator_.predict(X_test)\n",
    "grid_rfr.best_estimator_.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(ypred,y_test)\n",
    "plt.xlabel('y_pred')\n",
    "plt.ylabel('y_test')\n",
    "#plt.show\n",
    "plt.savefig('pruebas_rf_1234_0.3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Linear Regression**\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    " \n",
    "# GradientBoostingRegressor / RandomForestRegressor / SVR(kernel='linear')\n",
    "sel_estimator = GradientBoostingRegressor(random_state=1)\n",
    " \n",
    "# RFE / SelectFromModel\n",
    "selector = RFE(sel_estimator)\n",
    "estimator = LinearRegression()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('sel', selector),\n",
    "    ('est', estimator)\n",
    "])\n",
    " \n",
    "# params = {'est__n_jobs': [-1], \n",
    "#           'est__normalize': [True], \n",
    "#           'sel__estimator__learning_rate': [0.1], \n",
    "#           'sel__estimator__n_estimators': [100], \n",
    "#           'sel__max_features': [10], \n",
    "#           'sel__prefit': [False]}\n",
    "\n",
    "params = {\n",
    "    'sel__n_features_to_select' : [5],\n",
    "    'sel__step'                 : [1,2],\n",
    "    'est__n_jobs'               : [-1],\n",
    "}\n",
    " \n",
    "grid_lr = GridSearchCV(estimator=pipe,\n",
    "                       param_grid=params,\n",
    "                       scoring='r2',\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1,\n",
    "                       return_train_score=True,\n",
    "                       cv=KFold(n_splits=10, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -0.016828560011491144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab1', 'actq1_lab1', 'actq3_lab1', 'max(qat$_lab1)', 'mean(qmsr$_lab1)']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab1_shuffle = datalab1_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_lr_1 = run_process(datalab1_shuffle,grid_lr)\n",
    "selected_features_lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   13.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -0.025246618190295656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab2', 'ctr_lab2', 'rtr_lab2', 'max(qat$_lab2)', 'avgtime_lab2']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab2_shuffle = datalab2_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_lr_2 = run_process(datalab2_shuffle,grid_lr)\n",
    "selected_features_lr_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   34.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.03700660993150094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rtr_lab3', 'cer_lab3', 'actq1_lab3', 'max(qat$_lab3)', 'avgtime_lab3']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab3_shuffle = datalab3_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_lr_3 = run_process(datalab3_shuffle,grid_lr)\n",
    "selected_features_lr_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   11.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.060838909215779745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab4', 'cer_lab4', 'mean(qmsr$_lab4)', 'mean(qc$_lab4)', 'avgtime_lab4']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalab4_shuffle = datalab4_norm.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "selected_features_lr_4 = run_process(datalab4_shuffle,grid_lr)\n",
    "selected_features_lr_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LinearRegression()\n",
    " \n",
    "params = {\n",
    "    'n_jobs'    : [-1],\n",
    "}\n",
    " \n",
    "grid_lr = GridSearchCV(estimator=estimator,\n",
    "                       param_grid=params,\n",
    "                       scoring='r2',\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1,\n",
    "                       return_train_score=True,\n",
    "                       cv=KFold(n_splits=10, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.038805189651236006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab1',\n",
       " 'actq1_lab1',\n",
       " 'actq3_lab1',\n",
       " 'max(qat$_lab1)',\n",
       " 'mean(qmsr$_lab1)',\n",
       " 'ut_lab2',\n",
       " 'ctr_lab2',\n",
       " 'rtr_lab2',\n",
       " 'max(qat$_lab2)',\n",
       " 'avgtime_lab2']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinación de laboratorios 1 y 2\n",
    "dataset = datalab1_norm[[TARGET] + selected_features_lr_1].join(datalab2_norm[selected_features_lr_2])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.08989724090700761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab1',\n",
       " 'actq1_lab1',\n",
       " 'actq3_lab1',\n",
       " 'max(qat$_lab1)',\n",
       " 'mean(qmsr$_lab1)',\n",
       " 'ut_lab2',\n",
       " 'ctr_lab2',\n",
       " 'rtr_lab2',\n",
       " 'max(qat$_lab2)',\n",
       " 'avgtime_lab2',\n",
       " 'rtr_lab3',\n",
       " 'cer_lab3',\n",
       " 'actq1_lab3',\n",
       " 'max(qat$_lab3)',\n",
       " 'avgtime_lab3']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinación de laboratorios 1, 2 y 3\n",
    "dataset = datalab1_norm[[TARGET] + selected_features_lr_1].join(datalab2_norm[selected_features_lr_2]).join(datalab3_norm[selected_features_lr_3])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.13966032680714527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ut_lab1',\n",
       " 'actq1_lab1',\n",
       " 'actq3_lab1',\n",
       " 'max(qat$_lab1)',\n",
       " 'mean(qmsr$_lab1)',\n",
       " 'ut_lab2',\n",
       " 'ctr_lab2',\n",
       " 'rtr_lab2',\n",
       " 'max(qat$_lab2)',\n",
       " 'avgtime_lab2',\n",
       " 'rtr_lab3',\n",
       " 'cer_lab3',\n",
       " 'actq1_lab3',\n",
       " 'max(qat$_lab3)',\n",
       " 'avgtime_lab3',\n",
       " 'ut_lab4',\n",
       " 'cer_lab4',\n",
       " 'mean(qmsr$_lab4)',\n",
       " 'mean(qc$_lab4)',\n",
       " 'avgtime_lab4']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinación de laboratorios 1, 2, 3 y 4\n",
    "dataset = datalab1_norm[[TARGET] + selected_features_lr_1].join(datalab2_norm[selected_features_lr_2]).join(datalab3_norm[selected_features_lr_3]).join(datalab4_norm[selected_features_lr_4])\n",
    "dataset_shuffle = dataset.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "run_process(dataset_shuffle,grid_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
